[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Sebastian Gerdes (email: first name dot last name at posteo dot de) and this is my personal blog. I intend to publish some posts about statistics and maybe other topics as well on this site.\nHere is the link to the blog posts!\nHere is the link to the github repository, where the source code of this blog can be found!"
  },
  {
    "objectID": "posts/2022-09-08-humidity/2022-09-08-humidity.html",
    "href": "posts/2022-09-08-humidity/2022-09-08-humidity.html",
    "title": "Humidity",
    "section": "",
    "text": "# devtools::install_github('sebastian-gerdes/smisc')\nlibrary('smisc')\nlibrary('knitr')\nlibrary('tibble') \nlibrary('lattice')\nplot(absolute_humidity, -20, 20, log = 'y')\nvolume_of_flat &lt;- 120 * 3\nrelative_humidity_inside &lt;- 50\nrelative_humidity_outside &lt;- 70\ntemperature_inside &lt;- 20\ntemperature_outside &lt;- 0\n(water_in_air_inside &lt;- absolute_humidity(relative_humidity = relative_humidity_inside,\n                                  temperature = temperature_inside) * volume_of_flat)\n\n[1] 3.110073\n\n(water_in_air_outside &lt;- absolute_humidity(relative_humidity = relative_humidity_outside,\n                                  temperature = temperature_outside) * volume_of_flat)\n\n[1] 1.222142\n\nwater_out &lt;- water_in_air_inside - water_in_air_outside\nConclusion: Airing once with an indoor temperature of 20 degress Celsius with a relative humidity of 50% and an outdoor temperature of 0 degrees Celsius with a relative outdoor humidity of 70% transports approx. 1.9 liters of water out of the flat, assuming a volume of 360 \\(m^3\\) of the flat.\ntemperatures &lt;- seq(from = -10, to = 35)\nrelative_humidities &lt;- seq(from = 0, to = 100, by = 5)\nresult &lt;- matrix(nrow = length(temperatures), \n                 ncol = length(relative_humidities), \n                 dimnames = list(temperatures, relative_humidities))\nfor (i in 1:length(temperatures)) {\n  for (j in 1:length(relative_humidities)){\n    result[i, j] &lt;- absolute_humidity(temperature = temperatures[i], \n                                      relative_humidity = relative_humidities[j])\n  }\n}\nmy_breaks &lt;- c(0.5, 0.7, 1, 1.5, 2, 3, 4, 6, 8, 10, 12, 16, 20, 30) / 1000\ncontour(result, levels = c(0.5, 1, 2, 3, 4, 10, 20) / 1000)\n\n\n\ncontourplot(result, at = my_breaks,\n            scales=list(x=list(rot=90)), \n            xlab = 'Temperature (degrees Celsius)',\n            ylab = 'Relative humidity (%)')"
  },
  {
    "objectID": "posts/2022-09-08-humidity/2022-09-08-humidity.html#absolute-humidity-in-1-m3-at-a-relative-humiditiy-of-100",
    "href": "posts/2022-09-08-humidity/2022-09-08-humidity.html#absolute-humidity-in-1-m3-at-a-relative-humiditiy-of-100",
    "title": "Humidity",
    "section": "Absolute humidity in 1 \\(m^3\\) at a relative humiditiy of 100%",
    "text": "Absolute humidity in 1 \\(m^3\\) at a relative humiditiy of 100%\n\n# scenario 1:\ntemperatures &lt;- seq(from = -10, to = 35, by = 5)\nrelative_humidities &lt;- seq(from = 0, to = 100, by = 10)\nresult &lt;- matrix(nrow = length(temperatures), \n                 ncol = length(relative_humidities), \n                 dimnames = list(temperatures, relative_humidities))\nfor (i in 1:length(temperatures)) {\n  for (j in 1:length(relative_humidities)){\n    result[i, j] &lt;- absolute_humidity(temperature = temperatures[i], \n                                      relative_humidity = relative_humidities[j])\n  }\n}\n\nkable(round(result, digits = 2)) \n\n\n\n\n\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n\n\n\n\n-10\n0\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n-5\n0\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n0\n0\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n\n\n5\n0\n0\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.01\n0.01\n\n\n10\n0\n0\n0.00\n0.00\n0.00\n0.00\n0.01\n0.01\n0.01\n0.01\n0.01\n\n\n15\n0\n0\n0.00\n0.00\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n\n\n20\n0\n0\n0.00\n0.01\n0.01\n0.01\n0.01\n0.01\n0.01\n0.02\n0.02\n\n\n25\n0\n0\n0.00\n0.01\n0.01\n0.01\n0.01\n0.02\n0.02\n0.02\n0.02\n\n\n30\n0\n0\n0.01\n0.01\n0.01\n0.02\n0.02\n0.02\n0.02\n0.03\n0.03\n\n\n35\n0\n0\n0.01\n0.01\n0.02\n0.02\n0.02\n0.03\n0.03\n0.04\n0.04\n\n\n\n\n# scenario 2:\ntemperatures &lt;- -10:30\nabs_humidity &lt;- round(absolute_humidity(temperature = temperatures), digits = 3)\nkable(tibble(temperatures = temperatures, abs_humidity = abs_humidity))\n\n\n\n\ntemperatures\nabs_humidity\n\n\n\n\n-10\n0.002\n\n\n-9\n0.003\n\n\n-8\n0.003\n\n\n-7\n0.003\n\n\n-6\n0.003\n\n\n-5\n0.003\n\n\n-4\n0.004\n\n\n-3\n0.004\n\n\n-2\n0.004\n\n\n-1\n0.005\n\n\n0\n0.005\n\n\n1\n0.005\n\n\n2\n0.006\n\n\n3\n0.006\n\n\n4\n0.006\n\n\n5\n0.007\n\n\n6\n0.007\n\n\n7\n0.008\n\n\n8\n0.008\n\n\n9\n0.009\n\n\n10\n0.009\n\n\n11\n0.010\n\n\n12\n0.011\n\n\n13\n0.011\n\n\n14\n0.012\n\n\n15\n0.013\n\n\n16\n0.014\n\n\n17\n0.014\n\n\n18\n0.015\n\n\n19\n0.016\n\n\n20\n0.017\n\n\n21\n0.018\n\n\n22\n0.019\n\n\n23\n0.021\n\n\n24\n0.022\n\n\n25\n0.023\n\n\n26\n0.024\n\n\n27\n0.026\n\n\n28\n0.027\n\n\n29\n0.029\n\n\n30\n0.030"
  },
  {
    "objectID": "posts/2023-03-17-mcelreath-statistical-rethinking/2023-03-17-mcelreath-statistical-rethinking.html",
    "href": "posts/2023-03-17-mcelreath-statistical-rethinking/2023-03-17-mcelreath-statistical-rethinking.html",
    "title": "Self-study: McElreath - Statistical Rethinking",
    "section": "",
    "text": "Website of the book\nStatistical rethinking lectures on youtube"
  },
  {
    "objectID": "posts/2023-03-17-mcelreath-statistical-rethinking/2023-03-17-mcelreath-statistical-rethinking.html#general-stuff",
    "href": "posts/2023-03-17-mcelreath-statistical-rethinking/2023-03-17-mcelreath-statistical-rethinking.html#general-stuff",
    "title": "Self-study: McElreath - Statistical Rethinking",
    "section": "",
    "text": "Website of the book\nStatistical rethinking lectures on youtube"
  },
  {
    "objectID": "posts/2023-03-17-mcelreath-statistical-rethinking/2023-03-17-mcelreath-statistical-rethinking.html#lecture-1",
    "href": "posts/2023-03-17-mcelreath-statistical-rethinking/2023-03-17-mcelreath-statistical-rethinking.html#lecture-1",
    "title": "Self-study: McElreath - Statistical Rethinking",
    "section": "Lecture 1",
    "text": "Lecture 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n       posterior_2  posterior_3\n [1,] 0.000000e+00 0.000000e+00\n [2,] 8.338945e-28 8.338945e-28\n [3,] 8.470870e-25 8.470870e-25\n [4,] 4.845672e-23 4.845672e-23\n [5,] 8.535940e-22 8.535940e-22\n [6,] 7.886021e-21 7.886021e-21\n [7,] 4.843656e-20 4.843656e-20\n [8,] 2.244608e-19 2.244608e-19\n [9,] 8.463615e-19 8.463615e-19\n[10,] 2.726299e-18 2.726299e-18\n[11,] 7.755988e-18 7.755988e-18\n[12,] 1.995488e-17 1.995488e-17\n[13,] 4.725159e-17 4.725159e-17\n[14,] 1.043556e-16 1.043556e-16\n[15,] 2.171886e-16 2.171886e-16\n[16,] 4.294773e-16 4.294773e-16\n[17,] 8.122594e-16 8.122594e-16\n[18,] 1.477229e-15 1.477229e-15\n[19,] 2.595039e-15 2.595039e-15\n[20,] 4.419889e-15 4.419889e-15\n[21,] 7.321976e-15 7.321976e-15\n[22,] 1.182961e-14 1.182961e-14\n[23,] 1.868315e-14 1.868315e-14\n[24,] 2.890305e-14 2.890305e-14\n[25,] 4.387490e-14 4.387490e-14\n[26,] 6.545435e-14 6.545435e-14\n[27,] 9.609549e-14 9.609549e-14\n[28,] 1.390063e-13 1.390063e-13\n[29,] 1.983357e-13 1.983357e-13\n[30,] 2.793963e-13 2.793963e-13\n[31,] 3.889283e-13 3.889283e-13\n[32,] 5.354101e-13 5.354101e-13\n[33,] 7.294212e-13 7.294212e-13\n[34,] 9.840637e-13 9.840637e-13\n[35,] 1.315450e-12 1.315450e-12\n[36,] 1.743265e-12 1.743265e-12\n[37,] 2.291406e-12 2.291406e-12\n[38,] 2.988716e-12 2.988716e-12\n[39,] 3.869812e-12 3.869812e-12\n[40,] 4.976027e-12 4.976027e-12\n[41,] 6.356464e-12 6.356464e-12\n[42,] 8.069180e-12 8.069180e-12\n[43,] 1.018252e-11 1.018252e-11\n[44,] 1.277658e-11 1.277658e-11\n[45,] 1.594485e-11 1.594485e-11\n[46,] 1.979607e-11 1.979607e-11\n[47,] 2.445616e-11 2.445616e-11\n[48,] 3.007052e-11 3.007052e-11\n[49,] 3.680640e-11 3.680640e-11\n[50,] 4.485558e-11 4.485558e-11"
  },
  {
    "objectID": "posts/2023-03-17-mcelreath-statistical-rethinking/2023-03-17-mcelreath-statistical-rethinking.html#lecture-2---bayesian-inference",
    "href": "posts/2023-03-17-mcelreath-statistical-rethinking/2023-03-17-mcelreath-statistical-rethinking.html#lecture-2---bayesian-inference",
    "title": "Self-study: McElreath - Statistical Rethinking",
    "section": "Lecture 2 - Bayesian inference",
    "text": "Lecture 2 - Bayesian inference\n\nBayesian data analysis:\n\nFor each possible explanation of the data, count all the ways data can happen.\nExplanations with more ways to produce the data are more plausibel"
  },
  {
    "objectID": "posts/2023-07-23-bicycle-comparison/2023-07-23-bicycle-comparison.html#points-to-be-represented",
    "href": "posts/2023-07-23-bicycle-comparison/2023-07-23-bicycle-comparison.html#points-to-be-represented",
    "title": "Bicycle comparison",
    "section": "Points to be represented",
    "text": "Points to be represented\n\nfront axis\nback axis\nfront beginning of tire\nback beginning of tire\nhandlebar low\nhandlebar high\nsaddle front\nsaddle back\npedals\n\nCoordinate system: Point 1 is front axis, point 2 is back of saddle.\n\nloc_names &lt;- c('front_axis',\n               'back_axis',\n               'front_beginning_of_tire',\n               'back_beginning_of_tire',\n               'handlebar_low',\n               'handlebar_high',\n               'saddle_front',\n               'saddle_back',\n               'pedals')\n\nlibrary('digitize')\nload('not_yet_formatted.Rdata')\n# save(list = c('koga_new', 'koga_old', 'hammerbacher'), file = 'not_yet_formatted.Rdata')\n# help(package = 'digitize')\n# koga_old &lt;- digitize('koga_old.jpeg')\n# koga_new &lt;- digitize('koga_new.jpeg')\n# hammerbacher &lt;- digitize('hammerbacher.jpeg')\n# \n\n\ntidy_digit &lt;- function(dat) {\n  var_name &lt;-deparse(substitute(dat))\n  dat |&gt; \n    as_tibble() |&gt; \n    mutate(var_name = var_name,\n           loc = loc_names)\n}\n\n\ntidy_digit(koga_new)\n\n# A tibble: 9 × 4\n         x        y var_name loc                    \n     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                  \n1 -0.00334 -0.00309 koga_new front_axis             \n2  1.07     0.0401  koga_new back_axis              \n3 -0.321   -0.0679  koga_new front_beginning_of_tire\n4  1.08    -0.383   koga_new back_beginning_of_tire \n5  0.207    1.02    koga_new handlebar_low          \n6  0.0870   1.12    koga_new handlebar_high         \n7  0.722    0.978   koga_new saddle_front           \n8  0.997    0.994   koga_new saddle_back            \n9  0.622   -0.0432  koga_new pedals                 \n\ntidy_digit(koga_old)\n\n# A tibble: 9 × 4\n          x        y var_name loc                    \n      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;                  \n1  3.33e-16 -0.00317 koga_old front_axis             \n2  1.14e+ 0  0.0190  koga_old back_axis              \n3 -3.33e- 1  0.0254  koga_old front_beginning_of_tire\n4  1.15e+ 0 -0.410   koga_old back_beginning_of_tire \n5  2.46e- 1  1.03    koga_old handlebar_low          \n6  1.01e- 1  1.19    koga_old handlebar_high         \n7  7.25e- 1  0.952   koga_old saddle_front           \n8  9.96e- 1  0.997   koga_old saddle_back            \n9  6.63e- 1 -0.0698  koga_old pedals                 \n\ntidy_digit(hammerbacher)\n\n# A tibble: 9 × 4\n         x        y var_name     loc                    \n     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt;                  \n1  0.00376  0.00599 hammerbacher front_axis             \n2  1.20     0.0659  hammerbacher back_axis              \n3 -0.331   -0.150   hammerbacher front_beginning_of_tire\n4  0.932   -0.275   hammerbacher back_beginning_of_tire \n5  0.256    1.32    hammerbacher handlebar_low          \n6  0.256    1.32    hammerbacher handlebar_high         \n7  0.737    0.946   hammerbacher saddle_front           \n8  0.996    0.994   hammerbacher saddle_back            \n9  0.695   -0.0419  hammerbacher pedals                 \n\ndd &lt;- bind_rows(tidy_digit(koga_old), \n          tidy_digit(koga_new),\n          tidy_digit(hammerbacher))\n\ndd |&gt; \n  ggplot(aes(x, y, color = var_name)) +\n  geom_point()\n\n\n\nwrite_csv(dd, 'bikes.csv')"
  },
  {
    "objectID": "posts/2023-07-23-bicycle-comparison/2023-07-23-bicycle-comparison.html#todo",
    "href": "posts/2023-07-23-bicycle-comparison/2023-07-23-bicycle-comparison.html#todo",
    "title": "Bicycle comparison",
    "section": "Todo:",
    "text": "Todo:\n\nmeasure distance between axes\nmeasure distance between"
  },
  {
    "objectID": "posts/2023-08-05-regularization-in-r/2023-08-05-regularization-in-r.html",
    "href": "posts/2023-08-05-regularization-in-r/2023-08-05-regularization-in-r.html",
    "title": "Regularization in R",
    "section": "",
    "text": "ridge regression\nlasso regression\nelastic net\n\n\nlibrary('tidyverse')\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(1)"
  },
  {
    "objectID": "posts/2023-02_21-manual-colors-in-ggplot2/2023-02_21-manual-colors-in-ggplot2.html",
    "href": "posts/2023-02_21-manual-colors-in-ggplot2/2023-02_21-manual-colors-in-ggplot2.html",
    "title": "Manual colors in ggplot2",
    "section": "",
    "text": "After several instances of “relearning” how to use custom colour palettes in R, I have written this post in order to save some time when I want to use custom colours the next time.\n\nlibrary(ggplot2)\niris |&gt; ggplot(aes(Sepal.Length, Sepal.Width, colour = Species)) + \n  geom_point(size = 3) +\n  scale_colour_manual(values = c('lightgray', 'blue', 'black'), \n                      aesthetics = c('colour', 'fill'))"
  },
  {
    "objectID": "posts/2023-07-03-python-pandas/2023-07-03-python-pandas.html",
    "href": "posts/2023-07-03-python-pandas/2023-07-03-python-pandas.html",
    "title": "Python pandas",
    "section": "",
    "text": "pandas - Tutorials\nAccess R in Python chunk:\n\n\nr[\"mtcars\"]\n\n                      mpg  cyl   disp     hp  drat  ...   qsec   vs   am  gear  carb\nMazda RX4            21.0  6.0  160.0  110.0  3.90  ...  16.46  0.0  1.0   4.0   4.0\nMazda RX4 Wag        21.0  6.0  160.0  110.0  3.90  ...  17.02  0.0  1.0   4.0   4.0\nDatsun 710           22.8  4.0  108.0   93.0  3.85  ...  18.61  1.0  1.0   4.0   1.0\nHornet 4 Drive       21.4  6.0  258.0  110.0  3.08  ...  19.44  1.0  0.0   3.0   1.0\nHornet Sportabout    18.7  8.0  360.0  175.0  3.15  ...  17.02  0.0  0.0   3.0   2.0\nValiant              18.1  6.0  225.0  105.0  2.76  ...  20.22  1.0  0.0   3.0   1.0\nDuster 360           14.3  8.0  360.0  245.0  3.21  ...  15.84  0.0  0.0   3.0   4.0\nMerc 240D            24.4  4.0  146.7   62.0  3.69  ...  20.00  1.0  0.0   4.0   2.0\nMerc 230             22.8  4.0  140.8   95.0  3.92  ...  22.90  1.0  0.0   4.0   2.0\nMerc 280             19.2  6.0  167.6  123.0  3.92  ...  18.30  1.0  0.0   4.0   4.0\nMerc 280C            17.8  6.0  167.6  123.0  3.92  ...  18.90  1.0  0.0   4.0   4.0\nMerc 450SE           16.4  8.0  275.8  180.0  3.07  ...  17.40  0.0  0.0   3.0   3.0\nMerc 450SL           17.3  8.0  275.8  180.0  3.07  ...  17.60  0.0  0.0   3.0   3.0\nMerc 450SLC          15.2  8.0  275.8  180.0  3.07  ...  18.00  0.0  0.0   3.0   3.0\nCadillac Fleetwood   10.4  8.0  472.0  205.0  2.93  ...  17.98  0.0  0.0   3.0   4.0\nLincoln Continental  10.4  8.0  460.0  215.0  3.00  ...  17.82  0.0  0.0   3.0   4.0\nChrysler Imperial    14.7  8.0  440.0  230.0  3.23  ...  17.42  0.0  0.0   3.0   4.0\nFiat 128             32.4  4.0   78.7   66.0  4.08  ...  19.47  1.0  1.0   4.0   1.0\nHonda Civic          30.4  4.0   75.7   52.0  4.93  ...  18.52  1.0  1.0   4.0   2.0\nToyota Corolla       33.9  4.0   71.1   65.0  4.22  ...  19.90  1.0  1.0   4.0   1.0\nToyota Corona        21.5  4.0  120.1   97.0  3.70  ...  20.01  1.0  0.0   3.0   1.0\nDodge Challenger     15.5  8.0  318.0  150.0  2.76  ...  16.87  0.0  0.0   3.0   2.0\nAMC Javelin          15.2  8.0  304.0  150.0  3.15  ...  17.30  0.0  0.0   3.0   2.0\nCamaro Z28           13.3  8.0  350.0  245.0  3.73  ...  15.41  0.0  0.0   3.0   4.0\nPontiac Firebird     19.2  8.0  400.0  175.0  3.08  ...  17.05  0.0  0.0   3.0   2.0\nFiat X1-9            27.3  4.0   79.0   66.0  4.08  ...  18.90  1.0  1.0   4.0   1.0\nPorsche 914-2        26.0  4.0  120.3   91.0  4.43  ...  16.70  0.0  1.0   5.0   2.0\nLotus Europa         30.4  4.0   95.1  113.0  3.77  ...  16.90  1.0  1.0   5.0   2.0\nFord Pantera L       15.8  8.0  351.0  264.0  4.22  ...  14.50  0.0  1.0   5.0   4.0\nFerrari Dino         19.7  6.0  145.0  175.0  3.62  ...  15.50  0.0  1.0   5.0   6.0\nMaserati Bora        15.0  8.0  301.0  335.0  3.54  ...  14.60  0.0  1.0   5.0   8.0\nVolvo 142E           21.4  4.0  121.0  109.0  4.11  ...  18.60  1.0  1.0   4.0   2.0\n\n[32 rows x 11 columns]\n\n\n\nAccess Python object in R chunk\n\n\nxyz = 10\n\n\nlibrary(reticulate)\npy$xyz\n\n[1] 10\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\niris = pd.read_csv('iris.csv')\niris['Sepal.Length'].describe()\n\ncount    150.000000\nmean       5.843333\nstd        0.828066\nmin        4.300000\n25%        5.100000\n50%        5.800000\n75%        6.400000\nmax        7.900000\nName: Sepal.Length, dtype: float64\n\niris['Sepal.Width'].describe()\n\ncount    150.000000\nmean       3.057333\nstd        0.435866\nmin        2.000000\n25%        2.800000\n50%        3.000000\n75%        3.300000\nmax        4.400000\nName: Sepal.Width, dtype: float64\n\niris['total_length'] = iris['Sepal.Length'] + iris['Sepal.Width']\niris['total_length'].describe()\n\ncount    150.000000\nmean       8.900667\nstd        0.889272\nmin        6.800000\n25%        8.300000\n50%        8.850000\n75%        9.575000\nmax       11.700000\nName: total_length, dtype: float64\n\n# two identical results:\niris.loc[iris['total_length'] &gt; 10, 'total_length']\n\n15     10.1\n50     10.2\n102    10.1\n105    10.6\n107    10.2\n109    10.8\n117    11.5\n118    10.3\n120    10.1\n122    10.5\n125    10.4\n129    10.2\n130    10.2\n131    11.7\n135    10.7\nName: total_length, dtype: float64\n\niris[iris['total_length'] &gt; 10]['total_length']\n\n15     10.1\n50     10.2\n102    10.1\n105    10.6\n107    10.2\n109    10.8\n117    11.5\n118    10.3\n120    10.1\n122    10.5\n125    10.4\n129    10.2\n130    10.2\n131    11.7\n135    10.7\nName: total_length, dtype: float64\n\n\niris.loc[iris['total_length'] &gt; 10, 'total_length'] = 100"
  },
  {
    "objectID": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html",
    "href": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html",
    "title": "How bad are bananas",
    "section": "",
    "text": "Recently, I have come a across a book that I find really wonderful: “How bad are bananas” by Mike Berners-Lee Berners-Lee (2020).\nI would really like to use some of the data for a school project (of course citing the source in an adequate manner). It would be nice to use the data to develop a simple footprint calculator that is available online. Explicite permission from the author to use the data from his book with appropriate inidication of the data source is pending, however, with proper citation it should be okay to use the data I figure."
  },
  {
    "objectID": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#reading-notes",
    "href": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#reading-notes",
    "title": "How bad are bananas",
    "section": "Reading notes",
    "text": "Reading notes\n\nCarbon dioxide equivalent (CO2e):\n\namount of CO2 that would have the same impact over a time of 100 years\nTotal impact in the UK\n\ncarbon dioxide 81%\nmethane 11%\nnitrous oxide 5%\nrefrigerant and other gases 3%\n\n\nDirect and indirect emissions\nfold-width of confidence interval-like interval: approx: 10-fold\naverage footprint of a UK person: 13 tonnes per year\nglobal average: 7 tonnes per year\nA large cheeseburger (3.2 kg CO2e) corresponds to approx. 6 hours of a 5 tonne year"
  },
  {
    "objectID": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#my-own-thoughts",
    "href": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#my-own-thoughts",
    "title": "How bad are bananas",
    "section": "My own thoughts",
    "text": "My own thoughts\n\nCarbon responsibility should work like this:\n\nThe total CO2e emitted in the entire world should be estimated\nThe responsibilities should be attributed (algorithmically)"
  },
  {
    "objectID": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#issues",
    "href": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#issues",
    "title": "How bad are bananas",
    "section": "Issues",
    "text": "Issues\n\nIs there a German translation?\nI should buy paper copies!\nMaybe initiate a school-project\nAsk for permission about the book: info@howbadarebananas.com\nIs there a good webpage belonging to the book?\nShiny app for “how much time of a 5 tonne year does this correspond to?”"
  },
  {
    "objectID": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#table",
    "href": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#table",
    "title": "How bad are bananas",
    "section": "Table",
    "text": "Table\nHere, I started to collect some of the data in the book and put them in a table:\n\n\n\n\n\n\n\n\n\nCO2e\nItem\n\n\n\n\n0.20\nPint of tap water\n\n\n0.03\nspam mail picked up by filter\n\n\n0.20\nshort e-mail from phone to phone\n\n\n0.30\nshort e-mail from laptop to laptop\n\n\n17.00\nlong email that takes 10 minutes to write and 3 minutes to read, sent from laptop to laptop\n\n\n26.00\nan email that takes you 10 minutes to write, sent to 100 people, 99 of whom take 3 seconds to realise they should ignore it and on the whom reads it\n\n\n0.50\none simple google search\n\n\n5.60\n5 minutes web browsing from a smartphone\n\n\n8.20\n5 minutes web browsing from a laptop\n\n\n0.80\nsingle text message\n\n\n3.00\nvery lightweight plastic carrier bag\n\n\n10.00\nheavier supermarket bag\n\n\n50.00\nheavweight ‘bag for life’\n\n\n2.00\nDyson Airblade\n\n\n10.00\none paper towel\n\n\n11.00\nstandard electric dryer\n\n\n12.00\nrecycled and lightweight paper carrier bag\n\n\n80.00\na fashion paper bag from mainly virgin paper\n\n\n8.00\nquick expert ironing of a shirt\n\n\n14.00\naverage ironing\n\n\n40.00\nironing of a thorougly crumped shirt\n\n\n2.00\none hour zoom call on 13-inch Mac-Book Pro\n\n\n10.00\none hour zoom call on an averagely efficient laptop\n\n\n50.00\none hour zoom call on desktop computer\n\n\n28.00\n100g portion of carrots – local, in season, full-size varieties\n\n\n83.00\n100g portion of carrots – local, in season, baby carrots\n\n\n90.00\n100g portion of carrots – full-size varieties, shipped within Europe"
  },
  {
    "objectID": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#plots",
    "href": "posts/2023-02-27-how-bad-are-bananas/2023-02-27-how-bad-are-bananas.html#plots",
    "title": "How bad are bananas",
    "section": "Plots",
    "text": "Plots"
  },
  {
    "objectID": "posts/2023-08-03-kaplan-meier-plots/2023-08-03-kaplan-meier-plots.html",
    "href": "posts/2023-08-03-kaplan-meier-plots/2023-08-03-kaplan-meier-plots.html",
    "title": "Kaplan Meier plots",
    "section": "",
    "text": "Goal: draw facetted Kaplan-Meier-plots with p-value"
  },
  {
    "objectID": "posts/2023-08-03-kaplan-meier-plots/2023-08-03-kaplan-meier-plots.html#with-ggquickeda",
    "href": "posts/2023-08-03-kaplan-meier-plots/2023-08-03-kaplan-meier-plots.html#with-ggquickeda",
    "title": "Kaplan Meier plots",
    "section": "With ggquickeda",
    "text": "With ggquickeda\n\nlibrary('tidyverse')\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary('survival')\nlibrary('ggquickeda')\n\n\nAttaching package: 'ggquickeda'\n\nThe following object is masked from 'package:base':\n\n    +\n\ntext_to_add &lt;- \n  tribble(~ sex, ~ text, ~ x, ~ y, \n          'Male', 'p = 0.1', 3000, 0.95,\n          'Female', 'p = 0.03', 3000, 0.95)\n\n\n\ncolon |&gt; \n  mutate(sex = case_when(\n    sex == 1 ~ 'Male',\n    sex == 0 ~ 'Female')) |&gt; \n  mutate(perfor = factor(perfor)) |&gt; \n  \n  ggplot(aes(time = time, status = status, colour = perfor)) + \n  facet_wrap(vars(sex)) +\n  geom_km() + geom_kmticks() + \n  geom_label(mapping = aes(label = text, \n                           x = x, \n                           y = y), \n             data = text_to_add, \n             inherit.aes = FALSE) +\n  labs(x = 'Time', y = 'Estimated survival')\n\nWarning: The following aesthetics were dropped during statistical transformation: status\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: status\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: status\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: status\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\nWarning: Using the `size` aesthetic with geom_path was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "posts/2023-08-03-kaplan-meier-plots/2023-08-03-kaplan-meier-plots.html#with-survminer",
    "href": "posts/2023-08-03-kaplan-meier-plots/2023-08-03-kaplan-meier-plots.html#with-survminer",
    "title": "Kaplan Meier plots",
    "section": "With survminer",
    "text": "With survminer\n\nDo faceting with survminer\n\nlibrary('survminer')\n\nLoading required package: ggpubr\n\n\n\nAttaching package: 'survminer'\n\n\nThe following object is masked from 'package:survival':\n\n    myeloma\n\nfit &lt;- survfit( Surv(time, status) ~ sex + rx, data = colon )\nggsurvplot_facet(fit, colon, facet.by = c('rx', 'perfor'), \n                 pval = TRUE, ggtheme = theme_grey())\n\nWarning: `as.tibble()` was deprecated in tibble 2.0.0.\nℹ Please use `as_tibble()` instead.\nℹ The signature and semantics have changed, see `?as_tibble`.\nℹ The deprecated feature was likely used in the survminer package.\n  Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\n\nWarning: `select_()` was deprecated in dplyr 0.7.0.\nℹ Please use `select()` instead.\nℹ The deprecated feature was likely used in the survminer package.\n  Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\n\n\n\n\n\n\nExtract plot object so it can be modified in a more usual ggplot2 style\n\nfit &lt;- survfit( Surv(time, status) ~ sex + rx + perfor, data = colon )\ngg &lt;- ggsurvplot(fit, data = colon, color = 'perfor')\ngg$plot + theme_grey() + labs(color = 'perfor', x = 'Time [days]') + facet_wrap(~ sex + rx, labeller = label_both)\n\n\n\n\n\nfit &lt;- survfit( Surv(time, status) ~ sex + perfor, data = colon )\ngg &lt;- ggsurvplot(fit, data = colon, color = 'perfor', pval = TRUE, facet.by = 'sex')\n\n\n\nShow possible problem in survminer\n\nAfter facetting, the p-values seem not be correct anymore\n\n\nfit &lt;- survfit(Surv(time, status) ~ sex + rx, data = colon)\ngg &lt;- ggsurvplot(fit, pval = TRUE)\ngg\n\n\n\ngg$plot + facet_wrap(vars(rx))\n\n\n\ngg$plot + facet_wrap(vars(rx, sex))\n\n\n\n\n\n\nTry to help someone\n\nLink\n\n\nlibrary('survival')\nlibrary('tidyverse')\nlibrary('survminer')\n\ntext_to_add &lt;- \n  tribble(~ sex, ~ text, ~ x, ~ y, \n          0, 'Some text', 1000, 0.1,\n          1, 'Some other text', 1000, 0.1)\n\nfit &lt;- survfit(Surv(time, status) ~ sex, data = colon)\ngg &lt;- ggsurvplot(fit)\ngg$plot + facet_wrap(vars(sex)) + theme_gray() + \n  geom_label(aes(x = x, y = y, label = text),\n             text_to_add, inherit.aes = FALSE)\n\n\n\n\n\ngg &lt;- ggsurvplot(fit, palette = c('black', 'black'))\ngg$plot + facet_wrap(vars(sex)) + theme_gray() + \n  geom_label(aes(x = x, y = y, label = text),\n             text_to_add, inherit.aes = FALSE) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/2023-07-12-pie-chart/2023-07-12-pie-chart.html",
    "href": "posts/2023-07-12-pie-chart/2023-07-12-pie-chart.html",
    "title": "Pie chart",
    "section": "",
    "text": "To build a simple pie chart:\n\nlibrary('tidyverse')\n\nlibrary(ggplot2)\n\ndf &lt;- data.frame(value = c(10, 23, 15, 18),\n                 Group = paste0(\"G\", 1:4))\n\nggplot(df, aes(x = \"\", y = value, fill = Group)) +\n  geom_col(color = \"black\") +\n  ylab('') + \n  geom_text(aes(label = value),\n            position = position_stack(vjust = 0.5)) +\n  coord_polar(theta = \"y\") + \n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        panel.grid  = element_blank())"
  },
  {
    "objectID": "posts/2023-01-10-sustainibility-data/2023-01-10-sustainibility-data.html",
    "href": "posts/2023-01-10-sustainibility-data/2023-01-10-sustainibility-data.html",
    "title": "Sustainibility data",
    "section": "",
    "text": "Quelle: Blog\nEnergie pro Jahr:\n\n\n# Wie viele Watt pro qm Erdoberfläche liefert die Sonne im Mittel:\nwatt_per_sqm &lt;- 1340\n# Wie groß ist die Oberfläche der Erde:\nsurface_earth_sqm &lt;- 5.1e14\n# Wie viel Sonnenenergie erreicht die Erde pro Jahr:\nenergy_sun_per_year_joule &lt;- watt_per_sqm * surface_earth_sqm * 60 * 60 * 24 * 365.25\n(energy_per_year_kwh &lt;- energy_sun_per_year_joule / 3.6e6)\n\n[1] 5.990684e+18"
  },
  {
    "objectID": "posts/2023-01-10-sustainibility-data/2023-01-10-sustainibility-data.html#wie-viel-energie-liefert-die-sonne",
    "href": "posts/2023-01-10-sustainibility-data/2023-01-10-sustainibility-data.html#wie-viel-energie-liefert-die-sonne",
    "title": "Sustainibility data",
    "section": "",
    "text": "Quelle: Blog\nEnergie pro Jahr:\n\n\n# Wie viele Watt pro qm Erdoberfläche liefert die Sonne im Mittel:\nwatt_per_sqm &lt;- 1340\n# Wie groß ist die Oberfläche der Erde:\nsurface_earth_sqm &lt;- 5.1e14\n# Wie viel Sonnenenergie erreicht die Erde pro Jahr:\nenergy_sun_per_year_joule &lt;- watt_per_sqm * surface_earth_sqm * 60 * 60 * 24 * 365.25\n(energy_per_year_kwh &lt;- energy_sun_per_year_joule / 3.6e6)\n\n[1] 5.990684e+18"
  },
  {
    "objectID": "posts/2023-01-10-sustainibility-data/2023-01-10-sustainibility-data.html#weltenergiebedarf",
    "href": "posts/2023-01-10-sustainibility-data/2023-01-10-sustainibility-data.html#weltenergiebedarf",
    "title": "Sustainibility data",
    "section": "Weltenergiebedarf",
    "text": "Weltenergiebedarf\n\nWikipedia: Weltenergiebedarf 574 Exajoule = 5.74e20\n\n\n# Weltenergiebedarf:\nworld_energy_consumption_joule &lt;- 5.74e20\n(world_energy_consumption_kwh &lt;- world_energy_consumption_joule / 3.6e6)\n\n[1] 1.594444e+14\n\n# Wie viel mal so viel Energie liefert die Sonne:\nfac &lt;- energy_sun_per_year_joule / world_energy_consumption_joule\n\nDie Welt liefert also ca. 3.76e+04 mal so viel Energie wie von den Menschen verbraucht wird.\n\n# angenommene Effizienz von Photovoltaikanlagen:\nassumed_efficiency &lt;- 0.1\n# Welcher Anteil der Erde müsste mit 'mittleren' PV-Anlagen bedeckt sein, um den Weltenergiebedarf mit PV-Anlagen abdecken zu können:\nfraction_earth &lt;- 1 / (assumed_efficiency * fac)\n# Weltbevölkerung:\npopulation_earth &lt;- 8e9\n# Wie viel \n(pv_per_person &lt;- surface_earth_sqm * fraction_earth / population_earth)\n\n[1] 16.96732"
  },
  {
    "objectID": "posts/2023-01-10-sustainibility-data/2023-01-10-sustainibility-data.html#deutschland",
    "href": "posts/2023-01-10-sustainibility-data/2023-01-10-sustainibility-data.html#deutschland",
    "title": "Sustainibility data",
    "section": "Deutschland",
    "text": "Deutschland\n\nWikipedia: Weltenergiebedarf 574 Exajoule = 5.74e20\n\n\n(energy_consumption_germany_joule &lt;- 3.3 * 1e3 * 1e9 * 3.6e6)\n\n[1] 1.188e+19\n\nsurface_germany_sqm &lt;- 3e5 * 1e6\nenergy_sun_germany_year_joule &lt;- watt_per_sqm * surface_germany_sqm * 60 * 60 * 24 * 365.24\n# Anteil der Fläche in Deutschland, der mit PV-Anlagen ausgestatted sein müsste:\nenergy_consumption_germany_joule / (energy_sun_germany_year_joule * assumed_efficiency)\n\n[1] 0.009364796"
  },
  {
    "objectID": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html",
    "href": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html",
    "title": "Leo’s project",
    "section": "",
    "text": "In hematological cancers, blood stem cells can be transferred to a patient after chemothery. The blood stem cells may proliferate in the patient and populate the patient’s peripheral blood afterwards.\nChimerism is defined as the ratio of the number of blood cells originating from the transplanted cells and the total number of blood cells in the patient’s blood.\nHere, a sigmoidal model for the temporal development of chimerism after transplantion is introduced and in silico data are simulated.\nLeo also posted a question about this model on the STAN forum: https://discourse.mc-stan.org/t/bad-diagnostics-with-multilevel-models/29091?u=leonhardschwager\n\n\n\n\nlibrary('tidyverse')\nset.seed(1)\n\n\n\n\n\nSource: https://en.wikipedia.org/wiki/Logistic_function\n\n\ncalc_chimerism &lt;- function(day, d = 1, b = 1, e = 5) {\n# day: for which day the chimerism shall be calculated\n# d: maximum value of the curve\n# b: the logistic growth rate or steepness of the curve\n# e:  location of the midpoint of the sigmoid function\n  d / (1 + exp(-b * (day - e)))\n}\n\n# plot the function with default values:\nggplot() + stat_function(fun = calc_chimerism) + \n  xlim(0, 10) + labs(x = 'Time', y = 'Chimerism')\n\n\n\n\n\n\n\n\nnr_of_patients &lt;- 10\nnr_of_measurements &lt;- 10 # per patient\nd &lt;- runif(n = nr_of_patients, min = 0.6, max = 1)\nb &lt;- rnorm(n = nr_of_patients, mean = 1, sd = 0.1)\ne &lt;- runif(n = nr_of_patients, min = 5, max = 15)\n\n\n\n\n\ndat &lt;- tibble(patient_id = factor(1:nr_of_patients), \n              d = d, \n              b = b, \n              e = e) %&gt;%\n  expand_grid(measurement = 1:nr_of_measurements)\n\n# compute timepoints symetrical equidistant \n# in both directions of the sigmoid midpoint \n# of the curve:\ndat$day &lt;- dat$e * 2 * dat$measurement / nr_of_measurements\n\n\n\n\n\ndat$chimerism &lt;- calc_chimerism(day = dat$day, \n                                d = dat$d, \n                                b = dat$b, \n                                e = dat$e)\n\n\n\n\n\nggplot(dat, aes(day, chimerism)) + \n  geom_point() + geom_line() + \n  facet_wrap(~ patient_id, ncol = 3, scales = 'free_x') + \n  xlim(0, NA) + ylim(0, 1)\n\n\n\nggplot(dat, aes(day, chimerism, color = patient_id)) + \n  geom_point() + geom_line() + \n  ylim(0, 1)"
  },
  {
    "objectID": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#background",
    "href": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#background",
    "title": "Leo’s project",
    "section": "",
    "text": "In hematological cancers, blood stem cells can be transferred to a patient after chemothery. The blood stem cells may proliferate in the patient and populate the patient’s peripheral blood afterwards.\nChimerism is defined as the ratio of the number of blood cells originating from the transplanted cells and the total number of blood cells in the patient’s blood.\nHere, a sigmoidal model for the temporal development of chimerism after transplantion is introduced and in silico data are simulated.\nLeo also posted a question about this model on the STAN forum: https://discourse.mc-stan.org/t/bad-diagnostics-with-multilevel-models/29091?u=leonhardschwager"
  },
  {
    "objectID": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#initialization",
    "href": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#initialization",
    "title": "Leo’s project",
    "section": "",
    "text": "library('tidyverse')\nset.seed(1)"
  },
  {
    "objectID": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#definition-of-model-function-to-predict-chimerism",
    "href": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#definition-of-model-function-to-predict-chimerism",
    "title": "Leo’s project",
    "section": "",
    "text": "Source: https://en.wikipedia.org/wiki/Logistic_function\n\n\ncalc_chimerism &lt;- function(day, d = 1, b = 1, e = 5) {\n# day: for which day the chimerism shall be calculated\n# d: maximum value of the curve\n# b: the logistic growth rate or steepness of the curve\n# e:  location of the midpoint of the sigmoid function\n  d / (1 + exp(-b * (day - e)))\n}\n\n# plot the function with default values:\nggplot() + stat_function(fun = calc_chimerism) + \n  xlim(0, 10) + labs(x = 'Time', y = 'Chimerism')"
  },
  {
    "objectID": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#set-parameters",
    "href": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#set-parameters",
    "title": "Leo’s project",
    "section": "",
    "text": "nr_of_patients &lt;- 10\nnr_of_measurements &lt;- 10 # per patient\nd &lt;- runif(n = nr_of_patients, min = 0.6, max = 1)\nb &lt;- rnorm(n = nr_of_patients, mean = 1, sd = 0.1)\ne &lt;- runif(n = nr_of_patients, min = 5, max = 15)"
  },
  {
    "objectID": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#construct-independent-values",
    "href": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#construct-independent-values",
    "title": "Leo’s project",
    "section": "",
    "text": "dat &lt;- tibble(patient_id = factor(1:nr_of_patients), \n              d = d, \n              b = b, \n              e = e) %&gt;%\n  expand_grid(measurement = 1:nr_of_measurements)\n\n# compute timepoints symetrical equidistant \n# in both directions of the sigmoid midpoint \n# of the curve:\ndat$day &lt;- dat$e * 2 * dat$measurement / nr_of_measurements"
  },
  {
    "objectID": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#apply-model-function-to-calculate-dependendant-values",
    "href": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#apply-model-function-to-calculate-dependendant-values",
    "title": "Leo’s project",
    "section": "",
    "text": "dat$chimerism &lt;- calc_chimerism(day = dat$day, \n                                d = dat$d, \n                                b = dat$b, \n                                e = dat$e)"
  },
  {
    "objectID": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#plot-the-simulated-data",
    "href": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#plot-the-simulated-data",
    "title": "Leo’s project",
    "section": "",
    "text": "ggplot(dat, aes(day, chimerism)) + \n  geom_point() + geom_line() + \n  facet_wrap(~ patient_id, ncol = 3, scales = 'free_x') + \n  xlim(0, NA) + ylim(0, 1)\n\n\n\nggplot(dat, aes(day, chimerism, color = patient_id)) + \n  geom_point() + geom_line() + \n  ylim(0, 1)"
  },
  {
    "objectID": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#fitting-a-simple-model",
    "href": "posts/2022-10-16-leo-s-project/2022-10-16-leo-s-project.html#fitting-a-simple-model",
    "title": "Leo’s project",
    "section": "Fitting a simple model",
    "text": "Fitting a simple model\n\nlibrary('brms')\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.19.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\n# help(package = 'brms')\n# prior1 &lt;- prior(normal(1, 0.1), nlpar = ')"
  },
  {
    "objectID": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html",
    "href": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html",
    "title": "SIR-like model with deSolve in deterministic and stochastic version",
    "section": "",
    "text": "Here, an model based on the SIR model is presented in a deterministic version and in a non-deterministic version. For solving the ODEs (deterministic version) and the stochastic difference equations (stochastic version), the R package deSolve is used."
  },
  {
    "objectID": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#summary",
    "href": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#summary",
    "title": "SIR-like model with deSolve in deterministic and stochastic version",
    "section": "",
    "text": "Here, an model based on the SIR model is presented in a deterministic version and in a non-deterministic version. For solving the ODEs (deterministic version) and the stochastic difference equations (stochastic version), the R package deSolve is used."
  },
  {
    "objectID": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#defining-and-solving-a-simple-ode-model",
    "href": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#defining-and-solving-a-simple-ode-model",
    "title": "SIR-like model with deSolve in deterministic and stochastic version",
    "section": "Defining and solving a simple ODE model",
    "text": "Defining and solving a simple ODE model\nA simplified SIR model: \\[ \\frac{dS}{dt} = -\\frac{\\beta I S}{N} \\]\n\\[ \\frac{dI}{dt} = \\frac{\\beta I S}{N} - \\gamma I \\]\n\\[ \\frac{dR}{dt} = \\gamma I \\]"
  },
  {
    "objectID": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#loading-the-required-libraries-and-defining-utility-functions",
    "href": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#loading-the-required-libraries-and-defining-utility-functions",
    "title": "SIR-like model with deSolve in deterministic and stochastic version",
    "section": "Loading the required libraries and defining utility functions",
    "text": "Loading the required libraries and defining utility functions\n\nlibrary('ggplot2')\nlibrary('tidyverse')\nlibrary('deSolve')\ntheme_set(theme_bw())\n\n\nplot_deSolve_result &lt;- function(result) {\n  tib &lt;- as_tibble(unclass(result))  \n  vars &lt;- colnames(result)[-1]\n  long &lt;- pivot_longer(tib, cols = all_of(vars))\n  long$name &lt;- factor(long$name, levels = c('S', 'I', 'R'))\n  ggplot(long, aes(time, value, colour = name)) + \n    geom_line() + \n    labs(colour = 'Compartment', \n         x = 'Time',\n         y = 'Fraction')\n}"
  },
  {
    "objectID": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#definition-of-the-parameters",
    "href": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#definition-of-the-parameters",
    "title": "SIR-like model with deSolve in deterministic and stochastic version",
    "section": "Definition of the parameters",
    "text": "Definition of the parameters\n\nparameters &lt;- list(beta = 0.2, \n                   gamma = .01,\n                   lambda = 0)\n\ninfected_initial &lt;- 0.01\ninitial_condition &lt;- \n  c(S = 1 - infected_initial,\n    I = infected_initial,\n    R = 0)\n\nt_max &lt;- 100\nstep_length &lt;- t_max / 100\ntimepoints &lt;- seq(0, 150, length.out = 100)"
  },
  {
    "objectID": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#deterministic-model",
    "href": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#deterministic-model",
    "title": "SIR-like model with deSolve in deterministic and stochastic version",
    "section": "Deterministic model",
    "text": "Deterministic model\n\nsir_ode_deterministic &lt;- function(t, state, pars) {\n  with(as.list(c(state, pars)), {\n    dS &lt;- - beta * I * S + lambda * R\n    dI &lt;- beta * I * S - gamma * I\n    dR &lt;- gamma * I - lambda * R\n    return(list(c(dS = dS, dI = dI, dR = dR)))\n  }) \n}\n\n\nsir_solution_deterministic &lt;- \n  ode(y = initial_condition, \n      times = timepoints,\n      func = sir_ode_deterministic,\n      parms = parameters)\nplot_deSolve_result(sir_solution_deterministic)"
  },
  {
    "objectID": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#stochastic-model",
    "href": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#stochastic-model",
    "title": "SIR-like model with deSolve in deterministic and stochastic version",
    "section": "Stochastic model",
    "text": "Stochastic model\n\nsir_ode_stochastic &lt;- function(t, state, pars) {\n  with(as.list(c(state, pars)), {\n    S_new &lt;- S - beta * I * S * step_length\n    I_new &lt;- I + (beta * I * S - gamma * I) * step_length\n    R_new &lt;- R + gamma * I * step_length\n    \n    new_values &lt;-c(S = S_new, I = I_new, R = R_new)\n    new_values &lt;- new_values * rlnorm(3, meanlog = 0, sdlog = 0.05)\n    new_values &lt;- new_values / sum(new_values)\n    return(list(new_values))\n  })\n}\n\nsir_solution_stochastic &lt;- \n  ode(y = initial_condition, \n      times = timepoints,\n      func = sir_ode_stochastic,\n      parms = parameters, \n      method = 'iteration')\n\nplot_deSolve_result(sir_solution_stochastic)"
  },
  {
    "objectID": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#conclusion",
    "href": "posts/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version/2022-10-04-sir-like-model-with-desolve-in-deterministic-and-stochastic-version.html#conclusion",
    "title": "SIR-like model with deSolve in deterministic and stochastic version",
    "section": "Conclusion",
    "text": "Conclusion\nThe R package deSolve can be used to solve deterministic ODEs and stochastic difference equations."
  },
  {
    "objectID": "posts/2023-07-12-model-diagnostics-for-logistic-regression/2023-07-12-model-diagnostics-for-logistic-regression.html",
    "href": "posts/2023-07-12-model-diagnostics-for-logistic-regression/2023-07-12-model-diagnostics-for-logistic-regression.html",
    "title": "Model diagnostics for logistic regression",
    "section": "",
    "text": "library('tidyverse')\nlibrary('faraway')\nset.seed(1)\n\n\nn &lt;- 100\ndat &lt;- tibble(x1 = rnorm(n), \n              x2 = rnorm(n),\n              x3 = rnorm(n),)\n\nX &lt;- model.matrix(~ ., data = dat)\nbeta &lt;- rnorm(ncol(X))\n\n\n\ndat &lt;- dat |&gt; \n  mutate(mu = X %*% beta,\n         p = plogis(mu),\n         result = rbinom(n, 1, p))\n\nfitted_model &lt;- glm(result ~ x1 + x2 + x3, data = dat, family = binomial())\n\nplot(fitted_model)\n\n\n\n\n\n\n\n\n\n\n\n\nprplot(fitted_model, 1)\n\n\n\nprplot(fitted_model, 2)\n\n\n\nprplot(fitted_model, 3)"
  },
  {
    "objectID": "posts/2022-09-08-timeline/2022-09-08-timeline.html",
    "href": "posts/2022-09-08-timeline/2022-09-08-timeline.html",
    "title": "Timeline",
    "section": "",
    "text": "format_ods_data_frame &lt;- \n  function(dat, \n           colnames = TRUE, \n           type_definition = TRUE) {\n    \n    result &lt;- dat\n    \n    if (colnames) {\n      result &lt;- result[-1, ]\n      colnames(result) &lt;- dat[1, ]\n    }\n    \n    if (type_definition) {\n      type_defs &lt;- result[1, ]\n      result &lt;- result[-1, ]\n      for (i in 1:ncol(dat)) {\n        if (type_defs[i] == 'Factor') {\n          result[result[, i] == '', i] &lt;- NA\n          result[, i] &lt;- as.factor(result[, i])  \n        }\n        if (type_defs[i] == 'Numeric')\n          result[, i] &lt;- as.numeric(result[, i])\n        if (type_defs[i] == 'Character')\n          result[, i] &lt;- as.character(result[, i])\n      }\n    }\n    rownames(result) &lt;- NULL\n    return(result)\n  }\n\nplot_timeline &lt;- function(label,\n                          begin, \n                          end, \n                          group = NULL) {\n  n &lt;- length(label)\n  if(is.null(group)) {\n    group &lt;- 1:n\n  } else {\n    group &lt;- as.numeric(group)\n  }\n  timeline_colors &lt;- rainbow(max(group), \n                             s = 0.1)\n  par(mar = c(3.1, 2.1, 2.1, 2.1))\n  plot(0, type = 'n',\n       ylab = '', xlab = '', yaxt = 'n',\n       ylim = c(n, 0), \n       xlim = range(begin, end))\n  grid()\n  rect(xleft = begin,\n       xright = end,\n       ybottom = 0:(n - 1) + 0.1,\n       ytop = 1:n - 0.1,\n       col = timeline_colors[group])\n  text((begin + end) / 2,\n       1:n - 0.5,\n       label)\n}\n\n\nlibrary('readODS')\nModernism &lt;- format_ods_data_frame(\n  read.ods('Art_Modernism.ods', \n           sheet = 1))\n\nWarning in read.ods(\"Art_Modernism.ods\", sheet = 1): read.ods will be\ndeprecated in the next version. Use read_ods instead.\n\n\n\npar(mar = c(3.1, 2.1, 2.1, 2.1))\nplot(0, type = 'n',\n     ylab = '', xlab = '', yaxt = 'n',\n     ylim = c(nrow(Modernism), 0), \n     xlim = range(Modernism$Begin, Modernism$End))\ngrid()\nrect(xleft = Modernism$Begin,\n     xright = Modernism$End,\n     ybottom = 0:(nrow(Modernism) - 1) + 0.1,\n     ytop = 1:nrow(Modernism) - 0.1,\n     col = rainbow(nrow(Modernism), \n                   s = 0.2))\ntext((Modernism$Begin + Modernism$End) / 2,\n     1:nrow(Modernism) - 0.5,\n     Modernism$Label)\n\n\n\n\n\npainters &lt;- format_ods_data_frame(\n  read.ods('Painters.ods', sheet = 1))\n\nWarning in read.ods(\"Painters.ods\", sheet = 1): read.ods will be deprecated in\nthe next version. Use read_ods instead.\n\nwith(painters, \n     plot_timeline(Label, Begin, End, group = Group))"
  },
  {
    "objectID": "posts/2022-09-08-antipsychotics/2022-09-08-antipsychotics.html",
    "href": "posts/2022-09-08-antipsychotics/2022-09-08-antipsychotics.html",
    "title": "Antipsychotics",
    "section": "",
    "text": "Dopamin-Rezeptoren\n\nD_1/D_5-Gruppe: Signalübertragung durch stimulatorisches G-Protein –&gt; cAMP hoch\nD_2/D_3/D_4-Gruppe: Signalübertragung durch hemmendes G-Protein –&gt; cAMP runter\n\n\n\nWichtige dopaminerge Neuronensysteme\n\nNigrostriatales System: Motorik (damit auch EPS)\nMesolimbisches / mesokortikales System: Hauptangriffsort der Antipsychotika\nTuberoinfundibuläres System: vermittelt die neuroendokrinologischen NW, insbesondere Prolaktin-Anstieg\n\n\n\nAntipsychotika\n\n\nWarning in read.ods(\"Antipsychotics.ods\", sheet = 1): read.ods will be\ndeprecated in the next version. Use read_ods instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubstanz\nKlasse\nTrizyklisch\nTyp\nPotenz\nD1\nD2\nD3\nSer2\nM1\nalpha1\nH1\n\n\n\n\nAmisulprid\nBenzamid\n0\nAAP\nNA\n0\n3\n3\n0\n0\n0\n0\n\n\nAripiprazol\nPhenylpiperazinylcholin\n0\nAAP\nNA\n0\n3\n3\n2\n0\n1\n1\n\n\nAsenapin\nDibenzooxepinpyrrol\n0\nAAP\nNA\n1\n1\n2\n2\n0\n1\n1\n\n\nBenperidol\nButyrophenon\n0\nKAP\nHP\n0\n3\n2\n2\n0\n1\n0\n\n\nBromperidol\nButyrophenon\n0\nKAP\nHP\n1\n3\n2\n0\n0\n1\n0\n\n\nChorprothixen\nThioxanthen\n1\nKAP\nNP\n2\n1\n1\n2\n1\n1\n3\n\n\nClozapin\nDibenzodiazepin\n1\nAAP\nNA\n2\n1\n2\n3\n3\n1\n3\n\n\nFlupentixol\nThioxeanthen\n1\nKAP\nHP\n2\n3\n3\n2\n0\n1\n1\n\n\nFluphenazin\nPhenothiazin\n1\nKAP\nHP\n2\n3\n3\n2\n0\n2\n2\n\n\nFluspirilen\nDiphenylbutylpiperidin\n0\nKAP\nHP\n1\n3\n2\n1\n0\n0\n0\n\n\nHaloperidol\nButyrophenon\n0\nKAP\nHP\n2\n3\n2\n1\n0\n2\n0\n\n\nLevomepromazin\nPhenothiazin\n1\nKAP\nNP\n0\n1\n1\n1\n2\n2\n2\n\n\nLoxapin\nDibenzoxazepin\n1\nKAP\nMP\n0\n3\n1\n3\n2\n3\n3\n\n\nLurasidon\nBenzoisothiazol\n0\nAAP\nNA\n1\n3\n2\n3\n0\n2\n0\n\n\nMelperon\nButyrophenon\n0\nKAPA\nNP\n0\n1\n1\n2\n0\n1\n1\n\n\nOlanzapin\nThienobenzazepin\n1\nAAP\nNA\n2\n3\n2\n3\n2\n2\n3\n\n\nPaliperidon\nBenzisoxazol\n0\nAAP\nNA\n0\n3\n1\n3\n0\n1\n1\n\n\nPerazin\nPhenothiazin\n1\nKAP\nMP\n0\n2\n2\n2\n1\n2\n3\n\n\nPerphenazin\nPhenothiazin\n1\nKAP\nHP\n0\n3\n3\n2\n0\n2\n2\n\n\nPimozid\nDiphenylbutylpiperidin\n0\nKAP\nHP\n0\n3\n3\n2\n0\n0\n0\n\n\nPipameron\nButyrophenon\n0\nKAP\nNP\n0\n1\n1\n2\n0\n1\n0\n\n\nProthipendyl\nPhenothiazin\n1\nKAP\nNP\nNA\n1\nNA\nNA\nNA\nNA\nNA\n\n\nQuetiapin\nDibenzothiazepin\n1\nAAP\nNA\n1\n1\n1\n1\n0\n1\n2\n\n\nRisperidon\nBenzisoxazol\n0\nAAP\nNA\n2\n3\n2\n3\n0\n3\n1\n\n\nSertindol\nIndol\n0\nAAP\nNA\n2\n3\n1\n3\n0\n2\n0\n\n\nSulpirid\nBenzamid\n0\nKAPA\nMP\n0\n1\n3\n0\n0\n0\n0\n\n\nThioridazin\nPhenothiazin\n1\nKAP\nNP\n1\n2\n1\n2\n3\n3\n1\n\n\nZiprasidon\nBenzisothiazin\n0\nAAP\nNA\n1\n2\n2\n3\n0\n1\n2\n\n\nZuclopenthixol\nThioxanthen\n1\nKAP\nMP\n2\n3\n2\n0\n3\n3\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubstanz\nKlasse\nTrizyklisch\nTyp\nPotenz\nD1\nD2\nD3\nSer2\nM1\nalpha1\nH1\n\n\n\n\n\nAmisulprid : 1\nPhenothiazin : 6\nMin. :0.0000\nAAP :11\nHP : 8\nMin. :0.0000\nMin. :1.000\nMin. :1.000\nMin. :0.000\nMin. :0.0000\nMin. :0.000\nMin. :0.00\n\n\n\nAripiprazol : 1\nButyrophenon : 5\n1st Qu.:0.0000\nKAP :16\nMP : 4\n1st Qu.:0.0000\n1st Qu.:1.000\n1st Qu.:1.000\n1st Qu.:1.000\n1st Qu.:0.0000\n1st Qu.:1.000\n1st Qu.:0.00\n\n\n\nAsenapin : 1\nBenzamid : 2\nMedian :0.0000\nKAPA: 2\nNP : 6\nMedian :1.0000\nMedian :3.000\nMedian :2.000\nMedian :2.000\nMedian :0.0000\nMedian :1.000\nMedian :1.00\n\n\n\nBenperidol : 1\nBenzisoxazol : 2\nMean :0.4483\nNA\nNA’s:11\nMean :0.8929\nMean :2.276\nMean :1.929\nMean :1.857\nMean :0.6071\nMean :1.429\nMean :1.25\n\n\n\nBromperidol : 1\nDiphenylbutylpiperidin: 2\n3rd Qu.:1.0000\nNA\nNA\n3rd Qu.:2.0000\n3rd Qu.:3.000\n3rd Qu.:2.250\n3rd Qu.:3.000\n3rd Qu.:1.0000\n3rd Qu.:2.000\n3rd Qu.:2.00\n\n\n\nChorprothixen: 1\nThioxanthen : 2\nMax. :1.0000\nNA\nNA\nMax. :2.0000\nMax. :3.000\nMax. :3.000\nMax. :3.000\nMax. :3.0000\nMax. :3.000\nMax. :3.00\n\n\n\n(Other) :23\n(Other) :10\nNA\nNA\nNA\nNA’s :1\nNA\nNA’s :1\nNA’s :1\nNA’s :1\nNA’s :1\nNA’s :1"
  },
  {
    "objectID": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html",
    "href": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html",
    "title": "Python study notes",
    "section": "",
    "text": "This section contains my study notes on the official python tutorial.\nHere is a link to the PEP 8 style guide.\nAlso it might be worthwile to look at the official language definition.\n\n\n\nthe_world_is_flat = True\nif the_world_is_flat:\n  print(\"Be careful not to fall off!\")\n\nBe careful not to fall off!\n\n\n\n\n\nSource\n\nspam = 1 \ntext = \"# This is not a comment because it's inside quotes.\"\nsquares = [1, 4, 9, 16, 25]\nsquares[0]\n\n1\n\nsquares[-1]\n\n25\n\nsquares[-3:]\n\n[9, 16, 25]\n\nsquares[:]\n\n[1, 4, 9, 16, 25]\n\nsquares + [36, 49, 64, 81, 100]\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\ncubes = [1, 8, 27, 65, 125]  # something's wrong here\n4 ** 3  # the cube of 4 is 64, not 65!\n\n64\n\ncubes[3] = 64  # replace the wrong value\ncubes\n\n[1, 8, 27, 64, 125]\n\ncubes.append(216)  # add the cube of 6\ncubes.append(7 ** 3)  # and the cube of 7\ncubes\n\n[1, 8, 27, 64, 125, 216, 343]\n\n\n\n# Fibonacci series:\n# the sum of two elements defines the next\na, b = 0, 1\nwhile a &lt; 10:\n    print(a)\n    a, b = b, a+b\n\n0\n1\n1\n2\n3\n5\n8\n\n\n\n\n\n4 builtin data structures: * Lists - x = [1, 2, 3] * Tuples - x = 1, 2, 3 * Sets - x = {1, 2, 3} * Dictionaries x = {'a': 1, 'b': 2, 'c': 3}\n\n\n\nfruits = ['apple', 'banana', 'orange']\nfruits.reverse()\nfruits\n\n['orange', 'banana', 'apple']\n\nfruits[1:]\n\n['banana', 'apple']\n\nfruits[:1]\n\n['orange']\n\nfruits.append('kiwi')\nfruits\n\n['orange', 'banana', 'apple', 'kiwi']\n\n\n\n\n\n\n“immutable lists”\n\n\na = 1, 2, 3, (2, 3, 4), 'a', ('Hello', 'This')\na[3]\n\n(2, 3, 4)\n\n1 &lt; 3 &gt; 1 &lt; 27\n\nTrue\n\n\n\n\n\n\nx = {1, 2, 3}\nx\n\n{1, 2, 3}\n\ny = {'a', 1, 3}\ny\n\n{1, 3, 'a'}\n\nx & y\n\n{1, 3}\n\nx | y\n\n{1, 2, 3, 'a'}\n\nx.add(4)\nx\n\n{1, 2, 3, 4}\n\nx.update([7, 0])\nx\n\n{0, 1, 2, 3, 4, 7}\n\n\n\n\n\n\ntel = {'jack': 4098, 'sape': 4139}\ntel['jan-eggerik'] = 4898\ntel['jan-eggerik'] = 'a'\n'jack' in tel\n\nTrue\n\n'seb' in tel\n\nFalse\n\ndel tel['sape']\nlist(tel)\n\n['jack', 'jan-eggerik']\n\nlist(tel) &lt;= ['jack', 'jan-eggerik']\n\nTrue\n\ndict([('a', 1), ('b', 2)])\n\n{'a': 1, 'b': 2}\n\nx = dict([(1, 'a'), (2, 'b')])\nx[2]\n\n'b'\n\nx['a'] = 3\nx\n\n{1: 'a', 2: 'b', 'a': 3}\n\n# sorted(x) - this will throw an error because of mixing of str and int\nsorted(tel)\n\n['jack', 'jan-eggerik']\n\n\n\nx = 'string'\nbool(x)\n\nTrue\n\nx = ''\nbool(x)\n\nFalse"
  },
  {
    "objectID": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#official-python-tutorial",
    "href": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#official-python-tutorial",
    "title": "Python study notes",
    "section": "",
    "text": "This section contains my study notes on the official python tutorial.\nHere is a link to the PEP 8 style guide.\nAlso it might be worthwile to look at the official language definition.\n\n\n\nthe_world_is_flat = True\nif the_world_is_flat:\n  print(\"Be careful not to fall off!\")\n\nBe careful not to fall off!\n\n\n\n\n\nSource\n\nspam = 1 \ntext = \"# This is not a comment because it's inside quotes.\"\nsquares = [1, 4, 9, 16, 25]\nsquares[0]\n\n1\n\nsquares[-1]\n\n25\n\nsquares[-3:]\n\n[9, 16, 25]\n\nsquares[:]\n\n[1, 4, 9, 16, 25]\n\nsquares + [36, 49, 64, 81, 100]\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n\ncubes = [1, 8, 27, 65, 125]  # something's wrong here\n4 ** 3  # the cube of 4 is 64, not 65!\n\n64\n\ncubes[3] = 64  # replace the wrong value\ncubes\n\n[1, 8, 27, 64, 125]\n\ncubes.append(216)  # add the cube of 6\ncubes.append(7 ** 3)  # and the cube of 7\ncubes\n\n[1, 8, 27, 64, 125, 216, 343]\n\n\n\n# Fibonacci series:\n# the sum of two elements defines the next\na, b = 0, 1\nwhile a &lt; 10:\n    print(a)\n    a, b = b, a+b\n\n0\n1\n1\n2\n3\n5\n8\n\n\n\n\n\n4 builtin data structures: * Lists - x = [1, 2, 3] * Tuples - x = 1, 2, 3 * Sets - x = {1, 2, 3} * Dictionaries x = {'a': 1, 'b': 2, 'c': 3}\n\n\n\nfruits = ['apple', 'banana', 'orange']\nfruits.reverse()\nfruits\n\n['orange', 'banana', 'apple']\n\nfruits[1:]\n\n['banana', 'apple']\n\nfruits[:1]\n\n['orange']\n\nfruits.append('kiwi')\nfruits\n\n['orange', 'banana', 'apple', 'kiwi']\n\n\n\n\n\n\n“immutable lists”\n\n\na = 1, 2, 3, (2, 3, 4), 'a', ('Hello', 'This')\na[3]\n\n(2, 3, 4)\n\n1 &lt; 3 &gt; 1 &lt; 27\n\nTrue\n\n\n\n\n\n\nx = {1, 2, 3}\nx\n\n{1, 2, 3}\n\ny = {'a', 1, 3}\ny\n\n{1, 3, 'a'}\n\nx & y\n\n{1, 3}\n\nx | y\n\n{1, 2, 3, 'a'}\n\nx.add(4)\nx\n\n{1, 2, 3, 4}\n\nx.update([7, 0])\nx\n\n{0, 1, 2, 3, 4, 7}\n\n\n\n\n\n\ntel = {'jack': 4098, 'sape': 4139}\ntel['jan-eggerik'] = 4898\ntel['jan-eggerik'] = 'a'\n'jack' in tel\n\nTrue\n\n'seb' in tel\n\nFalse\n\ndel tel['sape']\nlist(tel)\n\n['jack', 'jan-eggerik']\n\nlist(tel) &lt;= ['jack', 'jan-eggerik']\n\nTrue\n\ndict([('a', 1), ('b', 2)])\n\n{'a': 1, 'b': 2}\n\nx = dict([(1, 'a'), (2, 'b')])\nx[2]\n\n'b'\n\nx['a'] = 3\nx\n\n{1: 'a', 2: 'b', 'a': 3}\n\n# sorted(x) - this will throw an error because of mixing of str and int\nsorted(tel)\n\n['jack', 'jan-eggerik']\n\n\n\nx = 'string'\nbool(x)\n\nTrue\n\nx = ''\nbool(x)\n\nFalse"
  },
  {
    "objectID": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#seaborn",
    "href": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#seaborn",
    "title": "Python study notes",
    "section": "Seaborn",
    "text": "Seaborn\n\nGallery\n\n\nimport seaborn as sns\nimport matplotlib as mp\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme()\n\n# Load an example dataset\n\n\ndots = sns.load_dataset(\"dots\")\n\nsns.set(rc={'figure.figsize':(10, 3)})\n\nsns.relplot(\n  data=dots, kind=\"line\",\n  x=\"time\", y=\"firing_rate\", col=\"align\", # style = \"choice\",\n  hue=\"choice\", size=\"coherence\", \n  facet_kws=dict(sharex=False)\n  )\n\n\n\n#mp.pyplot.show()\n\n\n# sns.set_theme(style=\"whitegrid\")\n\nx = (5,5)\n\nsns.set(rc={'figure.figsize': x})\n\n\n# Load the example diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Draw a scatter plot while assigning point colors and sizes to different\n# variables in the dataset\nf, ax = plt.subplots(figsize=x)\nsns.despine(f, left=True, bottom=True)\nclarity_ranking = [\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"]\nsns.scatterplot(x=\"carat\", y=\"price\",\n                hue=\"clarity\", size=\"depth\",\n                palette=\"ch:r=-.2,d=.3_r\",\n                hue_order=clarity_ranking,\n                sizes=(1, 8), linewidth=0,\n                data=diamonds, ax=ax)"
  },
  {
    "objectID": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#python-libraries-i-want-to-look-at",
    "href": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#python-libraries-i-want-to-look-at",
    "title": "Python study notes",
    "section": "Python libraries I want to look at",
    "text": "Python libraries I want to look at\n\nNumpy - Documentation\nMatplotlib - Tutorials\nSeaborn - Tutorials\nSciPy - User Guide -\nSciKit-Learn - Tutorials\nstatsmodels - Getting started\npandas - Tutorials\npytorch and co\nDjango - Tutorial using VS Code\nPlotly"
  },
  {
    "objectID": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#python-and-quarto-issues",
    "href": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#python-and-quarto-issues",
    "title": "Python study notes",
    "section": "Python and quarto issues",
    "text": "Python and quarto issues\n\nFigure size seems to be not adjustable by chunk options if engine is jupyter\n\n–&gt; use knitr as engine\nHere fig-size seems to work at least with matplotlib, other packages not tested\n\nWith knitr as engine, I think by default the following python executable is\n\n\nsystem('which python', intern = TRUE)\n\n[1] \"/Users/seb/Library/r-miniconda-arm64/envs/sbloggel/bin/python\"\n\n\n\nimport sys\nprint(sys.executable)\n\n/Users/seb/Library/r-miniconda-arm64/envs/sbloggel/bin/python\n\n\n\nIn the default environment, I was able to install packages with the following command:\n\n\nlibrary('reticulate')\n# conda_install('r-reticulate', 'matplotlib')\n \n\n# This did not work:\n# py_install(\"pandas\")"
  },
  {
    "objectID": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#pandas",
    "href": "posts/2023-03-17-python-study-notes/2023-03-17-python-study-notes.html#pandas",
    "title": "Python study notes",
    "section": "Pandas",
    "text": "Pandas\n\nTutorials"
  },
  {
    "objectID": "posts/2023-02-02-strange-behavior-of-geom_area/2023-02-02-strange-behavior-of-geom_area.html",
    "href": "posts/2023-02-02-strange-behavior-of-geom_area/2023-02-02-strange-behavior-of-geom_area.html",
    "title": "Strange behavior in ggplot2",
    "section": "",
    "text": "I have asked a question related to this issue on stackoverflow and I am providing the plots on this page. Here is the question on stackoverflow:\nLink to stackoverflow"
  },
  {
    "objectID": "posts/2023-02-02-strange-behavior-of-geom_area/2023-02-02-strange-behavior-of-geom_area.html#background",
    "href": "posts/2023-02-02-strange-behavior-of-geom_area/2023-02-02-strange-behavior-of-geom_area.html#background",
    "title": "Strange behavior in ggplot2",
    "section": "",
    "text": "I have asked a question related to this issue on stackoverflow and I am providing the plots on this page. Here is the question on stackoverflow:\nLink to stackoverflow"
  },
  {
    "objectID": "posts/2023-02-02-strange-behavior-of-geom_area/2023-02-02-strange-behavior-of-geom_area.html#illustration-of-the-problem",
    "href": "posts/2023-02-02-strange-behavior-of-geom_area/2023-02-02-strange-behavior-of-geom_area.html#illustration-of-the-problem",
    "title": "Strange behavior in ggplot2",
    "section": "Illustration of the problem",
    "text": "Illustration of the problem\n\nlibrary('tidyr')\nlibrary('tibble')\nlibrary('ggplot2')\nlibrary('dplyr')\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ngrid_values &lt;- 2^(-3:3)\n\nstrange_values &lt;- c(-1.90819287770871e-06, -1.68820727726686e-20, 1.90820561104585e-06, \n                    -2.03934646947346e-11, -2.66724989539021e-44, 1.82186185543446e-11, \n                    -3.08642000845794e-14, -2.84079624478981e-68, 3.8017746773927e-20, \n                    -1.55431223447522e-14, -2.12106623568441e-87, -7.04031177308046e-34, \n                    1.99840144432528e-15, 6.92960775272386e-94, -2.17911262386235e-50, \n                    6.43929354282591e-15, 1.60655203590779e-101, -2.4149683145618e-74, \n                    4.2188474935756e-15, -1.63364987668272e-101, -8.43744249369279e-100, \n                    -1.9234795292089e-06, -1.40915667489693e-20, 1.92348291032148e-06, \n                    -7.86031240096463e-11, 5.94869638827445e-40, 1.70218514362265e-11, \n                    -6.21724893790088e-15, -5.32110095875999e-63, 3.9895815730536e-20, \n                    2.39808173319034e-14, -9.91626723546227e-86, -2.47375723122866e-33, \n                    -1.77635683940025e-15, 8.7576689576463e-93, -2.1712899716615e-50, \n                    9.54791801177635e-15, 2.7961387551394e-101, -2.62250706570328e-74, \n                    -6.43929354282591e-15, -5.36337882495689e-101, -1.65540646490924e-99, \n                    -1.95935441282824e-06, -4.13414429145914e-22, 1.9595249500527e-06, \n                    -1.99651406518342e-11, 4.01993294485079e-37, 1.71444629227149e-11, \n                    2.31215047108435e-12, -5.46064224913782e-66, 6.44847728373308e-20, \n                    5.40456568387526e-13, 5.30352631285166e-83, 3.81832474229875e-33, \n                    -1.80966353013901e-14, -9.72418279929888e-94, -2.08007912888974e-51, \n                    -1.33226762955019e-14, 3.23934484578975e-100, -2.61033780134937e-74, \n                    6.88338275267597e-15, -1.29762567303952e-99, -2.07383582042798e-98, \n                    -2.01803061317118e-06, 3.3691909504469e-20, 2.01799435665222e-06, \n                    -1.83731918568242e-11, 7.21526941571507e-42, 1.72812031885596e-11, \n                    1.40298883621881e-11, 2.61529894687986e-38, 2.597261897857e-20, \n                    3.95239396766556e-14, -1.83777217239233e-53, 1.74710015191643e-33, \n                    -2.1316282072803e-14, 3.26611237858842e-68, -1.19804995812909e-51, \n                    -1.58761892521397e-14, 6.58410560104933e-90, -2.50682775406401e-74, \n                    2.46469511466785e-14, -1.08556164510476e-96, -8.68448004693709e-96, \n                    -2.14979959756167e-06, 4.65632478870001e-17, 2.14994451996064e-06, \n                    -2.15127915481617e-11, -5.03897310277035e-29, 2.37451670641299e-11, \n                    -7.30294713591206e-11, -6.74709311307118e-38, 4.03149402823541e-20, \n                    8.21565038222616e-14, 4.6575341905728e-50, -1.71156811032874e-33, \n                    -1.32116539930394e-14, 9.41355218246985e-67, -5.35957793195957e-51, \n                    7.99360577730113e-15, 8.43929875779189e-89, -2.9418326640071e-74, \n                    1.48769885299771e-14, -2.81028796325013e-90, -1.12411483527064e-89, \n                    -2.42728809329851e-06, -8.05503632048299e-18, 2.42718074969143e-06, \n                    5.7643223527748e-11, -5.67417009881705e-30, 2.18227445666839e-11, \n                    4.88498130835069e-15, 5.74369923731793e-37, 5.93253926283155e-20, \n                    -5.36681810103801e-13, 7.68115012003888e-48, 2.12048592220925e-31, \n                    1.43107747874183e-12, -5.9458568180203e-64, 3.32402936140172e-48, \n                    -8.88178419700125e-16, -1.75086766216852e-75, -3.32876019974183e-73, \n                    6.43929354282591e-15, -1.00992378651113e-74, -2.01984753722959e-74, \n                    -2.90572647954068e-06, -7.77642264438329e-10, 2.90649964611508e-06, \n                    9.09130548620851e-11, -3.52178085798882e-12, -8.73905452509179e-11, \n                    -1.03369389016228e-05, 6.07960561736676e-07, 9.72897833978829e-06, \n                    -0.00081769791399311, 9.032308610189e-05, 0.000727374827890998, \n                    -0.0011992740379827, 0.000238928931343064, 0.000960345106639307, \n                    -0.00123663646030969, 0.000411530217922524, 0.000825106242386583, \n                    -0.00124466009860591, 0.000621942028670438, 0.000622718069935415\n)\nmy_tib &lt;- expand_grid(beta = grid_values, lambda = grid_values, name = c('S', 'I', 'R'))\nmy_tib$value &lt;- 0\nmy_tib$value[my_tib$name == 'I'] &lt;- 1\nmy_tib$value_strange &lt;- my_tib$value + strange_values\n\n\n# expected behavior:\nmy_tib %&gt;%\n  ggplot(aes(x = beta, y = value, fill = name)) + geom_area() + facet_grid(cols = vars(lambda)) + scale_x_continuous(trans = 'log10')\n\n\n\n# strange behavior when adding these really small, strange (?) numbers:\nmy_tib %&gt;%\n  ggplot(aes(x = beta, y = value_strange, fill = name)) + geom_area() + facet_grid(cols = vars(lambda)) + scale_x_continuous(trans = 'log10')\n\n\n\n# expected behavior when x axis is not on log-scale\nmy_tib %&gt;%\n  ggplot(aes(x = beta, y = value_strange, fill = name)) + geom_area() + facet_grid(cols = vars(lambda))\n\n\n\n# expected behavior when only the 1's are plotted and the 0's are omitted\nmy_tib %&gt;% filter(name == 'I') %&gt;%\n  ggplot(aes(x = beta, y = value_strange, fill = name)) + geom_area() + facet_grid(cols = vars(lambda)) + scale_x_continuous(trans = 'log10')\n\n\n\n# can't reproduce strange behavior with random numbers - what makes the other numbers so strange that `geom_area` produces weird results?\nset.seed(1)\nmy_tib$value_strange_2 &lt;- my_tib$value + runif(nrow(my_tib), -1e-10, 1e-10)\nmy_tib %&gt;%\n  ggplot(aes(x = beta, y = value_strange_2, fill = name)) + geom_area() + facet_grid(cols = vars(lambda)) + scale_x_continuous(trans = 'log10')\n\n\n\nsessionInfo()\n\nR version 4.3.0 (2023-04-21)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.5\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/Berlin\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] dplyr_1.1.2   ggplot2_3.4.2 tibble_3.2.1  tidyr_1.3.0  \n\nloaded via a namespace (and not attached):\n [1] vctrs_0.6.2       cli_3.6.1         knitr_1.43        rlang_1.1.1      \n [5] xfun_0.39         purrr_1.0.1       generics_0.1.3    jsonlite_1.8.5   \n [9] labeling_0.4.2    glue_1.6.2        colorspace_2.1-0  htmltools_0.5.5  \n[13] scales_1.2.1      fansi_1.0.4       rmarkdown_2.22    grid_4.3.0       \n[17] munsell_0.5.0     evaluate_0.21     fastmap_1.1.1     yaml_2.3.7       \n[21] lifecycle_1.0.3   compiler_4.3.0    htmlwidgets_1.6.2 pkgconfig_2.0.3  \n[25] rstudioapi_0.14   farver_2.1.1      digest_0.6.31     R6_2.5.1         \n[29] tidyselect_1.2.0  utf8_1.2.3        pillar_1.9.0      magrittr_2.0.3   \n[33] withr_2.5.0       gtable_0.3.3      tools_4.3.0"
  },
  {
    "objectID": "posts/2023-08-04-getting-to-know-mlr3/2023-08-04-getting-to-know-mlr3.html",
    "href": "posts/2023-08-04-getting-to-know-mlr3/2023-08-04-getting-to-know-mlr3.html",
    "title": "Getting to know mlr3",
    "section": "",
    "text": "Link to package\n\nset.seed(1)\nlibrary('mlr3')"
  },
  {
    "objectID": "posts/2023-07-12-easier-publishing/2023-07-12-easier-publishing.html",
    "href": "posts/2023-07-12-easier-publishing/2023-07-12-easier-publishing.html",
    "title": "Easier publishing",
    "section": "",
    "text": "File “/usr/local/bin/pq”:\n\n#!   /bin/bash -e\nquarto render\ngit add .\ngit commit -m \"some changes\"\ngit push\n\nchmod +x /usr/local/bin/pq"
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html",
    "title": "Interesting R packages",
    "section": "",
    "text": "Flow charts in R: ggflowchart\nWeb scraping: rvest\nSpatial data / maps: Recommendations from Nicola Rennie’s blog"
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#short-notes-about-r-packages",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#short-notes-about-r-packages",
    "title": "Interesting R packages",
    "section": "",
    "text": "Flow charts in R: ggflowchart\nWeb scraping: rvest\nSpatial data / maps: Recommendations from Nicola Rennie’s blog"
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#check-package-names",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#check-package-names",
    "title": "Interesting R packages",
    "section": "Check package names",
    "text": "Check package names\n\nlibrary('available')\n# help(package = 'available')\n\noptions(available.browse = FALSE)\navailable('merge', )\navailable('track')\navailable('trackChanges')\navailable('trackR')\navailable('collab')\navailable('writeTogether')"
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#r-packages-built-from-a-single-.rmd-file",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#r-packages-built-from-a-single-.rmd-file",
    "title": "Interesting R packages",
    "section": "R packages built from a single *.RMD file",
    "text": "R packages built from a single *.RMD file\nLink"
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#interfaces-to-computer-algebra-systems-in-r",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#interfaces-to-computer-algebra-systems-in-r",
    "title": "Interesting R packages",
    "section": "Interfaces to computer algebra systems in R",
    "text": "Interfaces to computer algebra systems in R\nHerre, I have collected a few resources in the field of computer algebra systems that have an interface to R.\n\nGeneral\n\nCRAN task view: Section “Multi-Precision Arithmetic and Symbolic Mathematics” on this site\nBlog article\n\n\n\nMaxima\n\nRim package\n\nextended search path according to these instructions\nposted issue on rim package github site because it is still not working here\nedited /etc/launch.d\nadded shell script /usr/bin/maxima\n\n\n\nsystem('export PATH=${PATH}:/Applications/Maxima.app/Contents/Resources/opt/bin')\nsystem('echo $PATH')\nhelp(package='rim')\n\n\nlibrary(rim)\n\nMaxima successfully registered as knitr engine!\n\nrim::maxima.isInstalled()\n\n[1] TRUE\n\nrim::maxima.get(\"1+1;\")\n\n(%o1) 2\n\n\n\n(%i1) lambda_S: beta * psi;\n\n(%o1) beta*psi\n\n(%i2) Lambda_S: integrate(lambda_S, t);\n\n(%o2) beta*psi*t\n\n(%i3) S_S: exp(-Lambda_S);\n\n(%o3) %e^-(beta*psi*t)\n\n(%i4) assume(beta &gt; 0);\n\n(%o4) [beta &gt; 0]\n\n(%i5) assume(psi &gt; 0);\n\n(%o5) [psi &gt; 0]\n\n(%i6) E_S: integrate(S_S, t, 0, inf);\n\n(%o6) 1/(beta*psi)\n\n(%i7) tex(''E_S);\n\n(%o7) false\n\n\n\n\nRyacas\n\nlibrary(Ryacas)\n\n\nAttaching package: 'Ryacas'\n\n\nThe following object is masked from 'package:stats':\n\n    integrate\n\n\nThe following objects are masked from 'package:base':\n\n    %*%, det, diag, diag&lt;-, lower.tri, upper.tri\n\nas_r(yac_str(\"Simplify(a*b*a^2/b-a^3)\"))\n\n[1] 0\n\nyac_str('Simplify(x^2 + 2*(x-3)^3)')\n\n[1] \"2*x^3-17*x^2+54*x-54\"\n\nyac_str(\"TeXForm(x^2 - 1)\")\n\n[1] \"x ^{2} - 1\""
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#flowcharts",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#flowcharts",
    "title": "Interesting R packages",
    "section": "Flowcharts",
    "text": "Flowcharts\nSource: https://cran.r-project.org/web/packages/Gmisc/vignettes/Grid-based_flowcharts.html\n\nlibrary(Gmisc, quietly = TRUE)\nlibrary(glue)\nlibrary(htmlTable)\nlibrary(grid)\nlibrary(magrittr)\n\norg_cohort &lt;- boxGrob(glue(\"Stockholm population\",\n                           \"n = {pop}\",\n                           pop = txtInt(1632798),\n                           .sep = \"\\n\"))\neligible &lt;- boxGrob(glue(\"Eligible\",\n                          \"n = {pop}\",\n                           pop = txtInt(10032),\n                           .sep = \"\\n\"))\nincluded &lt;- boxGrob(glue(\"Randomized\",\n                         \"n = {incl}\",\n                         incl = txtInt(122),\n                         .sep = \"\\n\"))\ngrp_a &lt;- boxGrob(glue(\"Treatment A\",\n                      \"n = {recr}\",\n                      recr = txtInt(43),\n                      .sep = \"\\n\"))\n\ngrp_b &lt;- boxGrob(glue(\"Treatment B\",\n                      \"n = {recr}\",\n                      recr = txtInt(122 - 43 - 30),\n                      .sep = \"\\n\"))\n\nexcluded &lt;- boxGrob(glue(\"Excluded (n = {tot}):\",\n                         \" - not interested: {uninterested}\",\n                         \" - contra-indicated: {contra}\",\n                         tot = 30,\n                         uninterested = 12,\n                         contra = 30 - 12,\n                         .sep = \"\\n\"),\n                    just = \"left\")\n\ngrid.newpage()\nvert &lt;- spreadVertical(org_cohort,\n                       eligible = eligible,\n                       included = included,\n                       grps = grp_a)\ngrps &lt;- alignVertical(reference = vert$grps,\n                      grp_a, grp_b) %&gt;%\n  spreadHorizontal()\nvert$grps &lt;- NULL\n\nexcluded &lt;- moveBox(excluded,\n                    x = .8,\n                    y = coords(vert$included)$top + distance(vert$eligible, vert$included, half = TRUE, center = FALSE))\n\nfor (i in 1:(length(vert) - 1)) {\n  connectGrob(vert[[i]], vert[[i + 1]], type = \"vert\") %&gt;%\n    print\n}\nconnectGrob(vert$included, grps[[1]], type = \"N\")\nconnectGrob(vert$included, grps[[2]], type = \"N\")\n\nconnectGrob(vert$eligible, excluded, type = \"L\")\n\n# Print boxes\nvert\ngrps\nexcluded"
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#leaflet-for-maps",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#leaflet-for-maps",
    "title": "Interesting R packages",
    "section": "Leaflet for maps",
    "text": "Leaflet for maps\n\nLink"
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#here-for-finding-the-path-to-the-project-directory",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#here-for-finding-the-path-to-the-project-directory",
    "title": "Interesting R packages",
    "section": "Here for finding the path to the project directory",
    "text": "Here for finding the path to the project directory\n\nLink"
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#package-for-formatting-labels-in-ggplot",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#package-for-formatting-labels-in-ggplot",
    "title": "Interesting R packages",
    "section": "Package for formatting labels in ggplot",
    "text": "Package for formatting labels in ggplot\n\nLink\nVideo demonstrating the package"
  },
  {
    "objectID": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#concattenating-text",
    "href": "posts/2023-03-17-interesting-r-packages/2023-03-17-interesting-r-packages.html#concattenating-text",
    "title": "Interesting R packages",
    "section": "Concattenating text",
    "text": "Concattenating text\n\nPackage glue"
  },
  {
    "objectID": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html",
    "href": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html",
    "title": "Exploring the Python library statsmodels",
    "section": "",
    "text": "Link\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport statsmodels.api as sm\n\ndata = sm.datasets.statecrime.load_pandas().data\nmurder = data['murder']\nX = data[['poverty', 'hs_grad']].copy()\nX['constant'] = 1\n\ny = murder\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n# Create a plot just for the variable 'Poverty':\n\nfig, ax = plt.subplots()\nfig = sm.graphics.plot_fit(results, 0, ax=ax)\nax.set_ylabel(\"Murder Rate\")\nax.set_xlabel(\"Poverty Level\")\nax.set_title(\"Linear Regression\")\n\nplt.show()"
  },
  {
    "objectID": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html#plot_fit",
    "href": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html#plot_fit",
    "title": "Exploring the Python library statsmodels",
    "section": "",
    "text": "Link\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport statsmodels.api as sm\n\ndata = sm.datasets.statecrime.load_pandas().data\nmurder = data['murder']\nX = data[['poverty', 'hs_grad']].copy()\nX['constant'] = 1\n\ny = murder\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n# Create a plot just for the variable 'Poverty':\n\nfig, ax = plt.subplots()\nfig = sm.graphics.plot_fit(results, 0, ax=ax)\nax.set_ylabel(\"Murder Rate\")\nax.set_xlabel(\"Poverty Level\")\nax.set_title(\"Linear Regression\")\n\nplt.show()"
  },
  {
    "objectID": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html#plot_regress_exog",
    "href": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html#plot_regress_exog",
    "title": "Exploring the Python library statsmodels",
    "section": "plot_regress_exog",
    "text": "plot_regress_exog\n\nLink\n\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\n\nfig = plt.figure(figsize=(8, 6))\ncrime_data = sm.datasets.statecrime.load_pandas()\nresults = smf.ols('murder ~ hs_grad + urban + poverty + single',\n                   data=crime_data.data).fit()\nsm.graphics.plot_regress_exog(results, 'poverty', fig=fig)\nplt.show()"
  },
  {
    "objectID": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html#plot_partregress",
    "href": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html#plot_partregress",
    "title": "Exploring the Python library statsmodels",
    "section": "plot_partregress",
    "text": "plot_partregress\n\nLink\n\n\ncrime_data = sm.datasets.statecrime.load_pandas()\nsm.graphics.plot_partregress(endog='murder', exog_i='hs_grad',\n                              exog_others=['urban', 'poverty', 'single'],\n                              data=crime_data.data, obs_labels=False)\nplt.show()"
  },
  {
    "objectID": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html#plot_partregress_grid",
    "href": "posts/2023-07-04-python-statsmodels/2023-07-04-python-statsmodels.html#plot_partregress_grid",
    "title": "Exploring the Python library statsmodels",
    "section": "plot_partregress_grid",
    "text": "plot_partregress_grid\n\nLink\n\n\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom statsmodels.graphics.regressionplots import plot_partregress_grid\n\nfig = plt.figure(figsize=(8, 6))\ncrime_data = sm.datasets.statecrime.load_pandas()\nresults = smf.ols('murder ~ hs_grad + urban + poverty + single',\n                  data=crime_data.data).fit()\nplot_partregress_grid(results, fig=fig)\nplt.show()\n\n\n\n\n\nimport statsmodels.api as sm\nimport pandas\ndf = sm.datasets.get_rdataset(\"Guerry\", \"HistData\").data\n\nfm = sm.formula.ols('Lottery ~ Literacy + Wealth + C(Region)', data = df).fit()\nprint(fm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Lottery   R-squared:                       0.338\nModel:                            OLS   Adj. R-squared:                  0.287\nMethod:                 Least Squares   F-statistic:                     6.636\nDate:                Wed, 09 Aug 2023   Prob (F-statistic):           1.07e-05\nTime:                        13:39:12   Log-Likelihood:                -375.30\nNo. Observations:                  85   AIC:                             764.6\nDf Residuals:                      78   BIC:                             781.7\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept         38.6517      9.456      4.087      0.000      19.826      57.478\nC(Region)[T.E]   -15.4278      9.727     -1.586      0.117     -34.793       3.938\nC(Region)[T.N]   -10.0170      9.260     -1.082      0.283     -28.453       8.419\nC(Region)[T.S]    -4.5483      7.279     -0.625      0.534     -19.039       9.943\nC(Region)[T.W]   -10.0913      7.196     -1.402      0.165     -24.418       4.235\nLiteracy          -0.1858      0.210     -0.886      0.378      -0.603       0.232\nWealth             0.4515      0.103      4.390      0.000       0.247       0.656\n==============================================================================\nOmnibus:                        3.049   Durbin-Watson:                   1.785\nProb(Omnibus):                  0.218   Jarque-Bera (JB):                2.694\nSkew:                          -0.340   Prob(JB):                        0.260\nKurtosis:                       2.454   Cond. No.                         371.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nfig = sm.graphics.influence_plot(fm, criterion=\"cooks\")\nfig.show()\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"whitegrid\")\n\n# Load the example diamonds dataset\ndiamonds = sns.load_dataset(\"diamonds\")\n\n# Draw a scatter plot while assigning point colors and sizes to different\n# variables in the dataset\nf, ax = plt.subplots(figsize=(6.5, 6.5))\nsns.despine(f, left=True, bottom=True)\nclarity_ranking = [\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"]\nsns.scatterplot(x=\"carat\", y=\"price\",\n                hue=\"clarity\", size=\"depth\",\n                palette=\"ch:r=-.2,d=.3_r\",\n                hue_order=clarity_ranking,\n                sizes=(1, 8), linewidth=0,\n                data=diamonds, ax=ax)"
  },
  {
    "objectID": "posts/2023-07-21-first-pytorch-tutorial/2023-07-21-first-pytorch-tutorial.html",
    "href": "posts/2023-07-21-first-pytorch-tutorial/2023-07-21-first-pytorch-tutorial.html",
    "title": "First pytorch tutorial",
    "section": "",
    "text": "Link to tutorial\n\n\nlibrary('tidyverse')\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(1)\n\nlibrary('reticulate')\nuse_condaenv('machine_learning')\n\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\n\n/Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n  Referenced from: &lt;8CBD0B78-6C7C-3C8B-8C76-ACA7B6112818&gt; /Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torchvision/image.so\n  Expected in:     &lt;07CB8E54-8386-3606-A01E-B92223F93B74&gt; /Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n\nfrom torchvision.transforms import ToTensor\n\n# Download training data from open datasets.\ntraining_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=ToTensor(),\n)\n\n# Download test data from open datasets.\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=ToTensor(),\n)\n\n\ntraining_data.data.shape\n\ntorch.Size([60000, 28, 28])\n\ntest_data.data.shape\n\ntorch.Size([10000, 28, 28])\n\n\n\nbatch_size = 64\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in test_dataloader:\n    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n    print(f\"Shape of y: {y.shape} {y.dtype}\")\n    break\n\nShape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\nShape of y: torch.Size([64]) torch.int64\n\n\n\n# Get cpu, gpu or mps device for training.\ndevice = (\n    \"cuda\"\n    if torch.cuda.is_available()\n    else \"mps\"\n    if torch.backends.mps.is_available()\n    else \"cpu\"\n)\nprint(f\"Using {device} device\")\n\nUsing mps device\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Linear(28*28, 512),\n            nn.ReLU(),\n            nn.Linear(512, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n\nmodel = NeuralNetwork().to(device)\nprint(model)\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\n\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), (batch + 1) * len(X)\n            print(f\"loss: {loss:&gt;7f}  [{current:&gt;5d}/{size:&gt;5d}]\")\n            \n\n\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss /= num_batches\n    correct /= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):&gt;0.1f}%, Avg loss: {test_loss:&gt;8f} \\n\")\n\n\nepochs = 5\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss_fn, optimizer)\n    test(test_dataloader, model, loss_fn)\n\nEpoch 1\n-------------------------------\nloss: 2.293884  [   64/60000]\nloss: 2.280516  [ 6464/60000]\nloss: 2.254215  [12864/60000]\nloss: 2.257743  [19264/60000]\nloss: 2.241249  [25664/60000]\nloss: 2.206033  [32064/60000]\nloss: 2.222995  [38464/60000]\nloss: 2.183638  [44864/60000]\nloss: 2.180845  [51264/60000]\nloss: 2.147983  [57664/60000]\nTest Error: \n Accuracy: 40.3%, Avg loss: 2.137891 \n\nEpoch 2\n-------------------------------\nloss: 2.147081  [   64/60000]\nloss: 2.142349  [ 6464/60000]\nloss: 2.073414  [12864/60000]\nloss: 2.100391  [19264/60000]\nloss: 2.053971  [25664/60000]\nloss: 1.983526  [32064/60000]\nloss: 2.020415  [38464/60000]\nloss: 1.931436  [44864/60000]\nloss: 1.939984  [51264/60000]\nloss: 1.877790  [57664/60000]\nTest Error: \n Accuracy: 56.7%, Avg loss: 1.862921 \n\nEpoch 3\n-------------------------------\nloss: 1.894928  [   64/60000]\nloss: 1.875024  [ 6464/60000]\nloss: 1.740222  [12864/60000]\nloss: 1.794675  [19264/60000]\nloss: 1.693364  [25664/60000]\nloss: 1.631936  [32064/60000]\nloss: 1.665979  [38464/60000]\nloss: 1.550617  [44864/60000]\nloss: 1.587191  [51264/60000]\nloss: 1.492842  [57664/60000]\nTest Error: \n Accuracy: 60.1%, Avg loss: 1.494139 \n\nEpoch 4\n-------------------------------\nloss: 1.558476  [   64/60000]\nloss: 1.533684  [ 6464/60000]\nloss: 1.366379  [12864/60000]\nloss: 1.457518  [19264/60000]\nloss: 1.342172  [25664/60000]\nloss: 1.323192  [32064/60000]\nloss: 1.354460  [38464/60000]\nloss: 1.258602  [44864/60000]\nloss: 1.304382  [51264/60000]\nloss: 1.218093  [57664/60000]\nTest Error: \n Accuracy: 63.1%, Avg loss: 1.229637 \n\nEpoch 5\n-------------------------------\nloss: 1.300160  [   64/60000]\nloss: 1.291894  [ 6464/60000]\nloss: 1.113367  [12864/60000]\nloss: 1.240308  [19264/60000]\nloss: 1.116215  [25664/60000]\nloss: 1.128032  [32064/60000]\nloss: 1.166063  [38464/60000]\nloss: 1.082035  [44864/60000]\nloss: 1.127863  [51264/60000]\nloss: 1.062129  [57664/60000]\nTest Error: \n Accuracy: 64.9%, Avg loss: 1.068099 \n\nprint(\"Done!\")\n\nDone!\n\n\n\ntorch.save(model.state_dict(), \"model.pth\")\nprint(\"Saved PyTorch Model State to model.pth\")\n\nSaved PyTorch Model State to model.pth\n\n\n\nmodel = NeuralNetwork().to(device)\nmodel.load_state_dict(torch.load(\"model.pth\"))\n\n&lt;All keys matched successfully&gt;\n\n\n\nclasses = [\n    \"T-shirt/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\n\nNeuralNetwork(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear_relu_stack): Sequential(\n    (0): Linear(in_features=784, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=512, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    x = x.to(device)\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n\nPredicted: \"Ankle boot\", Actual: \"Ankle boot\""
  },
  {
    "objectID": "posts/2023-07-10-ideas-for-new-posts/2023-07-10-ideas-for-new-posts.html#longtermism",
    "href": "posts/2023-07-10-ideas-for-new-posts/2023-07-10-ideas-for-new-posts.html#longtermism",
    "title": "Ideas for new posts",
    "section": "Longtermism",
    "text": "Longtermism\n\nDespite my scepticisms about the notion of longtermism I want to think about it a little\nIdea: create a model in which proximity and number of people are balanced\n“Love thy neighbor as thyself.”"
  },
  {
    "objectID": "posts/2023-07-10-ideas-for-new-posts/2023-07-10-ideas-for-new-posts.html#track-changes-in-quarto",
    "href": "posts/2023-07-10-ideas-for-new-posts/2023-07-10-ideas-for-new-posts.html#track-changes-in-quarto",
    "title": "Ideas for new posts",
    "section": "Track changes in quarto",
    "text": "Track changes in quarto\n\nScenario: A has written something, B suggests changes and provides comments\nFile is given from A to B\nB just edits as he / she wishes\n\nComments can be made\npossibility to render with changes tracked on a per-word basis\n\nB returns the file to A\nA can look at the rendered changes with the comments\n\ncomments and changes are numbered\n\nA is provided with a file, in which for each change / comment it can be indicated:\n\naccept\ndecline\nedit manually (a hint is included in the quarto file) -&gt; or the whole line could be given in a new version\n\nHow can feedback be given from A to B which changes are accepted etc. for a discussion?"
  },
  {
    "objectID": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html",
    "href": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html",
    "title": "Retrieving and plotting COVID-19 data",
    "section": "",
    "text": "The COVID19 package allows to conveniently retrieve statistical data about the COVID-19 pandemy. Here, the data for Germany are retrieved and two exploratory plots are created."
  },
  {
    "objectID": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#background",
    "href": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#background",
    "title": "Retrieving and plotting COVID-19 data",
    "section": "",
    "text": "The COVID19 package allows to conveniently retrieve statistical data about the COVID-19 pandemy. Here, the data for Germany are retrieved and two exploratory plots are created."
  },
  {
    "objectID": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#loading-libraries",
    "href": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#loading-libraries",
    "title": "Retrieving and plotting COVID-19 data",
    "section": "Loading libraries",
    "text": "Loading libraries\n\nlibrary('ggplot2')\nlibrary('COVID19')"
  },
  {
    "objectID": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#retrieving-the-data",
    "href": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#retrieving-the-data",
    "title": "Retrieving and plotting COVID-19 data",
    "section": "Retrieving the data",
    "text": "Retrieving the data\n\ndeu &lt;- covid19('Germany', level = 1) \n\nWe have invested a lot of time and effort in creating COVID-19 Data\nHub, please cite the following when using it:\n\n  Guidotti, E., Ardia, D., (2020), \"COVID-19 Data Hub\", Journal of Open\n  Source Software 5(51):2376, doi: 10.21105/joss.02376\n\nThe implementation details and the latest version of the data are\ndescribed in:\n\n  Guidotti, E., (2022), \"A worldwide epidemiological database for\n  COVID-19 at fine-grained spatial resolution\", Sci Data 9(1):112, doi:\n  10.1038/s41597-022-01245-1\nTo print citations in BibTeX format use:\n &gt; print(citation('COVID19'), bibtex=TRUE)\n\nTo hide this message use 'verbose = FALSE'."
  },
  {
    "objectID": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#plotting-total-number-of-confirmed-cases",
    "href": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#plotting-total-number-of-confirmed-cases",
    "title": "Retrieving and plotting COVID-19 data",
    "section": "Plotting total number of confirmed cases",
    "text": "Plotting total number of confirmed cases\n\nggplot(data = deu, aes(date, confirmed)) + geom_point() + \n  geom_smooth()  + scale_y_log10()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 12 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 12 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#plotting-new-cases",
    "href": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#plotting-new-cases",
    "title": "Retrieving and plotting COVID-19 data",
    "section": "Plotting new cases",
    "text": "Plotting new cases\n\ndat &lt;- data.frame(date = deu$date[-1], new_cases = diff(deu$confirmed))\nggplot(data = dat, aes(date, new_cases)) + geom_point() + \n  geom_smooth()  + scale_y_log10()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 19 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 19 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#conclusion",
    "href": "posts/2022-10-16-retrieving-and-plotting-covid-19-data/2022-10-16-retrieving-and-plotting-covid-19-data.html#conclusion",
    "title": "Retrieving and plotting COVID-19 data",
    "section": "Conclusion",
    "text": "Conclusion\nThis package allows convenient retrieval of a lot of data about the COVID-19 pandemy."
  },
  {
    "objectID": "posts/2023-08-08-cars-dataset-and-tidymodels/2023-08-08-cars-dataset-and-tidymodels.html",
    "href": "posts/2023-08-08-cars-dataset-and-tidymodels/2023-08-08-cars-dataset-and-tidymodels.html",
    "title": "Cars dataset and tidymodels",
    "section": "",
    "text": "set.seed(1)\nlibrary('tidymodels')\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n\n\n✔ broom        1.0.4     ✔ recipes      1.0.6\n✔ dials        1.2.0     ✔ rsample      1.1.1\n✔ dplyr        1.1.2     ✔ tibble       3.2.1\n✔ ggplot2      3.4.2     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.1.1\n✔ modeldata    1.1.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.0     ✔ workflowsets 1.0.1\n✔ purrr        1.0.1     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary('knitr')"
  },
  {
    "objectID": "posts/2023-08-08-cars-dataset-and-tidymodels/2023-08-08-cars-dataset-and-tidymodels.html#init",
    "href": "posts/2023-08-08-cars-dataset-and-tidymodels/2023-08-08-cars-dataset-and-tidymodels.html#init",
    "title": "Cars dataset and tidymodels",
    "section": "",
    "text": "set.seed(1)\nlibrary('tidymodels')\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n\n\n✔ broom        1.0.4     ✔ recipes      1.0.6\n✔ dials        1.2.0     ✔ rsample      1.1.1\n✔ dplyr        1.1.2     ✔ tibble       3.2.1\n✔ ggplot2      3.4.2     ✔ tidyr        1.3.0\n✔ infer        1.0.4     ✔ tune         1.1.1\n✔ modeldata    1.1.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.0     ✔ workflowsets 1.0.1\n✔ purrr        1.0.1     ✔ yardstick    1.2.0\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary('knitr')"
  },
  {
    "objectID": "posts/2023-08-08-cars-dataset-and-tidymodels/2023-08-08-cars-dataset-and-tidymodels.html#do-vfold-cross-validation-on-the-cars-dataset-using-a-linear-model-a-random-forest-model-and-a-neural-net",
    "href": "posts/2023-08-08-cars-dataset-and-tidymodels/2023-08-08-cars-dataset-and-tidymodels.html#do-vfold-cross-validation-on-the-cars-dataset-using-a-linear-model-a-random-forest-model-and-a-neural-net",
    "title": "Cars dataset and tidymodels",
    "section": "Do vfold cross-validation on the cars dataset using a linear model, a random forest model and a neural net",
    "text": "Do vfold cross-validation on the cars dataset using a linear model, a random forest model and a neural net\n\n# Create data for resampling:\ncars_folds &lt;- vfold_cv(cars, v = 10, repeats = 1)\n\n# Create model specifications:\nlm_spec &lt;- linear_reg()\nrf_spec &lt;- rand_forest() |&gt; set_mode('regression')\nnn_spec &lt;- mlp(epochs = 500, hidden_units = 3, penalty = 0.01) |&gt; \n  set_mode('regression')\n\n# Create workflows:\nall_workflows &lt;- \n  workflow_set(\n    preproc = list(\"formula\" = speed ~ dist),\n    models = list(lm = lm_spec, \n                  rf = rf_spec,\n                  nn = nn_spec)\n  )\n\n# Run the workflows:\nresult &lt;- all_workflows |&gt; \n  workflow_map(resamples = cars_folds,\n               control = control_resamples(save_pred = TRUE))\n\n# Display summary statistics for all model types:\n(metrics &lt;- collect_metrics(result))\n\n# A tibble: 6 × 9\n  wflow_id   .config        preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;      &lt;chr&gt;          &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 formula_lm Preprocessor1… formula line… rmse    standard   3.18     10  0.288 \n2 formula_lm Preprocessor1… formula line… rsq     standard   0.656    10  0.0885\n3 formula_rf Preprocessor1… formula rand… rmse    standard   3.14     10  0.340 \n4 formula_rf Preprocessor1… formula rand… rsq     standard   0.621    10  0.0901\n5 formula_nn Preprocessor1… formula mlp   rmse    standard   2.99     10  0.253 \n6 formula_nn Preprocessor1… formula mlp   rsq     standard   0.643    10  0.0834\n\nmetrics |&gt; pivot_wider(names_from = .metric, \n                       values_from = c(mean, std_err), \n                       id_cols = wflow_id) |&gt; \n  kable(align = 'c')\n\n\n\n\nwflow_id\nmean_rmse\nmean_rsq\nstd_err_rmse\nstd_err_rsq\n\n\n\n\nformula_lm\n3.180583\n0.6561715\n0.2883717\n0.0885333\n\n\nformula_rf\n3.142965\n0.6212505\n0.3399041\n0.0901077\n\n\nformula_nn\n2.994578\n0.6426283\n0.2528375\n0.0833891\n\n\n\n\n# Plot predictions against actual values for all predictions:\ncollect_predictions(result) |&gt; \n  pivot_longer(cols = model) |&gt; \n  ggplot(aes(speed, .pred, colour = value)) + \n  geom_point() + \n  geom_abline(slope = 1, intercept = 0) +\n  labs(x = 'Real value of speed', y = 'Prediction', colour = 'Model')"
  },
  {
    "objectID": "posts/2023-08-08-cars-dataset-and-tidymodels/2023-08-08-cars-dataset-and-tidymodels.html#open-questions",
    "href": "posts/2023-08-08-cars-dataset-and-tidymodels/2023-08-08-cars-dataset-and-tidymodels.html#open-questions",
    "title": "Cars dataset and tidymodels",
    "section": "Open Questions",
    "text": "Open Questions\n\nI don’t exactly understand how tidymodels performs cross-validation, how the metrics are calculated etc. Documentation of the tidymodels package has not been informative to me so far (probably I have to read more into it).\nHow to collect other metrics, e.g. mean absolute error?\nHow to conventiently join the predictors to the prediction tibble?\nHow do I extract honest coefficients from the linear model?"
  },
  {
    "objectID": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html",
    "href": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html",
    "title": "The iris dataset",
    "section": "",
    "text": "library('tidyverse')\nlibrary('lme4')"
  },
  {
    "objectID": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#init",
    "href": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#init",
    "title": "The iris dataset",
    "section": "",
    "text": "library('tidyverse')\nlibrary('lme4')"
  },
  {
    "objectID": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#data-preparation",
    "href": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#data-preparation",
    "title": "The iris dataset",
    "section": "Data preparation",
    "text": "Data preparation\n\niris_long &lt;- iris |&gt; \n  mutate(id = 1:nrow(iris)) |&gt; \n  pivot_longer(cols = -c(Species, id), \n               names_to = 'Variable', \n               values_to = 'Value')"
  },
  {
    "objectID": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#plots",
    "href": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#plots",
    "title": "The iris dataset",
    "section": "Plots",
    "text": "Plots\n\niris_long |&gt; \n  ggplot(aes(Variable, Value, color = Species)) + \n  geom_boxplot()\n\n\n\nlast_plot() + scale_y_log10() \n\n\n\niris_long |&gt; \n  ggplot(aes(Variable, Value, color = Species)) + \n  geom_jitter(height = 0)\n\n\n\nlast_plot() + scale_y_log10()"
  },
  {
    "objectID": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#models",
    "href": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#models",
    "title": "The iris dataset",
    "section": "Models",
    "text": "Models\n\nfitted_model &lt;- \n  lm(Value ~ Species * Variable, \n     data = iris_long)\nanova(fitted_model)\n\nAnalysis of Variance Table\n\nResponse: Value\n                  Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies            2  309.61  154.80 1019.34 &lt; 2.2e-16 ***\nVariable           3 1656.26  552.09 3635.35 &lt; 2.2e-16 ***\nSpecies:Variable   6  282.47   47.08  309.99 &lt; 2.2e-16 ***\nResiduals        588   89.30    0.15                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(fitted_model, col = iris_long$Species)\n\n\n\n\n\n\n\n\n\n\n\n\nfitted_model &lt;- \n  lm(log(Value) ~ Species * Variable, \n     data = iris_long)\nanova(fitted_model)\n\nAnalysis of Variance Table\n\nResponse: log(Value)\n                  Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nSpecies            2  90.843  45.421 1772.90 &lt; 2.2e-16 ***\nVariable           3 297.393  99.131 3869.31 &lt; 2.2e-16 ***\nSpecies:Variable   6  95.977  15.996  624.37 &lt; 2.2e-16 ***\nResiduals        588  15.064   0.026                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(fitted_model, col = iris_long$Species)\n\n\n\n\n\n\n\n\n\n\n\n\nfitted_model &lt;- \n  lmer(log(Value) ~ Species * Variable + (1 | id),\n     data = iris_long)\nfitted_model\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log(Value) ~ Species * Variable + (1 | id)\n   Data: iris_long\nREML criterion at convergence: -496.6563\nRandom effects:\n Groups   Name        Std.Dev.\n id       (Intercept) 0.08565 \n Residual             0.13522 \nNumber of obs: 600, groups:  id, 150\nFixed Effects:\n                           (Intercept)                       Speciesversicolor  \n                                0.3728                                  1.0702  \n                      Speciesvirginica                     VariablePetal.Width  \n                                1.3367                                 -1.8574  \n                  VariableSepal.Length                     VariableSepal.Width  \n                                1.2354                                  0.8531  \n Speciesversicolor:VariablePetal.Width    Speciesvirginica:VariablePetal.Width  \n                                0.6854                                  0.8447  \nSpeciesversicolor:VariableSepal.Length   Speciesvirginica:VariableSepal.Length  \n                               -0.9011                                 -1.0642  \n Speciesversicolor:VariableSepal.Width    Speciesvirginica:VariableSepal.Width  \n                               -1.2837                                 -1.4783"
  },
  {
    "objectID": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#conclusion",
    "href": "posts/2023-07-12-the-iris-dataset/2023-07-12-the-iris-dataset.html#conclusion",
    "title": "The iris dataset",
    "section": "Conclusion",
    "text": "Conclusion\n\nIn general, I like the log scale modeling - however there is a problem with the model on log scale\n\nEffects can be given as fold-changes - this seems to make sense to me\nSpecies “setosa” is not fitted adequately on log scale - probably measurement error is not adequately modeled on log scale - higher variance of error on log scale\n\nDependence on individual level is not considered in the plain linear model\n\nLinear mixed model should be considered!!!"
  },
  {
    "objectID": "posts/2022-11-24-r-interfaces-to-trial-registries/2022-11-24-r-interfaces-to-trial-registries.html",
    "href": "posts/2022-11-24-r-interfaces-to-trial-registries/2022-11-24-r-interfaces-to-trial-registries.html",
    "title": "R interfaces to trial registries",
    "section": "",
    "text": "library('tidyverse')\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary('ggVennDiagram')\nlibrary('clinicaltrialr') \n\nresults_q_twist &lt;- ct_read_results('https://www.clinicaltrials.gov/ct2/results?cond=&term=Q-TWiST&type=&rslt=&age_v=&gndr=&intr=&titles=&outc=&spons=&lead=&id=&cntry=&state=&city=&dist=&locn=&rsub=&strd_s=&strd_e=&prcd_s=&prcd_e=&sfpd_s=&sfpd_e=&rfpd_s=&rfpd_e=&lupd_s=&lupd_e=&sort=')\n\nRows: 26 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): NCT Number, Title, Acronym, Status, Study Results, Conditions, Int...\ndbl  (2): Rank, Enrollment\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nresults_qas &lt;- ct_read_results('https://www.clinicaltrials.gov/ct2/results?cond=&term=%22quality+adjusted+survival%22&type=&rslt=&age_v=&gndr=&intr=&titles=&outc=&spons=&lead=&id=&cntry=&state=&city=&dist=&locn=&rsub=&strd_s=&strd_e=&prcd_s=&prcd_e=&sfpd_s=&sfpd_e=&rfpd_s=&rfpd_e=&lupd_s=&lupd_e=&sort=')\n\nRows: 41 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): NCT Number, Title, Acronym, Status, Study Results, Conditions, Int...\ndbl  (2): Rank, Enrollment\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(results_qas)\n\nspc_tbl_ [41 × 27] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Rank                   : num [1:41] 1 2 3 4 5 6 7 8 9 10 ...\n $ NCT Number             : chr [1:41] \"NCT01117740\" \"NCT02284308\" \"NCT03326570\" \"NCT00591526\" ...\n $ Title                  : chr [1:41] \"Pleural Catheters Versus Thoracoscopic Pleurodesis\" \"Elderly With Locally Advanced Lung Cancer: Deciding Through Geriatric Assessment on the Optimal Treatment Strategy\" \"Effectiveness of Bronchoscopic Interventions for Malignant Airway Obstruction\" \"A Randomized Trial Assessing the Roles of AraC in Newly Diagnosed APL Promyelocytic Leukemia (APL)\" ...\n $ Acronym                : chr [1:41] NA \"ELDAPT\" NA NA ...\n $ Status                 : chr [1:41] \"Completed\" \"Recruiting\" \"Completed\" \"Completed\" ...\n $ Study Results          : chr [1:41] \"No Results Available\" \"No Results Available\" \"No Results Available\" \"No Results Available\" ...\n $ Conditions             : chr [1:41] \"Lung Cancer\" \"NSCLC\" \"Lung Cancer\" \"Leukemia, Promyelocytic, Acute\" ...\n $ Interventions          : chr [1:41] \"Behavioral: MDASI Questionnaire\" NA \"Behavioral: Medical Data Collection\" \"Drug: Arac\" ...\n $ Outcome Measures       : chr [1:41] \"Quality-Adjusted Survival Times|Patient Responses to MDASI\" \"Correlation GA with QAS (quality adjusted survival)|Geriatric assessment|Medical comparison between treatments|\"| __truncated__ \"Time to Any Complication Requiring Treatment|Quality-Adjusted Survival\" \"for patients with initial WBC counts &gt; 10000/mm3 - the main end point for this second randomization is relapse \"| __truncated__ ...\n $ Sponsor/Collaborators  : chr [1:41] \"M.D. Anderson Cancer Center\" \"Maastricht Radiation Oncology|Dutch Society of Physicians for Pulmonology and Tuberculosis|Erasmus Medical Cent\"| __truncated__ \"M.D. Anderson Cancer Center\" \"Groupe d'etude et de travail sur les leucemies aigues promyelocytaires|University Hospital, Lille\" ...\n $ Gender                 : chr [1:41] \"All\" \"All\" \"All\" \"All\" ...\n $ Age                    : chr [1:41] \"18 Years and older   (Adult, Older Adult)\" \"75 Years and older   (Older Adult)\" \"18 Years and older   (Adult, Older Adult)\" \"Child, Adult, Older Adult\" ...\n $ Phases                 : chr [1:41] NA NA NA \"Phase 3\" ...\n $ Enrollment             : num [1:41] 445 288 110 250 241 ...\n $ Funded Bys             : chr [1:41] \"Other\" \"Other|Industry\" \"Other\" \"Other\" ...\n $ Study Type             : chr [1:41] \"Observational\" \"Observational\" \"Observational\" \"Interventional\" ...\n $ Study Designs          : chr [1:41] \"Observational Model: Case-Only|Time Perspective: Prospective\" \"Observational Model: Cohort|Time Perspective: Prospective\" \"Observational Model: Cohort|Time Perspective: Prospective\" \"Allocation: Randomized|Intervention Model: Parallel Assignment|Masking: None (Open Label)|Primary Purpose: Treatment\" ...\n $ Other IDs              : chr [1:41] \"2010-0103\" \"ELDAPT\" \"2011-0563|NCI-2018-01276\" \"APL2000\" ...\n $ Start Date             : chr [1:41] \"April 2010\" \"July 2016\" \"August 25, 2011\" \"June 2000\" ...\n $ Primary Completion Date: chr [1:41] \"April 2016\" \"July 2024\" \"April 1, 2019\" \"June 2004\" ...\n $ Completion Date        : chr [1:41] \"April 2016\" \"July 2025\" \"April 1, 2019\" \"June 2004\" ...\n $ First Posted           : chr [1:41] \"May 5, 2010\" \"November 6, 2014\" \"October 31, 2017\" \"January 11, 2008\" ...\n $ Results First Posted   : chr [1:41] NA NA NA NA ...\n $ Last Update Posted     : chr [1:41] \"April 11, 2016\" \"March 27, 2023\" \"April 16, 2019\" \"January 14, 2008\" ...\n $ Locations              : chr [1:41] \"University of Texas MD Anderson Cancer Center, Houston, Texas, United States\" \"Ziekenhuisgroep Twente, Almelo, Netherlands|Gelre ziekenhuis, Apeldoorn, Netherlands|Radiotherapiegroep Arnhem,\"| __truncated__ \"University of Texas MD Anderson Cancer Center, Houston, Texas, United States\" NA ...\n $ Study Documents        : chr [1:41] NA NA NA NA ...\n $ URL                    : chr [1:41] \"https://ClinicalTrials.gov/show/NCT01117740\" \"https://ClinicalTrials.gov/show/NCT02284308\" \"https://ClinicalTrials.gov/show/NCT03326570\" \"https://ClinicalTrials.gov/show/NCT00591526\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Rank = col_double(),\n  ..   `NCT Number` = col_character(),\n  ..   Title = col_character(),\n  ..   Acronym = col_character(),\n  ..   Status = col_character(),\n  ..   `Study Results` = col_character(),\n  ..   Conditions = col_character(),\n  ..   Interventions = col_character(),\n  ..   `Outcome Measures` = col_character(),\n  ..   `Sponsor/Collaborators` = col_character(),\n  ..   Gender = col_character(),\n  ..   Age = col_character(),\n  ..   Phases = col_character(),\n  ..   Enrollment = col_double(),\n  ..   `Funded Bys` = col_character(),\n  ..   `Study Type` = col_character(),\n  ..   `Study Designs` = col_character(),\n  ..   `Other IDs` = col_character(),\n  ..   `Start Date` = col_character(),\n  ..   `Primary Completion Date` = col_character(),\n  ..   `Completion Date` = col_character(),\n  ..   `First Posted` = col_character(),\n  ..   `Results First Posted` = col_character(),\n  ..   `Last Update Posted` = col_character(),\n  ..   Locations = col_character(),\n  ..   `Study Documents` = col_character(),\n  ..   URL = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# List of items\nx &lt;- list('Keyword \"Quality-adjusted survival\"' = results_qas$`NCT Number`, \n          'Keyword \"Q-TWiST\"' = results_q_twist$`NCT Number`)\n\n# 2D Venn diagram\nggVennDiagram(x) + theme(legend.position = 'none')"
  },
  {
    "objectID": "posts/2022-11-24-r-interfaces-to-trial-registries/2022-11-24-r-interfaces-to-trial-registries.html#package-clinicaltrialr",
    "href": "posts/2022-11-24-r-interfaces-to-trial-registries/2022-11-24-r-interfaces-to-trial-registries.html#package-clinicaltrialr",
    "title": "R interfaces to trial registries",
    "section": "",
    "text": "library('tidyverse')\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary('ggVennDiagram')\nlibrary('clinicaltrialr') \n\nresults_q_twist &lt;- ct_read_results('https://www.clinicaltrials.gov/ct2/results?cond=&term=Q-TWiST&type=&rslt=&age_v=&gndr=&intr=&titles=&outc=&spons=&lead=&id=&cntry=&state=&city=&dist=&locn=&rsub=&strd_s=&strd_e=&prcd_s=&prcd_e=&sfpd_s=&sfpd_e=&rfpd_s=&rfpd_e=&lupd_s=&lupd_e=&sort=')\n\nRows: 26 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): NCT Number, Title, Acronym, Status, Study Results, Conditions, Int...\ndbl  (2): Rank, Enrollment\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nresults_qas &lt;- ct_read_results('https://www.clinicaltrials.gov/ct2/results?cond=&term=%22quality+adjusted+survival%22&type=&rslt=&age_v=&gndr=&intr=&titles=&outc=&spons=&lead=&id=&cntry=&state=&city=&dist=&locn=&rsub=&strd_s=&strd_e=&prcd_s=&prcd_e=&sfpd_s=&sfpd_e=&rfpd_s=&rfpd_e=&lupd_s=&lupd_e=&sort=')\n\nRows: 41 Columns: 27\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (25): NCT Number, Title, Acronym, Status, Study Results, Conditions, Int...\ndbl  (2): Rank, Enrollment\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(results_qas)\n\nspc_tbl_ [41 × 27] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Rank                   : num [1:41] 1 2 3 4 5 6 7 8 9 10 ...\n $ NCT Number             : chr [1:41] \"NCT01117740\" \"NCT02284308\" \"NCT03326570\" \"NCT00591526\" ...\n $ Title                  : chr [1:41] \"Pleural Catheters Versus Thoracoscopic Pleurodesis\" \"Elderly With Locally Advanced Lung Cancer: Deciding Through Geriatric Assessment on the Optimal Treatment Strategy\" \"Effectiveness of Bronchoscopic Interventions for Malignant Airway Obstruction\" \"A Randomized Trial Assessing the Roles of AraC in Newly Diagnosed APL Promyelocytic Leukemia (APL)\" ...\n $ Acronym                : chr [1:41] NA \"ELDAPT\" NA NA ...\n $ Status                 : chr [1:41] \"Completed\" \"Recruiting\" \"Completed\" \"Completed\" ...\n $ Study Results          : chr [1:41] \"No Results Available\" \"No Results Available\" \"No Results Available\" \"No Results Available\" ...\n $ Conditions             : chr [1:41] \"Lung Cancer\" \"NSCLC\" \"Lung Cancer\" \"Leukemia, Promyelocytic, Acute\" ...\n $ Interventions          : chr [1:41] \"Behavioral: MDASI Questionnaire\" NA \"Behavioral: Medical Data Collection\" \"Drug: Arac\" ...\n $ Outcome Measures       : chr [1:41] \"Quality-Adjusted Survival Times|Patient Responses to MDASI\" \"Correlation GA with QAS (quality adjusted survival)|Geriatric assessment|Medical comparison between treatments|\"| __truncated__ \"Time to Any Complication Requiring Treatment|Quality-Adjusted Survival\" \"for patients with initial WBC counts &gt; 10000/mm3 - the main end point for this second randomization is relapse \"| __truncated__ ...\n $ Sponsor/Collaborators  : chr [1:41] \"M.D. Anderson Cancer Center\" \"Maastricht Radiation Oncology|Dutch Society of Physicians for Pulmonology and Tuberculosis|Erasmus Medical Cent\"| __truncated__ \"M.D. Anderson Cancer Center\" \"Groupe d'etude et de travail sur les leucemies aigues promyelocytaires|University Hospital, Lille\" ...\n $ Gender                 : chr [1:41] \"All\" \"All\" \"All\" \"All\" ...\n $ Age                    : chr [1:41] \"18 Years and older   (Adult, Older Adult)\" \"75 Years and older   (Older Adult)\" \"18 Years and older   (Adult, Older Adult)\" \"Child, Adult, Older Adult\" ...\n $ Phases                 : chr [1:41] NA NA NA \"Phase 3\" ...\n $ Enrollment             : num [1:41] 445 288 110 250 241 ...\n $ Funded Bys             : chr [1:41] \"Other\" \"Other|Industry\" \"Other\" \"Other\" ...\n $ Study Type             : chr [1:41] \"Observational\" \"Observational\" \"Observational\" \"Interventional\" ...\n $ Study Designs          : chr [1:41] \"Observational Model: Case-Only|Time Perspective: Prospective\" \"Observational Model: Cohort|Time Perspective: Prospective\" \"Observational Model: Cohort|Time Perspective: Prospective\" \"Allocation: Randomized|Intervention Model: Parallel Assignment|Masking: None (Open Label)|Primary Purpose: Treatment\" ...\n $ Other IDs              : chr [1:41] \"2010-0103\" \"ELDAPT\" \"2011-0563|NCI-2018-01276\" \"APL2000\" ...\n $ Start Date             : chr [1:41] \"April 2010\" \"July 2016\" \"August 25, 2011\" \"June 2000\" ...\n $ Primary Completion Date: chr [1:41] \"April 2016\" \"July 2024\" \"April 1, 2019\" \"June 2004\" ...\n $ Completion Date        : chr [1:41] \"April 2016\" \"July 2025\" \"April 1, 2019\" \"June 2004\" ...\n $ First Posted           : chr [1:41] \"May 5, 2010\" \"November 6, 2014\" \"October 31, 2017\" \"January 11, 2008\" ...\n $ Results First Posted   : chr [1:41] NA NA NA NA ...\n $ Last Update Posted     : chr [1:41] \"April 11, 2016\" \"March 27, 2023\" \"April 16, 2019\" \"January 14, 2008\" ...\n $ Locations              : chr [1:41] \"University of Texas MD Anderson Cancer Center, Houston, Texas, United States\" \"Ziekenhuisgroep Twente, Almelo, Netherlands|Gelre ziekenhuis, Apeldoorn, Netherlands|Radiotherapiegroep Arnhem,\"| __truncated__ \"University of Texas MD Anderson Cancer Center, Houston, Texas, United States\" NA ...\n $ Study Documents        : chr [1:41] NA NA NA NA ...\n $ URL                    : chr [1:41] \"https://ClinicalTrials.gov/show/NCT01117740\" \"https://ClinicalTrials.gov/show/NCT02284308\" \"https://ClinicalTrials.gov/show/NCT03326570\" \"https://ClinicalTrials.gov/show/NCT00591526\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Rank = col_double(),\n  ..   `NCT Number` = col_character(),\n  ..   Title = col_character(),\n  ..   Acronym = col_character(),\n  ..   Status = col_character(),\n  ..   `Study Results` = col_character(),\n  ..   Conditions = col_character(),\n  ..   Interventions = col_character(),\n  ..   `Outcome Measures` = col_character(),\n  ..   `Sponsor/Collaborators` = col_character(),\n  ..   Gender = col_character(),\n  ..   Age = col_character(),\n  ..   Phases = col_character(),\n  ..   Enrollment = col_double(),\n  ..   `Funded Bys` = col_character(),\n  ..   `Study Type` = col_character(),\n  ..   `Study Designs` = col_character(),\n  ..   `Other IDs` = col_character(),\n  ..   `Start Date` = col_character(),\n  ..   `Primary Completion Date` = col_character(),\n  ..   `Completion Date` = col_character(),\n  ..   `First Posted` = col_character(),\n  ..   `Results First Posted` = col_character(),\n  ..   `Last Update Posted` = col_character(),\n  ..   Locations = col_character(),\n  ..   `Study Documents` = col_character(),\n  ..   URL = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# List of items\nx &lt;- list('Keyword \"Quality-adjusted survival\"' = results_qas$`NCT Number`, \n          'Keyword \"Q-TWiST\"' = results_q_twist$`NCT Number`)\n\n# 2D Venn diagram\nggVennDiagram(x) + theme(legend.position = 'none')"
  },
  {
    "objectID": "posts/2022-11-24-r-interfaces-to-trial-registries/2022-11-24-r-interfaces-to-trial-registries.html#package-cthist",
    "href": "posts/2022-11-24-r-interfaces-to-trial-registries/2022-11-24-r-interfaces-to-trial-registries.html#package-cthist",
    "title": "R interfaces to trial registries",
    "section": "Package ‘cthist’",
    "text": "Package ‘cthist’\n\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC9249399/\n\n\nlibrary(tidyverse)\nlibrary(cthist)\nlibrary(lubridate)"
  },
  {
    "objectID": "posts/2023-08-04-getting-to-know-tidymodels/2023-08-04-getting-to-know-tidymodels.html",
    "href": "posts/2023-08-04-getting-to-know-tidymodels/2023-08-04-getting-to-know-tidymodels.html",
    "title": "Getting to know tidymodels",
    "section": "",
    "text": "Tutorials by Julia Silge: Link\nBlog post criticising tidymodels: Link, written by this person"
  },
  {
    "objectID": "posts/2023-08-04-getting-to-know-tidymodels/2023-08-04-getting-to-know-tidymodels.html#which-packages-belong-to-the-metapackage-tidymodels",
    "href": "posts/2023-08-04-getting-to-know-tidymodels/2023-08-04-getting-to-know-tidymodels.html#which-packages-belong-to-the-metapackage-tidymodels",
    "title": "Getting to know tidymodels",
    "section": "Which packages belong to the metapackage tidymodels",
    "text": "Which packages belong to the metapackage tidymodels\n\ntidymodels::tidymodels_packages(include_self = TRUE)\n\n [1] \"broom\"        \"cli\"          \"conflicted\"   \"dials\"        \"dplyr\"       \n [6] \"ggplot2\"      \"hardhat\"      \"infer\"        \"modeldata\"    \"parsnip\"     \n[11] \"purrr\"        \"recipes\"      \"rlang\"        \"rsample\"      \"rstudioapi\"  \n[16] \"tibble\"       \"tidyr\"        \"tune\"         \"workflows\"    \"workflowsets\"\n[21] \"yardstick\"    \"tidymodels\""
  },
  {
    "objectID": "posts/2023-08-04-getting-to-know-tidymodels/2023-08-04-getting-to-know-tidymodels.html#tutorial-1---palmerpenguins-dataset",
    "href": "posts/2023-08-04-getting-to-know-tidymodels/2023-08-04-getting-to-know-tidymodels.html#tutorial-1---palmerpenguins-dataset",
    "title": "Getting to know tidymodels",
    "section": "Tutorial 1 - palmerpenguins dataset",
    "text": "Tutorial 1 - palmerpenguins dataset\n\nLink\n\n\nset.seed(1)\nlibrary('tidyverse')\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary('palmerpenguins')\n\npenguins_df &lt;- penguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  select(-year, -island)\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n\n\n✔ broom        1.0.4     ✔ rsample      1.1.1\n✔ dials        1.2.0     ✔ tune         1.1.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.0     ✔ yardstick    1.2.0\n✔ recipes      1.0.6     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nglimpse(penguins_df)\n\nRows: 333\nColumns: 6\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, 36.7, 39.3, 38.9, 39.2, 41.1, 38.6…\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, 19.3, 20.6, 17.8, 19.6, 17.6, 21.2…\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, 193, 190, 181, 195, 182, 191, 198, 18…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, 3450, 3650, 3625, 4675, 3200, 3800…\n$ sex               &lt;fct&gt; male, female, female, female, male, female, male, fe…\n\npenguin_split &lt;- initial_split(penguins_df, strata = sex)\nstr(penguin_split)\n\nList of 4\n $ data  : tibble [333 × 6] (S3: tbl_df/tbl/data.frame)\n  ..$ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n  ..$ bill_length_mm   : num [1:333] 39.1 39.5 40.3 36.7 39.3 38.9 39.2 41.1 38.6 34.6 ...\n  ..$ bill_depth_mm    : num [1:333] 18.7 17.4 18 19.3 20.6 17.8 19.6 17.6 21.2 21.1 ...\n  ..$ flipper_length_mm: int [1:333] 181 186 195 193 190 181 195 182 191 198 ...\n  ..$ body_mass_g      : int [1:333] 3750 3800 3250 3450 3650 3625 4675 3200 3800 4400 ...\n  ..$ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 1 2 1 2 2 ...\n $ in_id : int [1:249] 2 3 4 11 12 18 24 26 28 33 ...\n $ out_id: logi NA\n $ id    : tibble [1 × 1] (S3: tbl_df/tbl/data.frame)\n  ..$ id: chr \"Resample1\"\n - attr(*, \"class\")= chr [1:3] \"initial_split\" \"mc_split\" \"rsplit\"\n\npenguin_train &lt;- training(penguin_split)\nstr(penguin_train)\n\ntibble [249 × 6] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ bill_length_mm   : num [1:249] 39.5 40.3 36.7 36.6 38.7 35.9 37.9 39.5 39.5 42.2 ...\n $ bill_depth_mm    : num [1:249] 17.4 18 19.3 17.8 19 19.2 18.6 16.7 17.8 18.5 ...\n $ flipper_length_mm: int [1:249] 186 195 193 185 195 189 172 178 188 180 ...\n $ body_mass_g      : int [1:249] 3800 3250 3450 3700 3450 3800 3150 3250 3300 3550 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 1 1 1 1 1 1 1 1 1 1 ...\n\npenguin_test &lt;- testing(penguin_split)\n\n\nset.seed(123)\npenguin_boot &lt;- bootstraps(penguin_train)\npenguin_boot\n\n# Bootstrap sampling \n# A tibble: 25 × 2\n   splits           id         \n   &lt;list&gt;           &lt;chr&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01\n 2 &lt;split [249/91]&gt; Bootstrap02\n 3 &lt;split [249/90]&gt; Bootstrap03\n 4 &lt;split [249/91]&gt; Bootstrap04\n 5 &lt;split [249/85]&gt; Bootstrap05\n 6 &lt;split [249/87]&gt; Bootstrap06\n 7 &lt;split [249/94]&gt; Bootstrap07\n 8 &lt;split [249/88]&gt; Bootstrap08\n 9 &lt;split [249/95]&gt; Bootstrap09\n10 &lt;split [249/89]&gt; Bootstrap10\n# ℹ 15 more rows\n\n\n\nQuestion:\n\n# how can I extract a single bootstrap sample?\nx &lt;- penguin_boot |&gt; \n  select(splits) |&gt; \n  slice(2) |&gt;\n  pull()\n\nx[[1]]$data\n\n# A tibble: 249 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie            39.5          17.4               186        3800 female\n 2 Adelie            40.3          18                 195        3250 female\n 3 Adelie            36.7          19.3               193        3450 female\n 4 Adelie            36.6          17.8               185        3700 female\n 5 Adelie            38.7          19                 195        3450 female\n 6 Adelie            35.9          19.2               189        3800 female\n 7 Adelie            37.9          18.6               172        3150 female\n 8 Adelie            39.5          16.7               178        3250 female\n 9 Adelie            39.5          17.8               188        3300 female\n10 Adelie            42.2          18.5               180        3550 female\n# ℹ 239 more rows\n\nx &lt;- penguin_boot |&gt; \n  filter(id == 'Bootstrap01') |&gt; \n  select(splits) |&gt; \n  pull()\n\nx[[1]]$data\n\n# A tibble: 249 × 6\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex   \n   &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt; \n 1 Adelie            39.5          17.4               186        3800 female\n 2 Adelie            40.3          18                 195        3250 female\n 3 Adelie            36.7          19.3               193        3450 female\n 4 Adelie            36.6          17.8               185        3700 female\n 5 Adelie            38.7          19                 195        3450 female\n 6 Adelie            35.9          19.2               189        3800 female\n 7 Adelie            37.9          18.6               172        3150 female\n 8 Adelie            39.5          16.7               178        3250 female\n 9 Adelie            39.5          17.8               188        3300 female\n10 Adelie            42.2          18.5               180        3550 female\n# ℹ 239 more rows\n\n\n\nglm_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\n\nrf_spec &lt;- rand_forest() %&gt;%\n  set_mode(\"classification\") %&gt;%\n  set_engine(\"ranger\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nComputational engine: ranger \n\n\n\npenguin_wf &lt;- workflow() %&gt;%\n  add_formula(sex ~ .)\n\npenguin_wf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Formula\nModel: None\n\n── Preprocessor ────────────────────────────────────────────────────────────────\nsex ~ .\n\n\n\nglm_rs &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n→ A | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\n\n\nglm_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 × 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [2 × 4]&gt; &lt;tibble [1 × 3]&gt; &lt;tibble&gt;    \n# ℹ 15 more rows\n\nThere were issues with some computations:\n\n  - Warning(s) x2: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\n\nrf_rs &lt;- penguin_wf %&gt;%\n  add_model(rf_spec) %&gt;%\n  fit_resamples(\n    resamples = penguin_boot,\n    control = control_resamples(save_pred = TRUE)\n  )\n\nrf_rs\n\n# Resampling results\n# Bootstrap sampling \n# A tibble: 25 × 5\n   splits           id          .metrics         .notes           .predictions\n   &lt;list&gt;           &lt;chr&gt;       &lt;list&gt;           &lt;list&gt;           &lt;list&gt;      \n 1 &lt;split [249/93]&gt; Bootstrap01 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 2 &lt;split [249/91]&gt; Bootstrap02 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 3 &lt;split [249/90]&gt; Bootstrap03 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 4 &lt;split [249/91]&gt; Bootstrap04 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 5 &lt;split [249/85]&gt; Bootstrap05 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 6 &lt;split [249/87]&gt; Bootstrap06 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 7 &lt;split [249/94]&gt; Bootstrap07 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 8 &lt;split [249/88]&gt; Bootstrap08 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n 9 &lt;split [249/95]&gt; Bootstrap09 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n10 &lt;split [249/89]&gt; Bootstrap10 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble&gt;    \n# ℹ 15 more rows\n\n\n\ncollect_metrics(rf_rs)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.899    25 0.00704 Preprocessor1_Model1\n2 roc_auc  binary     0.971    25 0.00307 Preprocessor1_Model1\n\ncollect_metrics(glm_rs)\n\n# A tibble: 2 × 6\n  .metric  .estimator  mean     n std_err .config             \n  &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary     0.919    25 0.00527 Preprocessor1_Model1\n2 roc_auc  binary     0.977    25 0.00220 Preprocessor1_Model1\n\n\n\nglm_rs %&gt;%\n  conf_mat_resampled()\n\n# A tibble: 4 × 3\n  Prediction Truth   Freq\n  &lt;fct&gt;      &lt;fct&gt;  &lt;dbl&gt;\n1 female     female 41.8 \n2 female     male    3.72\n3 male       female  3.68\n4 male       male   41.6 \n\n\n\nglm_rs %&gt;%\n  collect_predictions() %&gt;%\n  group_by(id) %&gt;%\n  roc_curve(sex, .pred_female) %&gt;%\n  ggplot(aes(1 - specificity, sensitivity, color = id)) +\n  geom_abline(lty = 2, color = \"gray80\", linewidth = 1.5) +\n  geom_path(show.legend = FALSE, alpha = 0.6, linewidth = 1.2) +\n  coord_equal()\n\n\n\n\n\npenguin_final &lt;- penguin_wf %&gt;%\n  add_model(glm_spec) %&gt;%\n  last_fit(penguin_split)\n\npenguin_final\n\n# Resampling results\n# Manual resampling \n# A tibble: 1 × 6\n  splits           id               .metrics .notes   .predictions .workflow \n  &lt;list&gt;           &lt;chr&gt;            &lt;list&gt;   &lt;list&gt;   &lt;list&gt;       &lt;list&gt;    \n1 &lt;split [249/84]&gt; train/test split &lt;tibble&gt; &lt;tibble&gt; &lt;tibble&gt;     &lt;workflow&gt;\n\n\n\ncollect_metrics(penguin_final)\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.905 Preprocessor1_Model1\n2 roc_auc  binary         0.966 Preprocessor1_Model1\n\n\n\ncollect_predictions(penguin_final) %&gt;%\n  conf_mat(sex, .pred_class)\n\n          Truth\nPrediction female male\n    female     39    5\n    male        3   37\n\n\n\npenguin_final$.workflow[[1]] %&gt;%\n  tidy(exponentiate = TRUE)\n\n# A tibble: 7 × 5\n  term              estimate std.error statistic     p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)       2.35e-42  18.1        -5.30  0.000000115\n2 speciesChinstrap  9.76e- 4   1.91       -3.62  0.000295   \n3 speciesGentoo     1.96e- 4   3.25       -2.63  0.00866    \n4 bill_length_mm    1.88e+ 0   0.159       4.00  0.0000639  \n5 bill_depth_mm     7.50e+ 0   0.455       4.43  0.00000940 \n6 flipper_length_mm 1.06e+ 0   0.0653      0.863 0.388      \n7 body_mass_g       1.01e+ 0   0.00138     4.63  0.00000374 \n\n\n\npenguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  ggplot(aes(bill_depth_mm, bill_length_mm, color = sex, size = body_mass_g)) +\n  geom_point(alpha = 0.5) +\n  facet_wrap(~species)"
  },
  {
    "objectID": "posts/2023-07-06-python-versions-environments/2023-07-06-python-versions-environments.html",
    "href": "posts/2023-07-06-python-versions-environments/2023-07-06-python-versions-environments.html",
    "title": "Managing python versions in RStudio",
    "section": "",
    "text": "Good resources are found in the {reticulate} vignettes: Link, in particular this\nLink to question on stackoverflow - Link"
  },
  {
    "objectID": "posts/2023-07-06-python-versions-environments/2023-07-06-python-versions-environments.html#ressources",
    "href": "posts/2023-07-06-python-versions-environments/2023-07-06-python-versions-environments.html#ressources",
    "title": "Managing python versions in RStudio",
    "section": "",
    "text": "Good resources are found in the {reticulate} vignettes: Link, in particular this\nLink to question on stackoverflow - Link"
  },
  {
    "objectID": "posts/2023-01-05-contribution-plots/2023-01-05-contribution-plots.html",
    "href": "posts/2023-01-05-contribution-plots/2023-01-05-contribution-plots.html",
    "title": "Contribution plots",
    "section": "",
    "text": "library('tibble')\nlibrary('ggplot2')\nlibrary('RColorBrewer')\nlibrary('plotly')\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\ntheme_set(theme_minimal())\n#help(package = 'RColorBrewer')\nnr_of_clones &lt;- 11\nnr_of_timepoins &lt;- 20\n\ndat &lt;- tibble(t = rep(1:nr_of_timepoins, each = nr_of_clones),\n              p = abs(rnorm(nr_of_clones * nr_of_timepoins)),\n              s = rep(1:nr_of_clones, nr_of_timepoins))\n\ndat$p &lt;- dat$p / rep(tapply(dat$p, dat$t, sum), each = nr_of_clones)\n\n# my_cols &lt;- rainbow(nr_of_clones)\nmy_cols &lt;- brewer.pal(nr_of_clones, 'Spectral')\n\n(g &lt;- ggplot(dat, aes(x = t, y = p, group = factor(s), fill = factor(s))) + \n  geom_area(alpha = 0.65) + \n  scale_fill_manual(name = 'State', values = my_cols) +\n  xlab('Time') + ylab('Relative contibution'))\n\n\n\n# ggplotly(g)"
  },
  {
    "objectID": "posts/2023-01-05-contribution-plots/2023-01-05-contribution-plots.html#contribution-plot-using-ggplot2-and-plotly",
    "href": "posts/2023-01-05-contribution-plots/2023-01-05-contribution-plots.html#contribution-plot-using-ggplot2-and-plotly",
    "title": "Contribution plots",
    "section": "",
    "text": "library('tibble')\nlibrary('ggplot2')\nlibrary('RColorBrewer')\nlibrary('plotly')\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\ntheme_set(theme_minimal())\n#help(package = 'RColorBrewer')\nnr_of_clones &lt;- 11\nnr_of_timepoins &lt;- 20\n\ndat &lt;- tibble(t = rep(1:nr_of_timepoins, each = nr_of_clones),\n              p = abs(rnorm(nr_of_clones * nr_of_timepoins)),\n              s = rep(1:nr_of_clones, nr_of_timepoins))\n\ndat$p &lt;- dat$p / rep(tapply(dat$p, dat$t, sum), each = nr_of_clones)\n\n# my_cols &lt;- rainbow(nr_of_clones)\nmy_cols &lt;- brewer.pal(nr_of_clones, 'Spectral')\n\n(g &lt;- ggplot(dat, aes(x = t, y = p, group = factor(s), fill = factor(s))) + \n  geom_area(alpha = 0.65) + \n  scale_fill_manual(name = 'State', values = my_cols) +\n  xlab('Time') + ylab('Relative contibution'))\n\n\n\n# ggplotly(g)"
  },
  {
    "objectID": "posts/2023-01-05-contribution-plots/2023-01-05-contribution-plots.html#another-plotly-plot",
    "href": "posts/2023-01-05-contribution-plots/2023-01-05-contribution-plots.html#another-plotly-plot",
    "title": "Contribution plots",
    "section": "Another plotly plot",
    "text": "Another plotly plot\n\n(g &lt;- ggplot(cars, aes(dist, speed)) + geom_point())\n\n\n\n# ggplotly(g)"
  },
  {
    "objectID": "posts/2023-07-20-starting-with-fastai/2023-07-20-starting-with-fastai.html",
    "href": "posts/2023-07-20-starting-with-fastai/2023-07-20-starting-with-fastai.html",
    "title": "Starting with fastai",
    "section": "",
    "text": "Link to the book\n\n\nlibrary('tidyverse')\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(1)\n\nlibrary('reticulate')\n\n# conda_create('machine_learning')\nuse_condaenv('machine_learning')\n\n# conda_install('machine_learning', 'fastai')\n# conda_install('machine_learning', 'torchvision')\n# conda_install('machine_learning', 'pytorch')\n\n\n#id first_training\n#caption Results from the first training\n# CLICK ME\n\n\nimport torch\nx = torch.rand(5, 3)\nprint(x)\n\ntensor([[0.7172, 0.8668, 0.7743],\n        [0.0462, 0.2161, 0.4469],\n        [0.2911, 0.3335, 0.2292],\n        [0.1917, 0.9395, 0.4347],\n        [0.9052, 0.2584, 0.6975]])\n\nfrom fastai.vision.all import *\n\n/Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n  Referenced from: &lt;8CBD0B78-6C7C-3C8B-8C76-ACA7B6112818&gt; /Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torchvision/image.so\n  Expected in:     &lt;07CB8E54-8386-3606-A01E-B92223F93B74&gt; /Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n    path, get_image_files(path), valid_pct=0.2, seed=42,\n    label_func=is_cat, item_tfms=Resize(224))\n\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\n\n/Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/seb/Library/r-miniconda-arm64/envs/machine_learning/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n\nlearn.fine_tune(1)\n\n█\nepoch     train_loss  valid_loss  error_rate  time    \n█\n\nEpoch 1/1 : |----------------------------------------| 0.00% [0/92 00:00&lt;?]\n\nEpoch 1/1 : |----------------------------------------| 1.09% [1/92 00:05&lt;07:54]\n\nEpoch 1/1 : |----------------------------------------| 2.17% [2/92 00:05&lt;04:20 1.4611]\n\nEpoch 1/1 : |█---------------------------------------| 3.26% [3/92 00:06&lt;03:08 1.4571]\n\nEpoch 1/1 : |█---------------------------------------| 4.35% [4/92 00:06&lt;02:32 1.5097]\n\nEpoch 1/1 : |██--------------------------------------| 5.43% [5/92 00:07&lt;02:11 1.4588]\n\nEpoch 1/1 : |██--------------------------------------| 6.52% [6/92 00:08&lt;01:56 1.4491]\n\nEpoch 1/1 : |███-------------------------------------| 7.61% [7/92 00:08&lt;01:46 1.3571]\n\nEpoch 1/1 : |███-------------------------------------| 8.70% [8/92 00:09&lt;01:37 1.3298]\n\nEpoch 1/1 : |███-------------------------------------| 9.78% [9/92 00:09&lt;01:31 1.2889]\n\nEpoch 1/1 : |████------------------------------------| 10.87% [10/92 00:10&lt;01:25 1.2722]\n\nEpoch 1/1 : |████------------------------------------| 11.96% [11/92 00:11&lt;01:21 1.2364]\n\nEpoch 1/1 : |█████-----------------------------------| 13.04% [12/92 00:11&lt;01:17 1.1908]\n\nEpoch 1/1 : |█████-----------------------------------| 14.13% [13/92 00:12&lt;01:14 1.1453]\n\nEpoch 1/1 : |██████----------------------------------| 15.22% [14/92 00:12&lt;01:11 1.1057]\n\nEpoch 1/1 : |██████----------------------------------| 16.30% [15/92 00:13&lt;01:08 1.0928]\n\nEpoch 1/1 : |██████----------------------------------| 17.39% [16/92 00:13&lt;01:06 1.0707]\n\nEpoch 1/1 : |███████---------------------------------| 18.48% [17/92 00:14&lt;01:04 1.0227]\n\nEpoch 1/1 : |███████---------------------------------| 19.57% [18/92 00:15&lt;01:02 1.0178]\n\nEpoch 1/1 : |████████--------------------------------| 20.65% [19/92 00:15&lt;01:00 0.9841]\n\nEpoch 1/1 : |████████--------------------------------| 21.74% [20/92 00:16&lt;00:58 0.9457]\n\nEpoch 1/1 : |█████████-------------------------------| 22.83% [21/92 00:16&lt;00:57 0.9079]\n\nEpoch 1/1 : |█████████-------------------------------| 23.91% [22/92 00:17&lt;00:55 0.8819]\n\nEpoch 1/1 : |██████████------------------------------| 25.00% [23/92 00:18&lt;00:54 0.8478]\n\nEpoch 1/1 : |██████████------------------------------| 26.09% [24/92 00:18&lt;00:52 0.8204]\n\nEpoch 1/1 : |██████████------------------------------| 27.17% [25/92 00:19&lt;00:51 0.7875]\n\nEpoch 1/1 : |███████████-----------------------------| 28.26% [26/92 00:19&lt;00:50 0.7594]\n\nEpoch 1/1 : |███████████-----------------------------| 29.35% [27/92 00:20&lt;00:48 0.7458]\n\nEpoch 1/1 : |████████████----------------------------| 30.43% [28/92 00:20&lt;00:47 0.7307]\n\nEpoch 1/1 : |████████████----------------------------| 31.52% [29/92 00:21&lt;00:46 0.7085]\n\nEpoch 1/1 : |█████████████---------------------------| 32.61% [30/92 00:22&lt;00:45 0.6856]\n\nEpoch 1/1 : |█████████████---------------------------| 33.70% [31/92 00:22&lt;00:44 0.6655]\n\nEpoch 1/1 : |█████████████---------------------------| 34.78% [32/92 00:23&lt;00:43 0.6427]\n\nEpoch 1/1 : |██████████████--------------------------| 35.87% [33/92 00:23&lt;00:42 0.6247]\n\nEpoch 1/1 : |██████████████--------------------------| 36.96% [34/92 00:24&lt;00:41 0.6038]\n\nEpoch 1/1 : |███████████████-------------------------| 38.04% [35/92 00:24&lt;00:40 0.5811]\n\nEpoch 1/1 : |███████████████-------------------------| 39.13% [36/92 00:25&lt;00:39 0.5666]\n\nEpoch 1/1 : |████████████████------------------------| 40.22% [37/92 00:25&lt;00:38 0.5499]\n\nEpoch 1/1 : |████████████████------------------------| 41.30% [38/92 00:26&lt;00:37 0.5333]\n\nEpoch 1/1 : |████████████████------------------------| 42.39% [39/92 00:27&lt;00:36 0.5166]\n\nEpoch 1/1 : |█████████████████-----------------------| 43.48% [40/92 00:27&lt;00:35 0.4998]\n\nEpoch 1/1 : |█████████████████-----------------------| 44.57% [41/92 00:28&lt;00:35 0.4840]\n\nEpoch 1/1 : |██████████████████----------------------| 45.65% [42/92 00:28&lt;00:34 0.4746]\n\nEpoch 1/1 : |██████████████████----------------------| 46.74% [43/92 00:29&lt;00:33 0.4632]\n\nEpoch 1/1 : |███████████████████---------------------| 47.83% [44/92 00:29&lt;00:32 0.4558]\n\nEpoch 1/1 : |███████████████████---------------------| 48.91% [45/92 00:30&lt;00:31 0.4430]\n\nEpoch 1/1 : |████████████████████--------------------| 50.00% [46/92 00:30&lt;00:30 0.4459]\n\nEpoch 1/1 : |████████████████████--------------------| 51.09% [47/92 00:31&lt;00:30 0.4323]\n\nEpoch 1/1 : |████████████████████--------------------| 52.17% [48/92 00:32&lt;00:29 0.4198]\n\nEpoch 1/1 : |█████████████████████-------------------| 53.26% [49/92 00:32&lt;00:28 0.4096]\n\nEpoch 1/1 : |█████████████████████-------------------| 54.35% [50/92 00:33&lt;00:27 0.3991]\n\nEpoch 1/1 : |██████████████████████------------------| 55.43% [51/92 00:33&lt;00:27 0.3871]\n\nEpoch 1/1 : |██████████████████████------------------| 56.52% [52/92 00:34&lt;00:26 0.3804]\n\nEpoch 1/1 : |███████████████████████-----------------| 57.61% [53/92 00:34&lt;00:25 0.3694]\n\nEpoch 1/1 : |███████████████████████-----------------| 58.70% [54/92 00:35&lt;00:24 0.3596]\n\nEpoch 1/1 : |███████████████████████-----------------| 59.78% [55/92 00:35&lt;00:24 0.3558]\n\nEpoch 1/1 : |████████████████████████----------------| 60.87% [56/92 00:36&lt;00:23 0.3468]\n\nEpoch 1/1 : |████████████████████████----------------| 61.96% [57/92 00:37&lt;00:22 0.3376]\n\nEpoch 1/1 : |█████████████████████████---------------| 63.04% [58/92 00:37&lt;00:22 0.3308]\n\nEpoch 1/1 : |█████████████████████████---------------| 64.13% [59/92 00:38&lt;00:21 0.3219]\n\nEpoch 1/1 : |██████████████████████████--------------| 65.22% [60/92 00:38&lt;00:20 0.3154]\n\nEpoch 1/1 : |██████████████████████████--------------| 66.30% [61/92 00:39&lt;00:20 0.3070]\n\nEpoch 1/1 : |██████████████████████████--------------| 67.39% [62/92 00:39&lt;00:19 0.2997]\n\nEpoch 1/1 : |███████████████████████████-------------| 68.48% [63/92 00:40&lt;00:18 0.2921]\n\nEpoch 1/1 : |███████████████████████████-------------| 69.57% [64/92 00:41&lt;00:17 0.2911]\n\nEpoch 1/1 : |████████████████████████████------------| 70.65% [65/92 00:41&lt;00:17 0.2831]\n\nEpoch 1/1 : |████████████████████████████------------| 71.74% [66/92 00:42&lt;00:16 0.2813]\n\nEpoch 1/1 : |█████████████████████████████-----------| 72.83% [67/92 00:42&lt;00:15 0.2766]\n\nEpoch 1/1 : |█████████████████████████████-----------| 73.91% [68/92 00:43&lt;00:15 0.2695]\n\nEpoch 1/1 : |██████████████████████████████----------| 75.00% [69/92 00:43&lt;00:14 0.2640]\n\nEpoch 1/1 : |██████████████████████████████----------| 76.09% [70/92 00:44&lt;00:13 0.2586]\n\nEpoch 1/1 : |██████████████████████████████----------| 77.17% [71/92 00:44&lt;00:13 0.2522]\n\nEpoch 1/1 : |███████████████████████████████---------| 78.26% [72/92 00:45&lt;00:12 0.2456]\n\nEpoch 1/1 : |███████████████████████████████---------| 79.35% [73/92 00:46&lt;00:12 0.2398]\n\nEpoch 1/1 : |████████████████████████████████--------| 80.43% [74/92 00:46&lt;00:11 0.2364]\n\nEpoch 1/1 : |████████████████████████████████--------| 81.52% [75/92 00:47&lt;00:10 0.2304]\n\nEpoch 1/1 : |█████████████████████████████████-------| 82.61% [76/92 00:47&lt;00:10 0.2249]\n\nEpoch 1/1 : |█████████████████████████████████-------| 83.70% [77/92 00:48&lt;00:09 0.2200]\n\nEpoch 1/1 : |█████████████████████████████████-------| 84.78% [78/92 00:48&lt;00:08 0.2168]\n\nEpoch 1/1 : |██████████████████████████████████------| 85.87% [79/92 00:49&lt;00:08 0.2143]\n\nEpoch 1/1 : |██████████████████████████████████------| 86.96% [80/92 00:50&lt;00:07 0.2101]\n\nEpoch 1/1 : |███████████████████████████████████-----| 88.04% [81/92 00:50&lt;00:06 0.2057]\n\nEpoch 1/1 : |███████████████████████████████████-----| 89.13% [82/92 00:51&lt;00:06 0.2008]\n\nEpoch 1/1 : |████████████████████████████████████----| 90.22% [83/92 00:51&lt;00:05 0.1978]\n\nEpoch 1/1 : |████████████████████████████████████----| 91.30% [84/92 00:52&lt;00:04 0.1937]\n\nEpoch 1/1 : |████████████████████████████████████----| 92.39% [85/92 00:52&lt;00:04 0.1925]\n\nEpoch 1/1 : |█████████████████████████████████████---| 93.48% [86/92 00:53&lt;00:03 0.1879]\n\nEpoch 1/1 : |█████████████████████████████████████---| 94.57% [87/92 00:54&lt;00:03 0.1834]\n\nEpoch 1/1 : |██████████████████████████████████████--| 95.65% [88/92 00:54&lt;00:02 0.1834]\n\nEpoch 1/1 : |██████████████████████████████████████--| 96.74% [89/92 00:55&lt;00:01 0.1855]\n\nEpoch 1/1 : |███████████████████████████████████████-| 97.83% [90/92 00:55&lt;00:01 0.1848]\n\nEpoch 1/1 : |███████████████████████████████████████-| 98.91% [91/92 00:56&lt;00:00 0.1809]\n\nEpoch 1/1 : |████████████████████████████████████████| 100.00% [92/92 00:56&lt;00:00 0.1839]\n\nEpoch 1/1 :                                                                              \n\nEpoch 1/1 :                                                                              \n█\n\nEpoch 1/1 : |----------------------------------------| 0.00% [0/24 00:00&lt;?]\n\nEpoch 1/1 : |█---------------------------------------| 4.17% [1/24 00:00&lt;00:10]\n\nEpoch 1/1 : |███-------------------------------------| 8.33% [2/24 00:00&lt;00:08 0.1796]\n\nEpoch 1/1 : |█████-----------------------------------| 12.50% [3/24 00:01&lt;00:08 0.1796]\n\nEpoch 1/1 : |██████----------------------------------| 16.67% [4/24 00:01&lt;00:07 0.1796]\n\nEpoch 1/1 : |████████--------------------------------| 20.83% [5/24 00:01&lt;00:07 0.1796]\n\nEpoch 1/1 : |██████████------------------------------| 25.00% [6/24 00:02&lt;00:06 0.1796]\n\nEpoch 1/1 : |███████████-----------------------------| 29.17% [7/24 00:02&lt;00:06 0.1796]\n\nEpoch 1/1 : |█████████████---------------------------| 33.33% [8/24 00:02&lt;00:05 0.1796]\n\nEpoch 1/1 : |███████████████-------------------------| 37.50% [9/24 00:03&lt;00:05 0.1796]\n\nEpoch 1/1 : |████████████████------------------------| 41.67% [10/24 00:03&lt;00:05 0.1796]\n\nEpoch 1/1 : |██████████████████----------------------| 45.83% [11/24 00:03&lt;00:04 0.1796]\n\nEpoch 1/1 : |████████████████████--------------------| 50.00% [12/24 00:04&lt;00:04 0.1796]\n\nEpoch 1/1 : |█████████████████████-------------------| 54.17% [13/24 00:04&lt;00:03 0.1796]\n\nEpoch 1/1 : |███████████████████████-----------------| 58.33% [14/24 00:05&lt;00:03 0.1796]\n\nEpoch 1/1 : |█████████████████████████---------------| 62.50% [15/24 00:05&lt;00:03 0.1796]\n\nEpoch 1/1 : |██████████████████████████--------------| 66.67% [16/24 00:05&lt;00:02 0.1796]\n\nEpoch 1/1 : |████████████████████████████------------| 70.83% [17/24 00:06&lt;00:02 0.1796]\n\nEpoch 1/1 : |██████████████████████████████----------| 75.00% [18/24 00:06&lt;00:02 0.1796]\n\nEpoch 1/1 : |███████████████████████████████---------| 79.17% [19/24 00:06&lt;00:01 0.1796]\n\nEpoch 1/1 : |█████████████████████████████████-------| 83.33% [20/24 00:07&lt;00:01 0.1796]\n\nEpoch 1/1 : |███████████████████████████████████-----| 87.50% [21/24 00:07&lt;00:01 0.1796]\n\nEpoch 1/1 : |████████████████████████████████████----| 91.67% [22/24 00:08&lt;00:00 0.1796]\n\nEpoch 1/1 : |██████████████████████████████████████--| 95.83% [23/24 00:08&lt;00:00 0.1796]\n\nEpoch 1/1 : |████████████████████████████████████████| 100.00% [24/24 00:08&lt;00:00 0.1796]\n\nEpoch 1/1 :                                                                              \n\nEpoch 1/1 :                                                                              \n0         0.179568    0.032552    0.012855    01:05     \n█\nepoch     train_loss  valid_loss  error_rate  time    \n█\n\nEpoch 1/1 : |----------------------------------------| 0.00% [0/92 00:00&lt;?]\n\nEpoch 1/1 : |----------------------------------------| 1.09% [1/92 00:01&lt;02:29]\n\nEpoch 1/1 : |----------------------------------------| 2.17% [2/92 00:02&lt;01:51 0.1797]\n\nEpoch 1/1 : |█---------------------------------------| 3.26% [3/92 00:03&lt;01:38 0.1064]\n\nEpoch 1/1 : |█---------------------------------------| 4.35% [4/92 00:04&lt;01:30 0.0731]\n\nEpoch 1/1 : |██--------------------------------------| 5.43% [5/92 00:04&lt;01:26 0.0828]\n\nEpoch 1/1 : |██--------------------------------------| 6.52% [6/92 00:05&lt;01:22 0.0750]\n\nEpoch 1/1 : |███-------------------------------------| 7.61% [7/92 00:06&lt;01:20 0.0639]\n\nEpoch 1/1 : |███-------------------------------------| 8.70% [8/92 00:07&lt;01:17 0.0550]\n\nEpoch 1/1 : |███-------------------------------------| 9.78% [9/92 00:08&lt;01:16 0.0483]\n\nEpoch 1/1 : |████------------------------------------| 10.87% [10/92 00:09&lt;01:14 0.0460]\n\nEpoch 1/1 : |████------------------------------------| 11.96% [11/92 00:09&lt;01:13 0.0580]\n\nEpoch 1/1 : |█████-----------------------------------| 13.04% [12/92 00:10&lt;01:11 0.0529]\n\nEpoch 1/1 : |█████-----------------------------------| 14.13% [13/92 00:11&lt;01:10 0.0525]\n\nEpoch 1/1 : |██████----------------------------------| 15.22% [14/92 00:12&lt;01:09 0.0680]\n\nEpoch 1/1 : |██████----------------------------------| 16.30% [15/92 00:13&lt;01:07 0.0632]\n\nEpoch 1/1 : |██████----------------------------------| 17.39% [16/92 00:14&lt;01:06 0.0612]\n\nEpoch 1/1 : |███████---------------------------------| 18.48% [17/92 00:14&lt;01:05 0.0966]\n\nEpoch 1/1 : |███████---------------------------------| 19.57% [18/92 00:15&lt;01:04 0.0946]\n\nEpoch 1/1 : |████████--------------------------------| 20.65% [19/92 00:16&lt;01:03 0.0967]\n\nEpoch 1/1 : |████████--------------------------------| 21.74% [20/92 00:17&lt;01:02 0.1005]\n\nEpoch 1/1 : |█████████-------------------------------| 22.83% [21/92 00:18&lt;01:01 0.0976]\n\nEpoch 1/1 : |█████████-------------------------------| 23.91% [22/92 00:19&lt;01:00 0.0920]\n\nEpoch 1/1 : |██████████------------------------------| 25.00% [23/92 00:19&lt;00:59 0.0877]\n\nEpoch 1/1 : |██████████------------------------------| 26.09% [24/92 00:20&lt;00:58 0.0830]\n\nEpoch 1/1 : |██████████------------------------------| 27.17% [25/92 00:21&lt;00:57 0.0832]\n\nEpoch 1/1 : |███████████-----------------------------| 28.26% [26/92 00:22&lt;00:56 0.0834]\n\nEpoch 1/1 : |███████████-----------------------------| 29.35% [27/92 00:23&lt;00:56 0.0868]\n\nEpoch 1/1 : |████████████----------------------------| 30.43% [28/92 00:24&lt;00:55 0.0931]\n\nEpoch 1/1 : |████████████----------------------------| 31.52% [29/92 00:24&lt;00:54 0.0893]\n\nEpoch 1/1 : |█████████████---------------------------| 32.61% [30/92 00:25&lt;00:53 0.0865]\n\nEpoch 1/1 : |█████████████---------------------------| 33.70% [31/92 00:26&lt;00:52 0.0861]\n\nEpoch 1/1 : |█████████████---------------------------| 34.78% [32/92 00:27&lt;00:51 0.0888]\n\nEpoch 1/1 : |██████████████--------------------------| 35.87% [33/92 00:28&lt;00:50 0.0852]\n\nEpoch 1/1 : |██████████████--------------------------| 36.96% [34/92 00:29&lt;00:49 0.0883]\n\nEpoch 1/1 : |███████████████-------------------------| 38.04% [35/92 00:29&lt;00:48 0.0849]\n\nEpoch 1/1 : |███████████████-------------------------| 39.13% [36/92 00:30&lt;00:47 0.0865]\n\nEpoch 1/1 : |████████████████------------------------| 40.22% [37/92 00:31&lt;00:47 0.0887]\n\nEpoch 1/1 : |████████████████------------------------| 41.30% [38/92 00:32&lt;00:46 0.0949]\n\nEpoch 1/1 : |████████████████------------------------| 42.39% [39/92 00:33&lt;00:45 0.0992]\n\nEpoch 1/1 : |█████████████████-----------------------| 43.48% [40/92 00:34&lt;00:44 0.0956]\n\nEpoch 1/1 : |█████████████████-----------------------| 44.57% [41/92 00:34&lt;00:43 0.0950]\n\nEpoch 1/1 : |██████████████████----------------------| 45.65% [42/92 00:35&lt;00:42 0.0925]\n\nEpoch 1/1 : |██████████████████----------------------| 46.74% [43/92 00:36&lt;00:41 0.0915]\n\nEpoch 1/1 : |███████████████████---------------------| 47.83% [44/92 00:37&lt;00:40 0.0920]\n\nEpoch 1/1 : |███████████████████---------------------| 48.91% [45/92 00:38&lt;00:39 0.0902]\n\nEpoch 1/1 : |████████████████████--------------------| 50.00% [46/92 00:39&lt;00:39 0.0919]\n\nEpoch 1/1 : |████████████████████--------------------| 51.09% [47/92 00:39&lt;00:38 0.1026]\n\nEpoch 1/1 : |████████████████████--------------------| 52.17% [48/92 00:40&lt;00:37 0.1011]\n\nEpoch 1/1 : |█████████████████████-------------------| 53.26% [49/92 00:41&lt;00:36 0.0981]\n\nEpoch 1/1 : |█████████████████████-------------------| 54.35% [50/92 00:42&lt;00:35 0.1058]\n\nEpoch 1/1 : |██████████████████████------------------| 55.43% [51/92 00:43&lt;00:34 0.1025]\n\nEpoch 1/1 : |██████████████████████------------------| 56.52% [52/92 00:44&lt;00:33 0.1030]\n\nEpoch 1/1 : |███████████████████████-----------------| 57.61% [53/92 00:44&lt;00:33 0.1020]\n\nEpoch 1/1 : |███████████████████████-----------------| 58.70% [54/92 00:45&lt;00:32 0.1046]\n\nEpoch 1/1 : |███████████████████████-----------------| 59.78% [55/92 00:46&lt;00:31 0.1033]\n\nEpoch 1/1 : |████████████████████████----------------| 60.87% [56/92 00:47&lt;00:30 0.1014]\n\nEpoch 1/1 : |████████████████████████----------------| 61.96% [57/92 00:48&lt;00:29 0.0984]\n\nEpoch 1/1 : |█████████████████████████---------------| 63.04% [58/92 00:49&lt;00:28 0.0960]\n\nEpoch 1/1 : |█████████████████████████---------------| 64.13% [59/92 00:49&lt;00:27 0.0944]\n\nEpoch 1/1 : |██████████████████████████--------------| 65.22% [60/92 00:50&lt;00:27 0.0969]\n\nEpoch 1/1 : |██████████████████████████--------------| 66.30% [61/92 00:51&lt;00:26 0.0946]\n\nEpoch 1/1 : |██████████████████████████--------------| 67.39% [62/92 00:52&lt;00:25 0.0945]\n\nEpoch 1/1 : |███████████████████████████-------------| 68.48% [63/92 00:53&lt;00:24 0.0930]\n\nEpoch 1/1 : |███████████████████████████-------------| 69.57% [64/92 00:54&lt;00:23 0.0931]\n\nEpoch 1/1 : |████████████████████████████------------| 70.65% [65/92 00:54&lt;00:22 0.0908]\n\nEpoch 1/1 : |████████████████████████████------------| 71.74% [66/92 00:55&lt;00:21 0.0884]\n\nEpoch 1/1 : |█████████████████████████████-----------| 72.83% [67/92 00:56&lt;00:21 0.0871]\n\nEpoch 1/1 : |█████████████████████████████-----------| 73.91% [68/92 00:57&lt;00:20 0.0861]\n\nEpoch 1/1 : |██████████████████████████████----------| 75.00% [69/92 00:58&lt;00:19 0.0839]\n\nEpoch 1/1 : |██████████████████████████████----------| 76.09% [70/92 00:59&lt;00:18 0.0831]\n\nEpoch 1/1 : |██████████████████████████████----------| 77.17% [71/92 01:00&lt;00:17 0.0838]\n\nEpoch 1/1 : |███████████████████████████████---------| 78.26% [72/92 01:00&lt;00:16 0.0848]\n\nEpoch 1/1 : |███████████████████████████████---------| 79.35% [73/92 01:01&lt;00:16 0.0865]\n\nEpoch 1/1 : |████████████████████████████████--------| 80.43% [74/92 01:02&lt;00:15 0.0885]\n\nEpoch 1/1 : |████████████████████████████████--------| 81.52% [75/92 01:03&lt;00:14 0.0864]\n\nEpoch 1/1 : |█████████████████████████████████-------| 82.61% [76/92 01:04&lt;00:13 0.0872]\n\nEpoch 1/1 : |█████████████████████████████████-------| 83.70% [77/92 01:05&lt;00:12 0.0859]\n\nEpoch 1/1 : |█████████████████████████████████-------| 84.78% [78/92 01:06&lt;00:11 0.0837]\n\nEpoch 1/1 : |██████████████████████████████████------| 85.87% [79/92 01:07&lt;00:11 0.0838]\n\nEpoch 1/1 : |██████████████████████████████████------| 86.96% [80/92 01:07&lt;00:10 0.0828]\n\nEpoch 1/1 : |███████████████████████████████████-----| 88.04% [81/92 01:08&lt;00:09 0.0814]\n\nEpoch 1/1 : |███████████████████████████████████-----| 89.13% [82/92 01:09&lt;00:08 0.0795]\n\nEpoch 1/1 : |████████████████████████████████████----| 90.22% [83/92 01:10&lt;00:07 0.0776]\n\nEpoch 1/1 : |████████████████████████████████████----| 91.30% [84/92 01:11&lt;00:06 0.0795]\n\nEpoch 1/1 : |████████████████████████████████████----| 92.39% [85/92 01:12&lt;00:05 0.0781]\n\nEpoch 1/1 : |█████████████████████████████████████---| 93.48% [86/92 01:13&lt;00:05 0.0765]\n\nEpoch 1/1 : |█████████████████████████████████████---| 94.57% [87/92 01:13&lt;00:04 0.0749]\n\nEpoch 1/1 : |██████████████████████████████████████--| 95.65% [88/92 01:14&lt;00:03 0.0731]\n\nEpoch 1/1 : |██████████████████████████████████████--| 96.74% [89/92 01:15&lt;00:02 0.0732]\n\nEpoch 1/1 : |███████████████████████████████████████-| 97.83% [90/92 01:16&lt;00:01 0.0729]\n\nEpoch 1/1 : |███████████████████████████████████████-| 98.91% [91/92 01:17&lt;00:00 0.0712]\n\nEpoch 1/1 : |████████████████████████████████████████| 100.00% [92/92 01:18&lt;00:00 0.0702]\n\nEpoch 1/1 :                                                                              \n\nEpoch 1/1 :                                                                              \n█\n\nEpoch 1/1 : |----------------------------------------| 0.00% [0/24 00:00&lt;?]\n\nEpoch 1/1 : |█---------------------------------------| 4.17% [1/24 00:00&lt;00:08]\n\nEpoch 1/1 : |███-------------------------------------| 8.33% [2/24 00:00&lt;00:08 0.0691]\n\nEpoch 1/1 : |█████-----------------------------------| 12.50% [3/24 00:01&lt;00:07 0.0691]\n\nEpoch 1/1 : |██████----------------------------------| 16.67% [4/24 00:01&lt;00:07 0.0691]\n\nEpoch 1/1 : |████████--------------------------------| 20.83% [5/24 00:01&lt;00:07 0.0691]\n\nEpoch 1/1 : |██████████------------------------------| 25.00% [6/24 00:02&lt;00:06 0.0691]\n\nEpoch 1/1 : |███████████-----------------------------| 29.17% [7/24 00:02&lt;00:06 0.0691]\n\nEpoch 1/1 : |█████████████---------------------------| 33.33% [8/24 00:02&lt;00:05 0.0691]\n\nEpoch 1/1 : |███████████████-------------------------| 37.50% [9/24 00:03&lt;00:05 0.0691]\n\nEpoch 1/1 : |████████████████------------------------| 41.67% [10/24 00:03&lt;00:05 0.0691]\n\nEpoch 1/1 : |██████████████████----------------------| 45.83% [11/24 00:03&lt;00:04 0.0691]\n\nEpoch 1/1 : |████████████████████--------------------| 50.00% [12/24 00:04&lt;00:04 0.0691]\n\nEpoch 1/1 : |█████████████████████-------------------| 54.17% [13/24 00:04&lt;00:04 0.0691]\n\nEpoch 1/1 : |███████████████████████-----------------| 58.33% [14/24 00:05&lt;00:03 0.0691]\n\nEpoch 1/1 : |█████████████████████████---------------| 62.50% [15/24 00:05&lt;00:03 0.0691]\n\nEpoch 1/1 : |██████████████████████████--------------| 66.67% [16/24 00:05&lt;00:02 0.0691]\n\nEpoch 1/1 : |████████████████████████████------------| 70.83% [17/24 00:06&lt;00:02 0.0691]\n\nEpoch 1/1 : |██████████████████████████████----------| 75.00% [18/24 00:06&lt;00:02 0.0691]\n\nEpoch 1/1 : |███████████████████████████████---------| 79.17% [19/24 00:07&lt;00:01 0.0691]\n\nEpoch 1/1 : |█████████████████████████████████-------| 83.33% [20/24 00:07&lt;00:01 0.0691]\n\nEpoch 1/1 : |███████████████████████████████████-----| 87.50% [21/24 00:07&lt;00:01 0.0691]\n\nEpoch 1/1 : |████████████████████████████████████----| 91.67% [22/24 00:08&lt;00:00 0.0691]\n\nEpoch 1/1 : |██████████████████████████████████████--| 95.83% [23/24 00:08&lt;00:00 0.0691]\n\nEpoch 1/1 : |████████████████████████████████████████| 100.00% [24/24 00:08&lt;00:00 0.0691]\n\nEpoch 1/1 :                                                                              \n\nEpoch 1/1 :                                                                              \n0         0.069070    0.013186    0.004736    01:26"
  },
  {
    "objectID": "posts/2023-07-05-python-graphics/2023-07-05-python-graphics.html",
    "href": "posts/2023-07-05-python-graphics/2023-07-05-python-graphics.html",
    "title": "Python graphics",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.get_dataset_names()\n\n['anagrams', 'anscombe', 'attention', 'brain_networks', 'car_crashes', 'diamonds', 'dots', 'dowjones', 'exercise', 'flights', 'fmri', 'geyser', 'glue', 'healthexp', 'iris', 'mpg', 'penguins', 'planets', 'seaice', 'taxis', 'tips', 'titanic']\n\niris = sns.load_dataset('iris')"
  },
  {
    "objectID": "posts/2023-07-05-python-graphics/2023-07-05-python-graphics.html#matplotlib",
    "href": "posts/2023-07-05-python-graphics/2023-07-05-python-graphics.html#matplotlib",
    "title": "Python graphics",
    "section": "Matplotlib",
    "text": "Matplotlib\n\nplt.rcParams['figure.figsize'] = [12, 5]\nfig, ax = plt.subplots()\nax.set_xlabel('Sepal length')\nax.set_ylabel('Sepal width')\nax.set_title('Iris dataset')\nax.scatter(iris['sepal_length'], iris['sepal_width'])\nplt.show()"
  },
  {
    "objectID": "posts/2023-07-05-python-graphics/2023-07-05-python-graphics.html#seaborn",
    "href": "posts/2023-07-05-python-graphics/2023-07-05-python-graphics.html#seaborn",
    "title": "Python graphics",
    "section": "Seaborn",
    "text": "Seaborn\n\ng = sns.FacetGrid(iris, col = 'species', col_wrap = 1, height = 2, aspect = 2)\ng = g.map(plt.scatter, 'sepal_length', 'sepal_width')\nplt.show()\n\n\n\n\n\n\n\nplt.close()"
  },
  {
    "objectID": "posts/2023-01-20-sirs-model-dynamics/2023-01-20-sirs-model-dynamics.html",
    "href": "posts/2023-01-20-sirs-model-dynamics/2023-01-20-sirs-model-dynamics.html",
    "title": "SIRS model dynamics",
    "section": "",
    "text": "library('ggplot2')\nlibrary('tidyverse')\nlibrary('deSolve')\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/2023-01-20-sirs-model-dynamics/2023-01-20-sirs-model-dynamics.html#loading-the-required-libraries",
    "href": "posts/2023-01-20-sirs-model-dynamics/2023-01-20-sirs-model-dynamics.html#loading-the-required-libraries",
    "title": "SIRS model dynamics",
    "section": "",
    "text": "library('ggplot2')\nlibrary('tidyverse')\nlibrary('deSolve')\ntheme_set(theme_bw())"
  },
  {
    "objectID": "posts/2023-01-20-sirs-model-dynamics/2023-01-20-sirs-model-dynamics.html#simulate-odes",
    "href": "posts/2023-01-20-sirs-model-dynamics/2023-01-20-sirs-model-dynamics.html#simulate-odes",
    "title": "SIRS model dynamics",
    "section": "Simulate ODEs",
    "text": "Simulate ODEs\n\nsir_ode_deterministic &lt;- function(t, state, pars) {\n  with(as.list(c(state, pars)), {\n    dS &lt;- - beta * I * S + lambda * R\n    dI &lt;- beta * I * S - gamma * I\n    dR &lt;- gamma * I - lambda * R\n    return(list(c(dS = dS, dI = dI, dR = dR)))\n  }) \n}\n\n\ninfected_initial &lt;- 0.5\ninitial_condition &lt;- \n  c(S = 1 - infected_initial,\n    I = infected_initial,\n    R = 0)\n\nstep_length &lt;- 1\nmax_time &lt;- 100\ntimepoints &lt;- seq(0, max_time, by = step_length)\ngrid_values &lt;- 2^(-1:1)\ngrid_values &lt;- 2^(-2:2)\ngrid_values &lt;- 2^(-3:3)\n\nresult &lt;- tibble()\nfor(beta in grid_values) {\n  for(gamma in grid_values) {\n    for(lambda in grid_values) {\n      parameters &lt;- list(beta = beta, \n                         gamma = gamma,\n                         lambda = lambda)\n      solution &lt;- ode(y = initial_condition, \n                      times = timepoints,\n                      func = sir_ode_deterministic,\n                      parms = parameters) %&gt;% \n        unclass %&gt;% as_tibble\n      solution$beta &lt;- beta\n      solution$gamma &lt;- gamma\n      solution$lambda &lt;- lambda\n      result &lt;- rbind(result, solution)\n    }\n  }\n}\n\nresult &lt;- pivot_longer(result, cols = c('S', 'I', 'R'))\n\nlast &lt;- result %&gt;% \n  filter(time == max_time) %&gt;% \n  mutate(name = factor(name, levels = c('S', 'I', 'R')))\n\nlast %&gt;%\n  ggplot(aes(beta, value, colour = name)) + \n  geom_point() + geom_line() +\n  facet_grid(rows = vars(gamma), \n             cols = vars(lambda), \n             labeller = label_both) +\n  labs(xlab = 'beta', ylab = 'Fraction', colour = 'Compartment') +\n  scale_x_continuous(trans='log10')\n\n\n\nlast %&gt;%\n  ggplot(aes(x = beta, y = value, fill = name)) +\n  geom_area() +\n  facet_grid(rows = vars(gamma), \n             cols = vars(lambda), \n             labeller = label_both) +\n  labs(xlab = 'beta', ylab = 'Fraction', fill = 'Compartment') +\n  scale_x_continuous(trans='log10')\n\n\n\nss &lt;- last %&gt;% \n  group_by(beta, gamma, lambda) %&gt;% \n  summarise(ss = sum(value))\n\n`summarise()` has grouped output by 'beta', 'gamma'. You can override using the\n`.groups` argument."
  },
  {
    "objectID": "posts/2023-01-20-sirs-model-dynamics/2023-01-20-sirs-model-dynamics.html#analytical-approach",
    "href": "posts/2023-01-20-sirs-model-dynamics/2023-01-20-sirs-model-dynamics.html#analytical-approach",
    "title": "SIRS model dynamics",
    "section": "Analytical approach",
    "text": "Analytical approach\nSolution in terms of parameters in red.\n\nEquilibrium conditions\n\\[\n\\begin{align}\n1 &= S + I + R  \\qquad &(1) \\\\\n0 &= -\\beta I S + \\lambda R \\qquad &(2) \\\\\n0 &= \\beta I S - \\gamma I \\qquad &(3) \\\\\n0 &= \\gamma I - \\lambda R \\qquad &(4)\n\\end{align}\n\\]\n\n\nAlgebraic transformation\n\\[\n\\begin{align}\nR &= 1 - S - I \\qquad &(1b) \\\\\nS &= \\gamma / \\beta \\qquad &(3b)\n\\end{align}\n\\]\n\n\nPlugging in\n\\[\n\\begin{align}\n(3b) \\rightarrow (2): \\quad I &= \\lambda / \\gamma R  \\qquad &(5) \\\\\n(1b) \\rightarrow (5): \\quad I &= \\frac{\\lambda(\\beta - \\gamma)}{\\beta(\\gamma + \\lambda)} \\qquad &(6) \\\\\n(3b) + (6) \\rightarrow (1b): R &= \\frac{\\gamma(\\beta - \\gamma)}{\\beta(\\gamma + \\lambda)} \\qquad &(7)\n\\end{align}\n\\]\n\n\nPlots\n\nlast_w &lt;- pivot_wider(last, names_from = name, values_from = value)\nlast_w$S_calc &lt;- pmin(1, last_w$gamma / last_w$beta)\nlast_w$I_calc &lt;- \n  pmax(0, pmin(1, with(last_w, (lambda * (beta - gamma))) / \n                 (beta * (gamma + lambda))))\nlast_w$R_calc &lt;- \n  pmax(0, pmin(1, with(last_w, (gamma * (beta - gamma)) / \n                         (beta * (gamma + lambda)))))\n\nlast_w$I_calc &lt;- 1 - last_w$S_calc - last_w$R_calc\n\nlast &lt;- last_w %&gt;% \n  pivot_longer(cols = c('S_calc', 'I_calc', 'R_calc'))\n\n\nlast %&gt;%\n  ggplot(aes(x = beta, y = value, fill = name)) + \n  geom_area() +\n  facet_grid(rows = vars(gamma), \n             cols = vars(lambda), \n             labeller = label_both) +\n  labs(ylab = 'Fraction',  xlab = 'beta', colour = 'Compartment') +\n  scale_x_continuous(trans='log10')"
  },
  {
    "objectID": "posts/2023-08-11-leave-one-out-cross-validation-in-python/2023-08-08-leave-one-out-cross-validation-in-python.html",
    "href": "posts/2023-08-11-leave-one-out-cross-validation-in-python/2023-08-08-leave-one-out-cross-validation-in-python.html",
    "title": "Leave-one-out cross-validation in Python",
    "section": "",
    "text": "Look at the cars dataset that is shipped in base R\nFit models\n\nfor each model, leave out one data point\npredict the response value for that data point based on the model that is not based on that data point\ncalculate mean absolute error and mean squared error"
  },
  {
    "objectID": "posts/2023-08-11-leave-one-out-cross-validation-in-python/2023-08-08-leave-one-out-cross-validation-in-python.html#strategy",
    "href": "posts/2023-08-11-leave-one-out-cross-validation-in-python/2023-08-08-leave-one-out-cross-validation-in-python.html#strategy",
    "title": "Leave-one-out cross-validation in Python",
    "section": "",
    "text": "Look at the cars dataset that is shipped in base R\nFit models\n\nfor each model, leave out one data point\npredict the response value for that data point based on the model that is not based on that data point\ncalculate mean absolute error and mean squared error"
  },
  {
    "objectID": "posts/2023-08-11-leave-one-out-cross-validation-in-python/2023-08-08-leave-one-out-cross-validation-in-python.html#implementation",
    "href": "posts/2023-08-11-leave-one-out-cross-validation-in-python/2023-08-08-leave-one-out-cross-validation-in-python.html#implementation",
    "title": "Leave-one-out cross-validation in Python",
    "section": "Implementation",
    "text": "Implementation\n\nInit\n\nChunk in R to setup my Python environment (can be omitted for users with Python ready to use)\n\nlibrary('reticulate')\nuse_condaenv('sbloggel')\n\n\n\nChunk to load Python libraries and set up the data\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport numpy as np\ncars = pd.DataFrame(r['cars'])\n\n\n\n\nShow that fitting in R and Python yields identical results\n\nfm_fit = smf.ols(formula = 'speed ~ dist', data = cars).fit()\nfm_fit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nspeed\nR-squared:\n0.651\n\n\nModel:\nOLS\nAdj. R-squared:\n0.644\n\n\nMethod:\nLeast Squares\nF-statistic:\n89.57\n\n\nDate:\nFri, 11 Aug 2023\nProb (F-statistic):\n1.49e-12\n\n\nTime:\n10:32:05\nLog-Likelihood:\n-127.39\n\n\nNo. Observations:\n50\nAIC:\n258.8\n\n\nDf Residuals:\n48\nBIC:\n262.6\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n8.2839\n0.874\n9.474\n0.000\n6.526\n10.042\n\n\ndist\n0.1656\n0.017\n9.464\n0.000\n0.130\n0.201\n\n\n\n\n\n\nOmnibus:\n0.720\nDurbin-Watson:\n1.195\n\n\nProb(Omnibus):\n0.698\nJarque-Bera (JB):\n0.827\n\n\nSkew:\n-0.207\nProb(JB):\n0.661\n\n\nKurtosis:\n2.526\nCond. No.\n98.0\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nfm &lt;- lm(speed ~ dist, data = cars)\nsummary(fm)\n\n\nCall:\nlm(formula = speed ~ dist, data = cars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5293 -2.1550  0.3615  2.4377  6.4179 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.28391    0.87438   9.474 1.44e-12 ***\ndist         0.16557    0.01749   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.156 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\n\nShow that prediction in R and Python yields identical results\n\nfm_fit.predict(cars.iloc[0])\n\n0    8.615041\ndtype: float64\n\n\n\npredict(fm, cars[1, ])\n\n       1 \n8.615041 \n\n\n\n\nPerform leave-one-out-cross-validation in Python\n\n# prepare column for predictions:\ncars['prediction'] = np.nan\n\n# fit models:\nfor i in range(50):\n  train = cars.drop(index = i)\n  test = cars.iloc[i]\n  fm_fit = smf.ols(formula='speed ~ dist', data=train).fit()\n  cars['prediction'].iloc[i] = fm_fit.predict(test)\n\n# calculate statistics:\ncars['error'] = cars['speed'] - cars['prediction']\ncars['abs_error'] = np.abs(cars['error'])\ncars['squared_error'] = np.square(cars['error'])\n\n# mean absolute error:\nnp.mean(cars['abs_error'])\n\n2.6335683917489323\n\n# root mean squared error:\nnp.sqrt(np.mean(cars['squared_error']))\n\n3.244276599631193"
  },
  {
    "objectID": "posts/2023-08-11-leave-one-out-cross-validation-in-python/2023-08-08-leave-one-out-cross-validation-in-python.html#conclusion",
    "href": "posts/2023-08-11-leave-one-out-cross-validation-in-python/2023-08-08-leave-one-out-cross-validation-in-python.html#conclusion",
    "title": "Leave-one-out cross-validation in Python",
    "section": "Conclusion",
    "text": "Conclusion\n\nResults are identical to those produced in R: Link\nHowever, they are not identical to Friedemann’s results - we still have to clarify why that is!"
  },
  {
    "objectID": "posts/2022-07-19-sir-model/2022-07-19-sir-model.html",
    "href": "posts/2022-07-19-sir-model/2022-07-19-sir-model.html",
    "title": "SIR model",
    "section": "",
    "text": "Here I want to show some sample calculations using the SIR model (https://simple.wikipedia.org/wiki/SIR_model, https://en.wikipedia.org/wiki/Compartmental_models_in_epidemiology).\nThe SIR model is a simple ordinary differential equation model that describes the dynamics of an epidemic within a population. The model has three variables:\n\n\\(S\\) (= susceptible) quantifies the susceptible individuals\n\\(I\\) (= infectious) quantifies the individuals who are infected with the disease and can transmit the disease to susceptible individuals\n\\(R\\) (= recovered) quantifies the individuals who have recovered from the disease and can no longer be infected with the disease\n\nThe following equations describe the dynamics of the model: \\[ \\frac{dS}{dt} = -\\frac{\\beta I S}{N} \\]\n\\[ \\frac{dI}{dt} = \\frac{\\beta I S}{N} - \\lambda I \\]\n\\[ \\frac{dR}{dt} = \\lambda I \\]\nIn the following, we solve the model equations for a random parameter set.\nFurthermore, we fix \\(N\\) to \\(1\\) and set the initial conditions in a way that \\(S + I + R = 1\\) so that \\(S\\), \\(R\\) and \\(I\\) describe fractions of the population.\n\nparameters &lt;- c(beta = 2, \n                gamma = 1)\n\ninfected_initial &lt;- 0.01\ninitial_condition &lt;- \n  c(S = 1 - infected_initial,\n    I = infected_initial,\n    R = 0)\n\ntimepoints &lt;- seq(0, 15, by = .1)\n\nThen we define the model equations in R:\n\nSIR_ODE = function(time, state, parameters){\n  with(as.list(c(state, parameters)), {\n    dS &lt;- -beta * I * S\n    dI &lt;- beta * I * S - gamma * I\n    dR &lt;- gamma * I\n    list(c(dS, dI, dR))\n    })\n  }\n\nFinally, we use the function ode from the deSolve package to solve the ordinary differential equations:\n\nlibrary(deSolve)\nresult &lt;- ode(y = initial_condition, \n             times = timepoints, \n             func = SIR_ODE, \n             parms = parameters)\n\nNow we use the builtin plot function from the deSolve-package to visualize the solution:\n\nplot(result)\n\n\n\n\nWe can make some plots that are nicer looking in my taste with the lattice package. To rearrange the data from ‘wide’ to ‘long’ format (as needed for the lattice package), we use the tidyverse package.\n\nlibrary(lattice)\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n# convert data from wide to long format:\nlong &lt;- as.data.frame(result) %&gt;% \n  pivot_longer(cols = c('S', 'I', 'R'))\n\n# order factor levels so they appear in the plots in the desired order:\nlong$name = factor(long$name, levels = c('S', 'I', 'R'))\n\n# plot all three variables in the same panel:\nxyplot(value ~ time, \n       data = long, \n       group = name, \n       auto.key = list(space = 'right', \n                       lines = TRUE, \n                       points = FALSE), \n       type = 'l',\n       lwd = 3)\n\n\n\n# plot the variables in separate panels:\nxyplot(value ~ time | name, \n       data = long, \n       type = 'l', \n       lwd = 3)\n\n\n\n\nVery similar plots can be made with the ggplot2 library:\n\nlong %&gt;% ggplot(aes(time, value, colour = name)) + \n  geom_line(size = 2) + \n  labs(x = 'Time', y = 'Fraction', colour = 'Compartment')\n\n\n\nlong %&gt;% ggplot(aes(time, value)) + \n  geom_line(size = 2) +\n  facet_wrap(~ name) +\n  labs(x = 'Time', y = 'Fraction', colour = 'Compartment')"
  },
  {
    "objectID": "posts/2023-06-08-specifying-mixed-effects-models/2023-06-08-specifying-mixed-effects-models.html",
    "href": "posts/2023-06-08-specifying-mixed-effects-models/2023-06-08-specifying-mixed-effects-models.html",
    "title": "Specifying mixed effects in lme4",
    "section": "",
    "text": "\\(n\\) individuals (\\(i = 1, 2, .., n\\)) are measured \\(m\\) times (\\(j = 1, 2, ..., m\\)) pre-intervention (denoted \\(y_{ij}^{pre}\\)) and post-intervention (denoted \\(y_{ij}^{post}\\))"
  },
  {
    "objectID": "posts/2023-06-08-specifying-mixed-effects-models/2023-06-08-specifying-mixed-effects-models.html#description-of-trial",
    "href": "posts/2023-06-08-specifying-mixed-effects-models/2023-06-08-specifying-mixed-effects-models.html#description-of-trial",
    "title": "Specifying mixed effects in lme4",
    "section": "",
    "text": "\\(n\\) individuals (\\(i = 1, 2, .., n\\)) are measured \\(m\\) times (\\(j = 1, 2, ..., m\\)) pre-intervention (denoted \\(y_{ij}^{pre}\\)) and post-intervention (denoted \\(y_{ij}^{post}\\))"
  },
  {
    "objectID": "posts/2023-06-08-specifying-mixed-effects-models/2023-06-08-specifying-mixed-effects-models.html#underlying-probability-model",
    "href": "posts/2023-06-08-specifying-mixed-effects-models/2023-06-08-specifying-mixed-effects-models.html#underlying-probability-model",
    "title": "Specifying mixed effects in lme4",
    "section": "Underlying probability model",
    "text": "Underlying probability model\n\nNotation\n\n\\(\\beta_0\\) fixed effect intercept\n\\(\\beta_1\\) fixed effect of intervention\n\\(\\epsilon \\sim \\mathcal{N}(\\mu = 0, sd = \\sigma_0)\\), for reasons of better legibility sub- and superscript are omitted here\n\\(b_{1,i}\\) random effect baseline, \\(b_{1,i} \\sim \\mathcal{N}(\\mu = 0, sd = \\sigma_1)\\)\n\\(b_{2,i}\\) random effect of intervention, \\(b_{2,i} \\sim \\mathcal{N}(\\mu = 0, sd = \\sigma_2)\\)\n\n\n\nParameters of the model to be estimated\n\n\\(\\beta_0\\), \\(\\beta_1\\), \\(\\sigma_0\\), \\(\\sigma_1\\) and \\(\\sigma_2\\)\n\n\n\nModel equations\n\nMeasurement pre intervention: \\[\\begin{align}\ny_{ij}^{pre} = \\beta_0 + b_i + \\epsilon_{ij}^{pre}\n\\end{align}\\]\nMeasurement post-intervention: \\[\\begin{align}\ny_{ij}^{post} = \\beta_0 + \\beta_1 + b_{1,i} + b_{2,i} + \\epsilon_{ij}^{post}\n\\end{align}\\]\n\n\n\nSimulate data\n\n# size of sample:\nn &lt;- 20\nm &lt;- 5\n\n# model parameters:\nbeta_0 &lt;- 0\nbeta_1 &lt;- 1\nsigma_0 &lt;- 2\nsigma_1 &lt;- 1\nsigma_2 &lt;- 5\n\n# construct data:\npatients &lt;- tibble(patient_id = 1:n, \n                   patient_re = rnorm(n, 0, sigma_1),\n                   treatment_re = rnorm(n, 0, sigma_2))\n\ndat &lt;- expand_grid(patients,\n                   replicate = 1:m,\n                   timepoint = c('pre', 'post'))\n\ndat &lt;- dat |&gt; \n  mutate(y = beta_0 + \n           patient_re + \n           ifelse(timepoint == 'post', treatment_re + beta_1, 0) + \n           rnorm(n * m * 2, 0, sigma_0))\n\n# make timepoint a factor to get order in plots right:\ndat &lt;- dat |&gt; \n  mutate(timepoint = factor(timepoint, levels = c('pre', 'post')))\n\n\n\nTabular characteristics\n\ndat |&gt; \n  group_by(timepoint) |&gt; \n  summarize(mean = mean(y)) |&gt; \n  kable()\n\n\n\n\ntimepoint\nmean\n\n\n\n\npre\n0.1868025\n\n\npost\n1.1930561\n\n\n\n\n\n\n\nVisualize data\n\nif(n &lt; 1e2) {\n  dat |&gt;\n    ggplot(aes(timepoint, y, col = factor(patient_id))) +\n    geom_jitter(width = 0.1, height = 0) + labs(color = 'Patient ID')\n}\n\n\n\n\n\n\nFit model\n\nLinear model\n\n# linear model to get beta_1 and beta_2 \n(fm_lm &lt;- lm(y ~ timepoint, data = dat)) |&gt; summary()\n\n\nCall:\nlm(formula = y ~ timepoint, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.4127  -1.5931   0.0072   2.0727  10.4653 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)     0.1868     0.3489   0.535   0.5930  \ntimepointpost   1.0063     0.4934   2.039   0.0427 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.489 on 198 degrees of freedom\nMultiple R-squared:  0.02057,   Adjusted R-squared:  0.01563 \nF-statistic: 4.159 on 1 and 198 DF,  p-value: 0.04274\n\n\n\n\nMixed effects model with correlation between (Intercept) and timepointpost\n\n(fm_lmer &lt;- lmer(y ~ timepoint + (timepoint | patient_id), data = dat))\n\nboundary (singular) fit: see help('isSingular')\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ timepoint + (timepoint | patient_id)\n   Data: dat\nREML criterion at convergence: 888.5308\nRandom effects:\n Groups     Name          Std.Dev. Corr \n patient_id (Intercept)   0.3268        \n            timepointpost 4.5307   -1.00\n Residual                 1.9086        \nNumber of obs: 200, groups:  patient_id, 20\nFixed Effects:\n  (Intercept)  timepointpost  \n       0.1868         1.0063  \noptimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings \n\n\n\n\nMixed effects model without correlation between (Intercept) and timepointpost\n\nThe specifications I tried seem almost right, but not quite:\n\n\n(fm_lmer &lt;- lmer(y ~ timepoint + (timepoint - 1 || patient_id), data = dat))\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ timepoint + ((0 + timepoint | patient_id))\n   Data: dat\nREML criterion at convergence: 888.5308\nRandom effects:\n Groups     Name          Std.Dev. Corr \n patient_id timepointpre  0.3268        \n            timepointpost 4.2038   -1.00\n Residual                 1.9086        \nNumber of obs: 200, groups:  patient_id, 20\nFixed Effects:\n  (Intercept)  timepointpost  \n       0.1868         1.0063  \n\n(fm_lmer &lt;- lmer(y ~ timepoint + (timepoint - 1 | patient_id), data = dat))\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ timepoint + (timepoint - 1 | patient_id)\n   Data: dat\nREML criterion at convergence: 888.5308\nRandom effects:\n Groups     Name          Std.Dev. Corr \n patient_id timepointpre  0.3268        \n            timepointpost 4.2038   -1.00\n Residual                 1.9086        \nNumber of obs: 200, groups:  patient_id, 20\nFixed Effects:\n  (Intercept)  timepointpost  \n       0.1868         1.0063  \n\n(fm_lmer &lt;- lmer(y ~ timepoint + (1 | patient_id) + (timepoint - 1 | patient_id), data = dat))\n\nboundary (singular) fit: see help('isSingular')\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ timepoint + (1 | patient_id) + (timepoint - 1 | patient_id)\n   Data: dat\nREML criterion at convergence: 888.5308\nRandom effects:\n Groups       Name          Std.Dev. Corr \n patient_id   (Intercept)   0.0000        \n patient_id.1 timepointpre  0.3268        \n              timepointpost 4.2039   -1.00\n Residual                   1.9086        \nNumber of obs: 200, groups:  patient_id, 20\nFixed Effects:\n  (Intercept)  timepointpost  \n       0.1868         1.0063  \noptimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings \n\n(fm_lmer &lt;- lmer(y ~ timepoint + (1 | patient_id) + (timepoint + 0 | patient_id), data = dat))\n\nboundary (singular) fit: see help('isSingular')\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ timepoint + (1 | patient_id) + (timepoint + 0 | patient_id)\n   Data: dat\nREML criterion at convergence: 888.5308\nRandom effects:\n Groups       Name          Std.Dev. Corr \n patient_id   (Intercept)   0.0000        \n patient_id.1 timepointpre  0.3268        \n              timepointpost 4.2039   -1.00\n Residual                   1.9086        \nNumber of obs: 200, groups:  patient_id, 20\nFixed Effects:\n  (Intercept)  timepointpost  \n       0.1868         1.0063  \noptimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings"
  },
  {
    "objectID": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html",
    "href": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html",
    "title": "Coursera Deep Learning Specialization",
    "section": "",
    "text": "How to input images of different dimensions in a neural net?\nI don’t quite understand yet how convolutions finally integrate more than just local information. In my understanding this can only happen efficiently in the fully connected layers then?\nUnderstand cost function and understand the derivatives\nKeras works with float32 (single float) - double is never used? Single is sufficient?\nRead a little about Boltzmann machines"
  },
  {
    "objectID": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html#study-questions",
    "href": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html#study-questions",
    "title": "Coursera Deep Learning Specialization",
    "section": "",
    "text": "How to input images of different dimensions in a neural net?\nI don’t quite understand yet how convolutions finally integrate more than just local information. In my understanding this can only happen efficiently in the fully connected layers then?\nUnderstand cost function and understand the derivatives\nKeras works with float32 (single float) - double is never used? Single is sufficient?\nRead a little about Boltzmann machines"
  },
  {
    "objectID": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html#course-2-week-2",
    "href": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html#course-2-week-2",
    "title": "Coursera Deep Learning Specialization",
    "section": "Course 2, Week 2",
    "text": "Course 2, Week 2\n\nMini-batch Gradient Descent\n\nhow do I controll this in Keras / PyTorch / whatever?\nmini batch size = 1 -&gt; stochastic gradient descent\nmini batch size power of 2\n\n\n\nExponentially moving averages:\n\\[V_t = \\beta V_{t-1} + (1 - \\beta) \\theta_t\\] * this corresponds approximately to averaging over \\(\\frac{1}{1-\\beta}\\) entries\n\n\nGradient descent with momentum\n\ngradients with exponentially moving average\nif learning rate too large risk of diverging\nlocal minima are not the main problem - the problem are saddle points!\nball on a hill: velocity is stored in the average, new gradient is the acceleration\n\\(\\beta = 0.9\\) averaging over approx. the last 10 iterations\nUpdate rule: \\[ V_{dw} = \\beta V_{dw} + (1-\\beta) dW\\]\noften approximated by: \\[ V_{dw} = \\beta V_{dw} + dW\\]\n\n\n\nRoot Mean Square Propagation\n\ndW is small, db is large (is this true?), here we call the hyperparameter \\(\\beta_2\\), also \\(\\epsilon = 10^{-8}\\) \\[ S_{dW} := \\beta S_{dW} + (1 - \\beta) dW^2\\] \\[ W := W - \\alpha \\frac{dW}{\\sqrt{S_{dW}} + \\epsilon}\\] \\[ S_{db} := \\beta S_{db} + (1 - \\beta) db^2\\] \\[ W := W - \\alpha \\frac{db}{\\sqrt{S_{db}} + \\epsilon}\\]\n\n\n\nAdaptive moment estimation optimization algorithm (Adam)\nInitiliazation: \\(V_{dW} = 0\\), \\(S_{dW} = 0\\), \\(V_{db} = 0\\), \\(S_{db} = 0\\), \\[ V_{dW} = \\beta_1 V_{dW} + (1 - \\beta_1) dW\\] \\[ V_{db} = \\beta_1 V_{db} + (1 - \\beta_1) db\\] \\[ S_{dW} = \\beta_2 S_{dW} + (1 - beta_2) dW^2\\] \\[ S_{db} = \\beta_2 S_{db} + (1 - beta_2) db^2\\] Typically implemented with bias correction \\[ V_{dW}^{corrected} = V_{dW} / (1 - \\beta_1 ^ t)\\] \\[ V_{db}^{corrected} = V_{db} / (1 - \\beta_1 ^ t)\\] \\[ S_{dW}^{corrected} = S_{dW} / (1 - \\beta_2 ^ t)\\] \\[ S_{db}^{corrected} = S_{db} / (1 - \\beta_2 ^ t)\\] Update rule: \\(w := w - \\alpha \\frac{V_{dW}^{corrected}}{\\sqrt{S^{corrected}_{dW}} + \\epsilon}\\) \\(b := b - \\alpha \\frac{V_{db}^{corrected}}{\\sqrt{S^{corrected}_{db}} + \\epsilon}\\) Hyperparameters:\n\n\\(\\alpha\\) needs to be tuned\n\\(\\beta_1\\): usually 0.9\n\\(\\beta_2\\): usually 0.999\n$ $:usually \\(10e-8\\)\n\n\n\nLearning rate decay\n\\[ \\alpha = 1 / (1 + {decayrate} \\times {epochnumber})  \\alpha_0\\]\n\n\nSoftmax activation function\n\\[ a_i = \\frac{\\exp{z_i}}{\\sum_{j=1}^{n}{\\exp{z_j}}}\\]\n\n\nSoftmax regression\n\ngeneralization of logistic regression to \\(n\\) classes\nLikelihood: \\[ L(\\hat{y}, y) = - \\sum^C_{j = 1} y_j log \\hat{y_j} \\]"
  },
  {
    "objectID": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html#course-3-week-2",
    "href": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html#course-3-week-2",
    "title": "Coursera Deep Learning Specialization",
    "section": "Course 3, Week 2",
    "text": "Course 3, Week 2\n\nAvoidable bias: discrepancy between Bayes’ error and training error\n\n\nBias and variance with mismatched data distrubtions\n\npotential help: add a Training-dev set\ntraining error, training-dev error, dev error\ndata mismatch problem\nData augmentation: when sampling only a small subset of all possible examples overfitting may occur (e. g. cars in a video game)\nTransfer learning: example image recognition\n\ntrain network to a lot of images\nnow x-rays: delete weights of the last layer of the network and retrain them (a few x-rays available for training) or all parameters of the network (a lot of x-rays available for training)\n\nMultitask learning - output vector, doesn’t have to be fully labeled\nTransfer learning currently learned more frequently than multitask learning\nMultitask learning: each class should have similar number of items"
  },
  {
    "objectID": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html#course-4",
    "href": "posts/2023-03-17-coursera-deep-learning/2023-03-17-coursera-deep-learning.html#course-4",
    "title": "Coursera Deep Learning Specialization",
    "section": "Course 4",
    "text": "Course 4\n\nstrided and padded convolution\ncross-correlation (without flipping) vs. convolution (flipping around vertical and horizontal axis) - in ML, cross-correlation is by convention also called convolution\nconvolutions over volumes (\\(n_c\\) = number of channels) \\[ n \\times n \\times n_c * f \\times f \\times n_c \\rightarrow (n - f + 1) * (n - f + 1) * n_c^{prime}\\]\nprime means in the next layer\n\n\nPooling\n\nmax pooling: only hyperparameters stride, padding (usually zero) and f\naverage pooling: rarely used, very sometimes for example to collapse spatial dimension (e.g. 7x7 with 1000 channels to 1x1 with 1000 channels)\nno parameters to learn"
  },
  {
    "objectID": "posts/2022-09-08-psychopharmaca-and-time/2022-09-08-psychopharmaca-and-time.html",
    "href": "posts/2022-09-08-psychopharmaca-and-time/2022-09-08-psychopharmaca-and-time.html",
    "title": "Psychopharmaca and time",
    "section": "",
    "text": "library('lattice')\nlibrary('readxl')\n\ndat &lt;- read_excel('Psychopharmaka_Data.xlsx')\n\n\ndat &lt;- dat[order(dat$Jahr), ]\n\n\nstripplot(order(dat$Jahr) ~ Jahr, data = dat, \n          panel = \n            function(x, y, subscripts, ...) {\n              panel.grid(h = -1, v = -1)\n              lsegments(x0 = x, y0 = y - 0.2, \n                        x1 = x, y1 = y - 0.6,\n                        lwd = 3)\n              ltext(x = x, y = y, \n                    labels = dat$Substanz[subscripts])\n              }, \n          xlim = c(1945, 2015), \n          ylab = '')"
  },
  {
    "objectID": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html",
    "href": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html",
    "title": "Simulate time to-event-data",
    "section": "",
    "text": "In this post, two methods to draw from a survival time distribution defined by an arbitrary hazard function are demonstrated. First, some central equations of survival analysis are given in order to refresh the background and establish a consistent notation. Then, the two different approaches are demonstrated."
  },
  {
    "objectID": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#intro",
    "href": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#intro",
    "title": "Simulate time to-event-data",
    "section": "",
    "text": "In this post, two methods to draw from a survival time distribution defined by an arbitrary hazard function are demonstrated. First, some central equations of survival analysis are given in order to refresh the background and establish a consistent notation. Then, the two different approaches are demonstrated."
  },
  {
    "objectID": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#mathematical-aspects",
    "href": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#mathematical-aspects",
    "title": "Simulate time to-event-data",
    "section": "Mathematical aspects",
    "text": "Mathematical aspects\n\nTaken from wikipedia - at this wikipedia page also further explanation can be found\nSurvival function \\(S\\) \\[ S(t) = Pr(T &gt; t) \\]\nHazard function \\(\\lambda\\) \\[ \\lambda(t) = - S'(t) / S(t) \\]\nCumulative hazard function \\(\\Lambda\\) \\[ \\Lambda(t)  = \\int_0^t{\\lambda(u) du} \\]\nRelation between \\(\\Lambda\\) and \\(S\\) \\[ S(t) = exp(-\\Lambda(t)) \\]"
  },
  {
    "objectID": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#ad-hoc-code-to-draw-from-survival-time-distribution",
    "href": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#ad-hoc-code-to-draw-from-survival-time-distribution",
    "title": "Simulate time to-event-data",
    "section": "Ad hoc code to draw from survival time distribution",
    "text": "Ad hoc code to draw from survival time distribution\n\nset.seed(1)\nlibrary('survival')\n\n# define hazard function:\nlambda &lt;- function(t) {\n  abs(sin(t))\n}\n\n\n# hazard for t = 7 and t = 12:\nlambda(c(7, 12)) \n\n[1] 0.6569866 0.5365729\n\n# plot hazard function:\nplot(lambda, from = 0, to = 10, xlab = 't', ylab = expression(lambda))\n\n\n\n# define function that computes cumulative hazard function:\nLambda &lt;- Vectorize(\n  function(t) {\n    integrate(lambda,\n              subdivisions = 1e3L,\n              lower = 0, \n              upper = t)$value\n  })\n\n# cumulative hazard for t = 3\nLambda(3) \n\n[1] 1.989992\n\n# plot cumulative hazard function:\nplot(Lambda, from = 0, to = 10, xlab = 't', ylab = expression(Lambda))\n\n\n\n# define function that computes survival function \n# from cumulative hazard function:\nS &lt;- Vectorize(\n  function(t) {\n    exp(-Lambda(t))\n  })\n\n# plot survival function:\nplot(S, from = 0, to = 10, xlab = 't')\n\n\n\n# determine at which time cumulative hazard\n# reaches 2e1 (i. e. S(t) &lt; 1e-8)\n# the value is used as upper bound for\n# optimization to find the quantile\nupper_bound &lt;- optimize(\n  function(x) {\n    abs(Lambda(x) - 2e1)\n  }, \n  interval = c(0, 1e2))$minimum\n\n# define function to compute quantile of survival function\nquantile_function &lt;- Vectorize(\n  function(t) {\n    optimize(function(x) {\n      abs(S(x) - t)}, \n      interval = c(0, upper_bound))$min\n  })\n\n# compute some quantiles for hazard function\n# that has been specified above:\nquantile_function(c(0.25, 0.5, 0.75))\n\n[1] 1.9674130 1.2589101 0.7779829\n\n# evaluate how long it takes to determine 10 quantiles:\nmicrobenchmark::microbenchmark(quantile_function(1:10))\n\nWarning in microbenchmark::microbenchmark(quantile_function(1:10)): less\naccurate nanosecond times to avoid potential integer overflows\n\n\nUnit: milliseconds\n                    expr      min       lq     mean   median       uq      max\n quantile_function(1:10) 10.19416 10.70949 11.62673 11.01805 12.94298 15.45019\n neval\n   100\n\n# draw n random numbers from survival distribution \n# defined by hazard function above\n(times &lt;- quantile_function(runif(n = 20)))\n\n [1] 0.9040743 1.0566904 2.0173300 1.9328205 0.8170795 1.3620799 2.4072092\n [8] 0.7842570 3.8702329 0.5462974 1.0317729 1.1427625 1.6834271 1.3608340\n[15] 1.2579906 2.3602768 1.1977742 4.2861974 1.8556640 2.1505993\n\nplot(survfit(Surv(times) ~ 1), xlab = 't', ylab = 'S')"
  },
  {
    "objectID": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#more-sophisticated-solution-using-the-coxed-package",
    "href": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#more-sophisticated-solution-using-the-coxed-package",
    "title": "Simulate time to-event-data",
    "section": "More sophisticated solution using the coxed package",
    "text": "More sophisticated solution using the coxed package\n\nSolution found here:\n\n\nlibrary('coxed')\n\nmy.hazard &lt;- function(t){ \n  abs(sin(t)) * 0.01\n}\n\nsimdata &lt;- sim.survdata(N = 100, \n                        T = 1000, \n                        hazard.fun = my.hazard)\n\nWarning in FUN(X[[i]], ...): 0 additional observations right-censored because the user-supplied hazard function\n                                  is nonzero at the latest timepoint. To avoid these extra censored observations, increase T\n\nplot(survfit(Surv(simdata$data$y) ~ 1), xlab = 't', ylab = 'S')"
  },
  {
    "objectID": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#conclusion",
    "href": "posts/2022-10-25-simulate-time-to-event-data/2022-10-25-simulate-time-to-event-data.html#conclusion",
    "title": "Simulate time to-event-data",
    "section": "Conclusion",
    "text": "Conclusion\nIt is possible to draw survival times defined by an arbitrary hazard function with just a few lines of code. However, the code is not very time-efficient and there might be issues with numerical stability in certain constellations - hence, plausibility of the results needs to be checked manually.\nThe coxed package provides convenient, more sophisticated methods to achieve a similar goal. However, the coxed approach might be more difficult to adapt to specific situations since the code base is more rigid."
  },
  {
    "objectID": "posts/2023-03-17-goodfellow-deep-learning/2023-03-17-goodfellow-deep-learning.html#probability-and-information-theory",
    "href": "posts/2023-03-17-goodfellow-deep-learning/2023-03-17-goodfellow-deep-learning.html#probability-and-information-theory",
    "title": "Self-study: Deep Learning by Goodfellow et al.",
    "section": "Probability and Information Theory",
    "text": "Probability and Information Theory\n\n“Researchers have made compelling arguments for quantifying uncertainty using probability since at least the 1980s.”\nThree sources of uncertainty:\n\nInherent stochasticity (e. g. quantum mechanics, card shuffling)\nIncomplete observability (e. g. Monty hall problem)\nIncomplete modeling\n\nProbability of used as degree of belief\n\n\nRandom variables (3.2)\n\nnotation:\n\nrandom variable: plain typeface\nvalues it can take with lowerscript letters\nvector-valued: bold\n\n\n\n\nProbability distribution (3.3)\n\ndiscrete variables: probability mass function (notation: usually \\(P\\))\nnotation: \\(x \\sim P(x)\\)"
  },
  {
    "objectID": "posts/2023-03-17-goodfellow-deep-learning/2023-03-17-goodfellow-deep-learning.html#general-statistics",
    "href": "posts/2023-03-17-goodfellow-deep-learning/2023-03-17-goodfellow-deep-learning.html#general-statistics",
    "title": "Self-study: Deep Learning by Goodfellow et al.",
    "section": "General statistics",
    "text": "General statistics\n\nLaw of total probability\n\\(B_n\\): partition of entire sample space \\(\\rightarrow \\sum p(B_n) = 1\\) \\[\nP(A) = \\sum_n P(A \\cap B_n)\n\\]\n\n\nStatistical independence\n\\[\np(A|B) = p(A)\n\\]\n\n\nConditional probability\n\\[\nP(A | B) = \\frac{A \\cap B}{P(B)}\n\\]\n\n\nBayes` theorem\n\\[\n\\begin{align}\n\\frac{P(A|B)}{P(B|A)} &= \\frac{P(A)}{P(B)} \\\\\nP(A|B) \\cdot P(B) &= P(B|A) \\cdot P(A)\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/2023-03-17-goodfellow-deep-learning/2023-03-17-goodfellow-deep-learning.html#look-at-file-_notation.tex-in-the-corresponding-folder",
    "href": "posts/2023-03-17-goodfellow-deep-learning/2023-03-17-goodfellow-deep-learning.html#look-at-file-_notation.tex-in-the-corresponding-folder",
    "title": "Self-study: Deep Learning by Goodfellow et al.",
    "section": "Look at file “_notation.tex” in the corresponding folder!",
    "text": "Look at file “_notation.tex” in the corresponding folder!"
  },
  {
    "objectID": "posts/2023-08-08-leave-one-out-cross-validation-in-r/2023-08-08-leave-one-out-cross-validation-in-r.html",
    "href": "posts/2023-08-08-leave-one-out-cross-validation-in-r/2023-08-08-leave-one-out-cross-validation-in-r.html",
    "title": "Leave-one-out cross-validation in R",
    "section": "",
    "text": "Fit “straight” models\n\nlinear model - fm_lm\nneural net with 2 layers with 3 perceptrons each - fm_nn\n\nFit models with “Leave-one-out-cross-validation”\n\nlinear model - lm_looc\nneural net - nn_looc\nCalculate mean absolute prediction error for both model types (predict each data point based on a model that is based on all but the current data point)"
  },
  {
    "objectID": "posts/2023-08-08-leave-one-out-cross-validation-in-r/2023-08-08-leave-one-out-cross-validation-in-r.html#strategy",
    "href": "posts/2023-08-08-leave-one-out-cross-validation-in-r/2023-08-08-leave-one-out-cross-validation-in-r.html#strategy",
    "title": "Leave-one-out cross-validation in R",
    "section": "",
    "text": "Fit “straight” models\n\nlinear model - fm_lm\nneural net with 2 layers with 3 perceptrons each - fm_nn\n\nFit models with “Leave-one-out-cross-validation”\n\nlinear model - lm_looc\nneural net - nn_looc\nCalculate mean absolute prediction error for both model types (predict each data point based on a model that is based on all but the current data point)"
  },
  {
    "objectID": "posts/2023-08-08-leave-one-out-cross-validation-in-r/2023-08-08-leave-one-out-cross-validation-in-r.html#implementation",
    "href": "posts/2023-08-08-leave-one-out-cross-validation-in-r/2023-08-08-leave-one-out-cross-validation-in-r.html#implementation",
    "title": "Leave-one-out cross-validation in R",
    "section": "Implementation",
    "text": "Implementation\n\nInit\n\nset.seed(123)\nlibrary('tidyverse')\nlibrary('knitr')\nlibrary('neuralnet')\nlibrary('plotly')\n\n\n\nDefine function for performing leave-one-out-crossvalidation\n\n# A function that removes one data point from the cars data set, \n# fits either a linear model or a neural net to the remaining data \n# and returns the model prediction for the data point \n# that had been removed in the first place\n# \n# Parameters:\n#   * i: which data points to remove - the function is vectorized for this parameter\n#   * model_type: which type of model is to be fit - either \"lm\" or if not \"lm\", a neural \nleave_one_out &lt;- Vectorize(\n  function(i, model_type) {\n    training &lt;- cars[-i, ]\n    test &lt;- cars[i, ]\n    if(model_type == 'lm') {\n      fm &lt;- lm(speed ~ dist, data = training)\n      pp &lt;- predict(fm, newdata = test)\n    } else if(model_type == 'nn'){\n      fm &lt;- neuralnet(speed ~ dist, \n                      linear.output = TRUE, data = training, \n                      hidden = c(3, 3), \n                      stepmax = 1e+7, threshold = .5)\n      pp &lt;- predict(fm, newdata = test)[, 1]\n    } else {\n      stop('Invalid model type')\n    }\n    return(pp)\n  }, vectorize.args = 'i')\n\n\n\nFit models\n\nfm_lm &lt;- lm(speed ~ dist, data = cars)\nfm_nn &lt;- neuralnet(speed ~ dist, \n                   linear.output = TRUE, data = cars, \n                   hidden = c(3, 3), \n                   stepmax = 1e+7, threshold = .5)\n\ndata &lt;- tibble(\n  cars,\n  prediction_lm_straight = predict(fm_lm),\n  prediction_nn_straight = predict(fm_nn, newdata = cars)[, 1],\n  prediction_lm_looc = leave_one_out(1:nrow(cars), model_type = 'lm'),\n  prediction_nn_looc = leave_one_out(1:nrow(cars), model_type = 'nn')) |&gt; \n  rename(measured_speed = 'speed')\n\n\n\nShow and analyze results\n\ndata |&gt; \n  pivot_longer(cols = -dist, names_to = 'Method', values_to = 'speed') |&gt; \n  ggplot(aes(dist, speed, colour = Method)) +\n  geom_point() +\n  labs(y = 'speed')\n\n\n\n# the previous plot with plotly library so it can be appreciated interactively:\nggplot2::last_plot() |&gt; ggplotly()\n\n\n\n\ndata_long &lt;- data |&gt; \n  pivot_longer(cols = -c(dist, measured_speed), names_to = 'Method',\n               values_to = 'prediction') |&gt;\n  mutate(error = prediction - measured_speed,\n         squared_error = error^2)\n\ndata_long |&gt; \n  ggplot(aes(dist, error)) + geom_point() + \n  facet_wrap(vars(Method), labeller = label_both) +\n  labs(y = 'Prediction error')\n\n\n\ndata_long |&gt; \n  group_by(Method) |&gt; \n  summarize('Mean absolute error' = mean(abs(error)),\n            'Root mean squared error' = sqrt(mean(squared_error))) |&gt; \n  kable(align = 'c')\n\n\n\n\n\n\n\n\n\nMethod\nMean absolute error\nRoot mean squared error\n\n\n\n\nprediction_lm_looc\n2.633568\n3.244277\n\n\nprediction_lm_straight\n2.518286\n3.091994\n\n\nprediction_nn_looc\n2.888501\n3.458509\n\n\nprediction_nn_straight\n1.948435\n2.430247"
  },
  {
    "objectID": "posts/2023-08-08-leave-one-out-cross-validation-in-r/2023-08-08-leave-one-out-cross-validation-in-r.html#conclusion",
    "href": "posts/2023-08-08-leave-one-out-cross-validation-in-r/2023-08-08-leave-one-out-cross-validation-in-r.html#conclusion",
    "title": "Leave-one-out cross-validation in R",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe neural net seems to overfit!"
  },
  {
    "objectID": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html",
    "href": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html",
    "title": "Chess ressources",
    "section": "",
    "text": "Link"
  },
  {
    "objectID": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html#chess-related-packages-on-cran",
    "href": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html#chess-related-packages-on-cran",
    "title": "Chess ressources",
    "section": "",
    "text": "Link"
  },
  {
    "objectID": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html#nice-little-chess-engine",
    "href": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html#nice-little-chess-engine",
    "title": "Chess ressources",
    "section": "nice little chess engine:",
    "text": "nice little chess engine:\nhttps://www.codertime.org/minimax-chess-engine-programming-r/"
  },
  {
    "objectID": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html#interesting-people",
    "href": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html#interesting-people",
    "title": "Chess ressources",
    "section": "Interesting people:",
    "text": "Interesting people:\n\nhttps://www.chessprogramming.org/Guy_Haworth\nhttps://www.chessprogramming.org/Kenneth_W._Regan"
  },
  {
    "objectID": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html#interesting-publications",
    "href": "posts/2022-09-08-chess-ressources/2022-09-08-chess-ressources.html#interesting-publications",
    "title": "Chess ressources",
    "section": "interesting publications:",
    "text": "interesting publications:\n\nhttps://cse.buffalo.edu/~regan/papers/pdf/BHR2015ACG.pdf"
  },
  {
    "objectID": "posts/2023-07-23-approximating-pi/2023-07-23-approximating-pi.html",
    "href": "posts/2023-07-23-approximating-pi/2023-07-23-approximating-pi.html",
    "title": "Approximating Pi",
    "section": "",
    "text": "This is a really pointless post about approximating \\(\\pi\\), inspired by this post.\n\ncurrent_min_error &lt;- abs(pi - 22 / 7)\ncurrent_best_numerator &lt;- 22\ncurrent_best_denominator &lt;- 7\n\nfor(denominator in 1:1e7) {\n  numerator &lt;- round(denominator * pi)\n  pi_approx &lt;- numerator / denominator\n  pi_error &lt;- abs(pi - pi_approx)\n  if(pi_error &lt; current_min_error) {\n    current_min_error &lt;- pi_error\n    current_best_numerator &lt;- numerator\n    current_best_denominator &lt;- denominator\n    cat(paste(c('numerator:', numerator, \n                  ', denominator:', denominator, \n                  ', pi_approx: ', numerator / denominator, \n                  ', absolute error', pi_error, '\\n')))\n  }\n}\n\nnumerator: 179 , denominator: 57 , pi_approx:  3.14035087719298 , absolute error 0.00124177639681067 \nnumerator: 201 , denominator: 64 , pi_approx:  3.140625 , absolute error 0.000967653589793116 \nnumerator: 223 , denominator: 71 , pi_approx:  3.14084507042254 , absolute error 0.000747583167258092 \nnumerator: 245 , denominator: 78 , pi_approx:  3.14102564102564 , absolute error 0.000567012564152147 \nnumerator: 267 , denominator: 85 , pi_approx:  3.14117647058824 , absolute error 0.000416183001557879 \nnumerator: 289 , denominator: 92 , pi_approx:  3.14130434782609 , absolute error 0.000288305763706198 \nnumerator: 311 , denominator: 99 , pi_approx:  3.14141414141414 , absolute error 0.000178512175651679 \nnumerator: 333 , denominator: 106 , pi_approx:  3.14150943396226 , absolute error 8.32196275291075e-05 \nnumerator: 355 , denominator: 113 , pi_approx:  3.14159292035398 , absolute error 2.66764189404967e-07 \nnumerator: 52163 , denominator: 16604 , pi_approx:  3.14159238737654 , absolute error 2.66213257216208e-07 \nnumerator: 52518 , denominator: 16717 , pi_approx:  3.14159239097924 , absolute error 2.62610550638698e-07 \nnumerator: 52873 , denominator: 16830 , pi_approx:  3.14159239453357 , absolute error 2.5905622225153e-07 \nnumerator: 53228 , denominator: 16943 , pi_approx:  3.14159239804049 , absolute error 2.55549304384317e-07 \nnumerator: 53583 , denominator: 17056 , pi_approx:  3.14159240150094 , absolute error 2.52088855123844e-07 \nnumerator: 53938 , denominator: 17169 , pi_approx:  3.14159240491584 , absolute error 2.48673956537715e-07 \nnumerator: 54293 , denominator: 17282 , pi_approx:  3.14159240828608 , absolute error 2.45303715118439e-07 \nnumerator: 54648 , denominator: 17395 , pi_approx:  3.14159241161253 , absolute error 2.41977260895254e-07 \nnumerator: 55003 , denominator: 17508 , pi_approx:  3.14159241489605 , absolute error 2.3869374565777e-07 \nnumerator: 55358 , denominator: 17621 , pi_approx:  3.14159241813745 , absolute error 2.35452343400055e-07 \nnumerator: 55713 , denominator: 17734 , pi_approx:  3.14159242133754 , absolute error 2.32252249432463e-07 \nnumerator: 56068 , denominator: 17847 , pi_approx:  3.14159242449711 , absolute error 2.29092678605269e-07 \nnumerator: 56423 , denominator: 17960 , pi_approx:  3.14159242761693 , absolute error 2.25972866640944e-07 \nnumerator: 56778 , denominator: 18073 , pi_approx:  3.14159243069773 , absolute error 2.22892067025526e-07 \nnumerator: 57133 , denominator: 18186 , pi_approx:  3.14159243374024 , absolute error 2.19849553229068e-07 \nnumerator: 57488 , denominator: 18299 , pi_approx:  3.14159243674518 , absolute error 2.16844615597012e-07 \nnumerator: 57843 , denominator: 18412 , pi_approx:  3.14159243971323 , absolute error 2.13876562682458e-07 \nnumerator: 58198 , denominator: 18525 , pi_approx:  3.14159244264507 , absolute error 2.10944719025719e-07 \nnumerator: 58553 , denominator: 18638 , pi_approx:  3.14159244554137 , absolute error 2.08048426042495e-07 \nnumerator: 58908 , denominator: 18751 , pi_approx:  3.14159244840275 , absolute error 2.05187041135702e-07 \nnumerator: 59263 , denominator: 18864 , pi_approx:  3.14159245122986 , absolute error 2.02359937251373e-07 \nnumerator: 59618 , denominator: 18977 , pi_approx:  3.14159245402329 , absolute error 1.995665015464e-07 \nnumerator: 59973 , denominator: 19090 , pi_approx:  3.14159245678366 , absolute error 1.96806136720795e-07 \nnumerator: 60328 , denominator: 19203 , pi_approx:  3.14159245951153 , absolute error 1.94078258353159e-07 \nnumerator: 60683 , denominator: 19316 , pi_approx:  3.1415924622075 , absolute error 1.91382296677034e-07 \nnumerator: 61038 , denominator: 19429 , pi_approx:  3.1415924648721 , absolute error 1.88717694804552e-07 \nnumerator: 61393 , denominator: 19542 , pi_approx:  3.14159246750588 , absolute error 1.86083908282342e-07 \nnumerator: 61748 , denominator: 19655 , pi_approx:  3.14159247010939 , absolute error 1.83480405979708e-07 \nnumerator: 62103 , denominator: 19768 , pi_approx:  3.14159247268312 , absolute error 1.80906668756364e-07 \nnumerator: 62458 , denominator: 19881 , pi_approx:  3.1415924752276 , absolute error 1.78362189018344e-07 \nnumerator: 62813 , denominator: 19994 , pi_approx:  3.14159247774332 , absolute error 1.7584647027391e-07 \nnumerator: 63168 , denominator: 20107 , pi_approx:  3.14159248023077 , absolute error 1.73359027577646e-07 \nnumerator: 63523 , denominator: 20220 , pi_approx:  3.14159248269041 , absolute error 1.70899387530454e-07 \nnumerator: 63878 , denominator: 20333 , pi_approx:  3.14159248512271 , absolute error 1.6846708605911e-07 \nnumerator: 64233 , denominator: 20446 , pi_approx:  3.14159248752812 , absolute error 1.66061670192619e-07 \nnumerator: 64588 , denominator: 20559 , pi_approx:  3.1415924899071 , absolute error 1.63682696285861e-07 \nnumerator: 64943 , denominator: 20672 , pi_approx:  3.14159249226006 , absolute error 1.61329731351856e-07 \nnumerator: 65298 , denominator: 20785 , pi_approx:  3.14159249458744 , absolute error 1.5900235039723e-07 \nnumerator: 65653 , denominator: 20898 , pi_approx:  3.14159249688965 , absolute error 1.56700138642663e-07 \nnumerator: 66008 , denominator: 21011 , pi_approx:  3.1415924991671 , absolute error 1.54422690190614e-07 \nnumerator: 66363 , denominator: 21124 , pi_approx:  3.14159250142019 , absolute error 1.52169607581243e-07 \nnumerator: 66718 , denominator: 21237 , pi_approx:  3.14159250364929 , absolute error 1.499405017924e-07 \nnumerator: 67073 , denominator: 21350 , pi_approx:  3.1415925058548 , absolute error 1.47734992239634e-07 \nnumerator: 67428 , denominator: 21463 , pi_approx:  3.14159250803709 , absolute error 1.45552705888008e-07 \nnumerator: 67783 , denominator: 21576 , pi_approx:  3.14159251019651 , absolute error 1.43393278584369e-07 \nnumerator: 68138 , denominator: 21689 , pi_approx:  3.14159251233344 , absolute error 1.41256352392816e-07 \nnumerator: 68493 , denominator: 21802 , pi_approx:  3.14159251444822 , absolute error 1.39141577371049e-07 \nnumerator: 68848 , denominator: 21915 , pi_approx:  3.14159251654118 , absolute error 1.37048611126289e-07 \nnumerator: 69203 , denominator: 22028 , pi_approx:  3.14159251861267 , absolute error 1.34977118371182e-07 \nnumerator: 69558 , denominator: 22141 , pi_approx:  3.14159252066302 , absolute error 1.32926769591535e-07 \nnumerator: 69913 , denominator: 22254 , pi_approx:  3.14159252269255 , absolute error 1.30897243266759e-07 \nnumerator: 70268 , denominator: 22367 , pi_approx:  3.14159252470157 , absolute error 1.28888223649426e-07 \nnumerator: 70623 , denominator: 22480 , pi_approx:  3.14159252669039 , absolute error 1.26899401653446e-07 \nnumerator: 70978 , denominator: 22593 , pi_approx:  3.14159252865932 , absolute error 1.2493047396589e-07 \nnumerator: 71333 , denominator: 22706 , pi_approx:  3.14159253060865 , absolute error 1.22981143491074e-07 \nnumerator: 71688 , denominator: 22819 , pi_approx:  3.14159253253867 , absolute error 1.21051119350568e-07 \nnumerator: 72043 , denominator: 22932 , pi_approx:  3.14159253444968 , absolute error 1.19140115995009e-07 \nnumerator: 72398 , denominator: 23045 , pi_approx:  3.14159253634194 , absolute error 1.17247853648195e-07 \nnumerator: 72753 , denominator: 23158 , pi_approx:  3.14159253821574 , absolute error 1.15374057862994e-07 \nnumerator: 73108 , denominator: 23271 , pi_approx:  3.14159254007133 , absolute error 1.13518459521345e-07 \nnumerator: 73463 , denominator: 23384 , pi_approx:  3.141592541909 , absolute error 1.11680795722435e-07 \nnumerator: 73818 , denominator: 23497 , pi_approx:  3.14159254372899 , absolute error 1.09860806229989e-07 \nnumerator: 74173 , denominator: 23610 , pi_approx:  3.14159254553155 , absolute error 1.08058238801334e-07 \nnumerator: 74528 , denominator: 23723 , pi_approx:  3.14159254731695 , absolute error 1.06272843414246e-07 \nnumerator: 74883 , denominator: 23836 , pi_approx:  3.14159254908542 , absolute error 1.04504376263748e-07 \nnumerator: 75238 , denominator: 23949 , pi_approx:  3.1415925508372 , absolute error 1.02752597541667e-07 \nnumerator: 75593 , denominator: 24062 , pi_approx:  3.14159255257252 , absolute error 1.0101727232481e-07 \nnumerator: 75948 , denominator: 24175 , pi_approx:  3.14159255429162 , absolute error 9.92981696867901e-08 \nnumerator: 76303 , denominator: 24288 , pi_approx:  3.14159255599473 , absolute error 9.75950631421085e-08 \nnumerator: 76658 , denominator: 24401 , pi_approx:  3.14159255768206 , absolute error 9.59077310902501e-08 \nnumerator: 77013 , denominator: 24514 , pi_approx:  3.14159255935384 , absolute error 9.42359545952343e-08 \nnumerator: 77368 , denominator: 24627 , pi_approx:  3.14159256101027 , absolute error 9.25795200501511e-08 \nnumerator: 77723 , denominator: 24740 , pi_approx:  3.14159256265158 , absolute error 9.0938216512626e-08 \nnumerator: 78078 , denominator: 24853 , pi_approx:  3.14159256427795 , absolute error 8.9311838813444e-08 \nnumerator: 78433 , denominator: 24966 , pi_approx:  3.14159256588961 , absolute error 8.77001831156576e-08 \nnumerator: 78788 , denominator: 25079 , pi_approx:  3.14159256748674 , absolute error 8.61030513554795e-08 \nnumerator: 79143 , denominator: 25192 , pi_approx:  3.14159256906955 , absolute error 8.45202472454787e-08 \nnumerator: 79498 , denominator: 25305 , pi_approx:  3.14159257063821 , absolute error 8.29515793832059e-08 \nnumerator: 79853 , denominator: 25418 , pi_approx:  3.14159257219293 , absolute error 8.13968590307468e-08 \nnumerator: 80208 , denominator: 25531 , pi_approx:  3.14159257373389 , absolute error 7.9855901002901e-08 \nnumerator: 80563 , denominator: 25644 , pi_approx:  3.14159257526127 , absolute error 7.83285232230924e-08 \nnumerator: 80918 , denominator: 25757 , pi_approx:  3.14159257677525 , absolute error 7.68145476115478e-08 \nnumerator: 81273 , denominator: 25870 , pi_approx:  3.141592578276 , absolute error 7.5313797864851e-08 \nnumerator: 81628 , denominator: 25983 , pi_approx:  3.14159257976369 , absolute error 7.38261016763886e-08 \nnumerator: 81983 , denominator: 26096 , pi_approx:  3.1415925812385 , absolute error 7.23512889599931e-08 \nnumerator: 82338 , denominator: 26209 , pi_approx:  3.1415925827006 , absolute error 7.08891940703893e-08 \nnumerator: 82693 , denominator: 26322 , pi_approx:  3.14159258415014 , absolute error 6.94396526945695e-08 \nnumerator: 83048 , denominator: 26435 , pi_approx:  3.14159258558729 , absolute error 6.80025036281506e-08 \nnumerator: 83403 , denominator: 26548 , pi_approx:  3.1415925870122 , absolute error 6.65775887753739e-08 \nnumerator: 83758 , denominator: 26661 , pi_approx:  3.14159258842504 , absolute error 6.51647527050159e-08 \nnumerator: 84113 , denominator: 26774 , pi_approx:  3.14159258982595 , absolute error 6.37638426503884e-08 \nnumerator: 84468 , denominator: 26887 , pi_approx:  3.14159259121509 , absolute error 6.237470762116e-08 \nnumerator: 84823 , denominator: 27000 , pi_approx:  3.14159259259259 , absolute error 6.09972006238024e-08 \nnumerator: 85178 , denominator: 27113 , pi_approx:  3.14159259395862 , absolute error 5.96311755529655e-08 \nnumerator: 85533 , denominator: 27226 , pi_approx:  3.1415925953133 , absolute error 5.82764898560129e-08 \nnumerator: 85888 , denominator: 27339 , pi_approx:  3.14159259665679 , absolute error 5.69330023125758e-08 \nnumerator: 86243 , denominator: 27452 , pi_approx:  3.14159259798922 , absolute error 5.56005756990885e-08 \nnumerator: 86598 , denominator: 27565 , pi_approx:  3.14159259931072 , absolute error 5.42790727919851e-08 \nnumerator: 86953 , denominator: 27678 , pi_approx:  3.14159260062143 , absolute error 5.29683608085918e-08 \nnumerator: 87308 , denominator: 27791 , pi_approx:  3.14159260192149 , absolute error 5.16683078544133e-08 \nnumerator: 87663 , denominator: 27904 , pi_approx:  3.14159260321101 , absolute error 5.03787838113112e-08 \nnumerator: 88018 , denominator: 28017 , pi_approx:  3.14159260449013 , absolute error 4.90996621138606e-08 \nnumerator: 88373 , denominator: 28130 , pi_approx:  3.14159260575898 , absolute error 4.78308170848152e-08 \nnumerator: 88728 , denominator: 28243 , pi_approx:  3.14159260701767 , absolute error 4.65721248232853e-08 \nnumerator: 89083 , denominator: 28356 , pi_approx:  3.14159260826633 , absolute error 4.53234649810952e-08 \nnumerator: 89438 , denominator: 28469 , pi_approx:  3.14159260950508 , absolute error 4.40847172100689e-08 \nnumerator: 89793 , denominator: 28582 , pi_approx:  3.14159261073403 , absolute error 4.28557647147443e-08 \nnumerator: 90148 , denominator: 28695 , pi_approx:  3.1415926119533 , absolute error 4.16364911437483e-08 \nnumerator: 90503 , denominator: 28808 , pi_approx:  3.14159261316301 , absolute error 4.04267828102434e-08 \nnumerator: 90858 , denominator: 28921 , pi_approx:  3.14159261436327 , absolute error 3.92265278037485e-08 \nnumerator: 91213 , denominator: 29034 , pi_approx:  3.14159261555418 , absolute error 3.80356151019612e-08 \nnumerator: 91568 , denominator: 29147 , pi_approx:  3.14159261673586 , absolute error 3.68539367912035e-08 \nnumerator: 91923 , denominator: 29260 , pi_approx:  3.14159261790841 , absolute error 3.56813858459759e-08 \nnumerator: 92278 , denominator: 29373 , pi_approx:  3.14159261907194 , absolute error 3.45178561289572e-08 \nnumerator: 92633 , denominator: 29486 , pi_approx:  3.14159262022655 , absolute error 3.33632450555399e-08 \nnumerator: 92988 , denominator: 29599 , pi_approx:  3.14159262137234 , absolute error 3.22174495970273e-08 \nnumerator: 93343 , denominator: 29712 , pi_approx:  3.14159262250942 , absolute error 3.10803693892581e-08 \nnumerator: 93698 , denominator: 29825 , pi_approx:  3.14159262363789 , absolute error 2.99519054003383e-08 \nnumerator: 94053 , denominator: 29938 , pi_approx:  3.14159262475783 , absolute error 2.88319603747311e-08 \nnumerator: 94408 , denominator: 30051 , pi_approx:  3.14159262586936 , absolute error 2.77204375009887e-08 \nnumerator: 94763 , denominator: 30164 , pi_approx:  3.14159262697255 , absolute error 2.66172430762879e-08 \nnumerator: 95118 , denominator: 30277 , pi_approx:  3.14159262806751 , absolute error 2.55222829537161e-08 \nnumerator: 95473 , denominator: 30390 , pi_approx:  3.14159262915433 , absolute error 2.44354660949853e-08 \nnumerator: 95828 , denominator: 30503 , pi_approx:  3.14159263023309 , absolute error 2.33567014618075e-08 \nnumerator: 96183 , denominator: 30616 , pi_approx:  3.14159263130389 , absolute error 2.22858997922515e-08 \nnumerator: 96538 , denominator: 30729 , pi_approx:  3.14159263236682 , absolute error 2.1222973600743e-08 \nnumerator: 96893 , denominator: 30842 , pi_approx:  3.14159263342196 , absolute error 2.01678358457968e-08 \nnumerator: 97248 , denominator: 30955 , pi_approx:  3.14159263446939 , absolute error 1.91204021504632e-08 \nnumerator: 97603 , denominator: 31068 , pi_approx:  3.14159263550921 , absolute error 1.80805876937029e-08 \nnumerator: 97958 , denominator: 31181 , pi_approx:  3.14159263654148 , absolute error 1.70483094308338e-08 \nnumerator: 98313 , denominator: 31294 , pi_approx:  3.14159263756631 , absolute error 1.60234865376196e-08 \nnumerator: 98668 , denominator: 31407 , pi_approx:  3.14159263858376 , absolute error 1.50060377457351e-08 \nnumerator: 99023 , denominator: 31520 , pi_approx:  3.14159263959391 , absolute error 1.39958844513899e-08 \nnumerator: 99378 , denominator: 31633 , pi_approx:  3.14159264059685 , absolute error 1.2992948050794e-08 \nnumerator: 99733 , denominator: 31746 , pi_approx:  3.14159264159264 , absolute error 1.19971517165141e-08 \nnumerator: 100088 , denominator: 31859 , pi_approx:  3.14159264258137 , absolute error 1.1008419065206e-08 \nnumerator: 100443 , denominator: 31972 , pi_approx:  3.14159264356312 , absolute error 1.00266754898826e-08 \nnumerator: 100798 , denominator: 32085 , pi_approx:  3.14159264453795 , absolute error 9.0518468276457e-09 \nnumerator: 101153 , denominator: 32198 , pi_approx:  3.14159264550593 , absolute error 8.08386113604342e-09 \nnumerator: 101508 , denominator: 32311 , pi_approx:  3.14159264646715 , absolute error 7.12264558444531e-09 \nnumerator: 101863 , denominator: 32424 , pi_approx:  3.14159264742166 , absolute error 6.1681300067562e-09 \nnumerator: 102218 , denominator: 32537 , pi_approx:  3.14159264836955 , absolute error 5.22024468097015e-09 \nnumerator: 102573 , denominator: 32650 , pi_approx:  3.14159264931087 , absolute error 4.27892032917043e-09 \nnumerator: 102928 , denominator: 32763 , pi_approx:  3.1415926502457 , absolute error 3.34408900570793e-09 \nnumerator: 103283 , denominator: 32876 , pi_approx:  3.14159265117411 , absolute error 2.41568454129037e-09 \nnumerator: 103638 , denominator: 32989 , pi_approx:  3.14159265209615 , absolute error 1.49363987844708e-09 \nnumerator: 103993 , denominator: 33102 , pi_approx:  3.1415926530119 , absolute error 5.77890624242627e-10 \nnumerator: 104348 , denominator: 33215 , pi_approx:  3.14159265392142 , absolute error 3.31628058347633e-10 \nnumerator: 208341 , denominator: 66317 , pi_approx:  3.14159265346744 , absolute error 1.22356347276309e-10 \nnumerator: 312689 , denominator: 99532 , pi_approx:  3.14159265361894 , absolute error 2.91433543964104e-11 \nnumerator: 833719 , denominator: 265381 , pi_approx:  3.14159265358108 , absolute error 8.71525074330748e-12 \nnumerator: 1146408 , denominator: 364913 , pi_approx:  3.1415926535914 , absolute error 1.61071156412618e-12 \nnumerator: 3126535 , denominator: 995207 , pi_approx:  3.14159265358865 , absolute error 1.14264153694421e-12 \nnumerator: 4272943 , denominator: 1360120 , pi_approx:  3.14159265358939 , absolute error 4.04121180963557e-13 \nnumerator: 5419351 , denominator: 1725033 , pi_approx:  3.14159265358982 , absolute error 2.22044604925031e-14"
  },
  {
    "objectID": "posts/2022-10-19-model-meadow/2022-10-19-model-meadow.html#generalized-linear-model",
    "href": "posts/2022-10-19-model-meadow/2022-10-19-model-meadow.html#generalized-linear-model",
    "title": "Model meadow",
    "section": "Generalized linear model",
    "text": "Generalized linear model\n\nBinary data\n\n\nCount data"
  },
  {
    "objectID": "posts/2022-10-19-model-meadow/2022-10-19-model-meadow.html#mixed-effect-models",
    "href": "posts/2022-10-19-model-meadow/2022-10-19-model-meadow.html#mixed-effect-models",
    "title": "Model meadow",
    "section": "Mixed effect models",
    "text": "Mixed effect models"
  },
  {
    "objectID": "posts/2022-10-19-model-meadow/2022-10-19-model-meadow.html#models-for-survival-data",
    "href": "posts/2022-10-19-model-meadow/2022-10-19-model-meadow.html#models-for-survival-data",
    "title": "Model meadow",
    "section": "Models for survival data",
    "text": "Models for survival data\n\nKaplan-Meier\n\n\nCox-PH\n\n\nCompeting risks\n\n\nMultistate models\n\nlibrary(survival)\nlibrary(mstate)\ntmat &lt;- transMat(list(c(2,3,4), c(3,4), c(4), c()), c('tox', 'twist', 'rel', 'death'))\n\ndata &lt;- data.frame(id = c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5),\n                   from = c(1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1), \n                   to = c(2, 3, 4, 3, 4, 2, 3, 4, 3, 4, 4, 4, 2, 3, 4, 2, 3, 4, 2, 3, 4),\n                   start_time = c(0, 0, 0, 5, 5, 0, 0, 0, 4, 4, 8, 9, 0, 0, 0, 0 , 0 , 0, 0, 0, 0),\n                   stop_time = c(5, 5, 5, 8, 8, 4, 4, 4, 9, 9, 12, 13, 2, 2, 2, 3, 3, 3, 7, 7, 7),\n                   status = c(1, 0, 0, 0, 1, 1, 0 , 0 , 0, 1, 0, 0, 0 , 0, 1, 0 , 1, 0, 0 , 0 , 0), \n                   group = c('a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'a', 'b', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b'))\ndata$trans &lt;- data$to - 1\n\nfm &lt;- coxph(Surv(time = start_time, time2 = stop_time, event = status) ~ strata(trans), data = data)\n\nfm_msfit &lt;- msfit(object = fm, trans = tmat)\nplot(fm_msfit)\n\n\n\np_model &lt;- probtrans(fm_msfit, predt = 0)\np_model\n\n[[1]]\n  time pstate1 pstate2 pstate3 pstate4        se1       se2       se3       se4\n1    0    1.00     0.0     0.0    0.00 0.00000000 0.0000000 0.0000000 0.0000000\n2    2    0.80     0.0     0.0    0.20 0.16000000 0.0000000 0.0000000 0.1600000\n3    3    0.60     0.0     0.2    0.20 0.19209373 0.0000000 0.1552417 0.1600000\n4    4    0.40     0.2     0.2    0.20 0.18487233 0.1479114 0.1552417 0.1600000\n5    5    0.20     0.4     0.2    0.20 0.13617799 0.1756259 0.1552417 0.1600000\n6    8    0.10     0.4     0.2    0.30 0.08447551 0.1756259 0.1552417 0.1622840\n7    9    0.05     0.4     0.2    0.35 0.04908185 0.1756259 0.1552417 0.1663768\n8   13    0.05     0.4     0.2    0.35 0.04908185 0.1756259 0.1552417 0.1663768\n\n[[2]]\n  time pstate1 pstate2 pstate3 pstate4 se1 se2 se3 se4\n1    0       0       1       0       0   0   0   0   0\n2    2       0       1       0       0   0   0   0   0\n3    3       0       1       0       0   0   0   0   0\n4    4       0       1       0       0   0   0   0   0\n5    5       0       1       0       0   0   0   0   0\n6    8       0       1       0       0   0   0   0   0\n7    9       0       1       0       0   0   0   0   0\n8   13       0       1       0       0   0   0   0   0\n\n[[3]]\n  time pstate1 pstate2 pstate3 pstate4 se1 se2 se3 se4\n1    0       0       0       1       0   0   0   0   0\n2    2       0       0       1       0   0   0   0   0\n3    3       0       0       1       0   0   0   0   0\n4    4       0       0       1       0   0   0   0   0\n5    5       0       0       1       0   0   0   0   0\n6    8       0       0       1       0   0   0   0   0\n7    9       0       0       1       0   0   0   0   0\n8   13       0       0       1       0   0   0   0   0\n\n[[4]]\n  time pstate1 pstate2 pstate3 pstate4 se1 se2 se3 se4\n1    0       0       0       0       1   0   0   0   0\n2    2       0       0       0       1   0   0   0   0\n3    3       0       0       0       1   0   0   0   0\n4    4       0       0       0       1   0   0   0   0\n5    5       0       0       0       1   0   0   0   0\n6    8       0       0       0       1   0   0   0   0\n7    9       0       0       0       1   0   0   0   0\n8   13       0       0       0       1   0   0   0   0\n\n$trans\n       to\nfrom    tox twist rel death\n  tox    NA     1   2     3\n  twist  NA    NA   4     5\n  rel    NA    NA  NA     6\n  death  NA    NA  NA    NA\n\n$method\n[1] \"aalen\"\n\n$predt\n[1] 0\n\n$direction\n[1] \"forward\"\n\nattr(,\"class\")\n[1] \"probtrans\"\n\nplot(p_model, use.ggplot = TRUE)\n\n\n\nplot(p_model, use.ggplot = TRUE, type = 'separate', conf.int = 0.95,conf.type = 'log')"
  },
  {
    "objectID": "posts/2022-10-19-model-meadow/2022-10-19-model-meadow.html#machine-learning-models",
    "href": "posts/2022-10-19-model-meadow/2022-10-19-model-meadow.html#machine-learning-models",
    "title": "Model meadow",
    "section": "Machine learning models",
    "text": "Machine learning models\n\nNeural nets"
  },
  {
    "objectID": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html",
    "href": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html",
    "title": "Reducing image file size with ImageMagick",
    "section": "",
    "text": "stackoverflow post recommandations for compressing\nstackoverflow maximum file size\nExtensive analysis of imagemagick options"
  },
  {
    "objectID": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#links",
    "href": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#links",
    "title": "Reducing image file size with ImageMagick",
    "section": "",
    "text": "stackoverflow post recommandations for compressing\nstackoverflow maximum file size\nExtensive analysis of imagemagick options"
  },
  {
    "objectID": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#initial-analysis",
    "href": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#initial-analysis",
    "title": "Reducing image file size with ImageMagick",
    "section": "Initial analysis",
    "text": "Initial analysis"
  },
  {
    "objectID": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#currently-best-version",
    "href": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#currently-best-version",
    "title": "Reducing image file size with ImageMagick",
    "section": "Currently best version",
    "text": "Currently best version\n\nsystem('convert koga_original.jpg -quality 50 -resize 2048 low_quality_3.jpg')\n\n\n\n\nCurrently best version"
  },
  {
    "objectID": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#even-a-lot-smaller",
    "href": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#even-a-lot-smaller",
    "title": "Reducing image file size with ImageMagick",
    "section": "Even a lot smaller",
    "text": "Even a lot smaller\n\nsystem('convert koga_original.jpg -quality 50 -resize 1024 low_quality_4.jpg')\nsystem('convert koga_original.jpg -quality 50 -resize 512 low_quality_5.jpg')"
  },
  {
    "objectID": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#file-sizes",
    "href": "posts/2023-08-03-reducing-image-file-size-with-imagemagick/2023-08-03-reducing-image-file-size-with-imagemagick.html#file-sizes",
    "title": "Reducing image file size with ImageMagick",
    "section": "File sizes",
    "text": "File sizes\n\nlibrary('fs')\nlibrary('knitr')\n\ntibble(filenames = dir_ls(), \n       filesizes = file_size(filenames)) |&gt; \n  kable(align = 'c')\n\n\n\n\n\n\n\n\nfilenames\nfilesizes\n\n\n\n\n2023-08-03-reducing-image-file-size-with-imagemagick.qmd\n2.09K\n\n\n2023-08-03-reducing-image-file-size-with-imagemagick.rmarkdown\n2.1K\n\n\n2023-08-03-reducing-image-file-size-with-imagemagick_files\n96\n\n\nkoga_edited.jpg\n193.93K\n\n\nkoga_original.jpg\n2.5M\n\n\nkoga_signal.jpeg\n505.13K\n\n\nlow_quality_1.jpg\n206.18K\n\n\nlow_quality_2.jpg\n516.7K\n\n\nlow_quality_3.jpg\n387.38K\n\n\nlow_quality_4.jpg\n206.18K\n\n\nlow_quality_5.jpg\n146.79K"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nLeave-one-out cross-validation in Python\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nCars dataset and tidymodels\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nLeave-one-out cross-validation in R\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRegularization in R\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGetting to know mlr3\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGetting to know tidymodels\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\n\n\n\n\n  \n\n\n\n\nKaplan Meier plots\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nReducing image file size with ImageMagick\n\n\n\n\n\n\n\nR\n\n\nMishmash\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nApproximating Pi\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nBicycle comparison\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nFirst pytorch tutorial\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nStarting with fastai\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nThe iris dataset\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nEasier publishing\n\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nModel diagnostics for logistic regression\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPie chart\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nIdeas for new posts\n\n\n\n\n\n\n\nMishmash\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\n\n\n\n\n  \n\n\n\n\nManaging python versions in RStudio\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPython graphics\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nExploring the Python library statsmodels\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJul 4, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPython pandas\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSpecifying mixed effects in lme4\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2023\n\n\n\n\n\n\n  \n\n\n\n\nInteresting R packages\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPython study notes\n\n\n\n\n\n\n\nSelf-study\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nCoursera Deep Learning Specialization\n\n\n\n\n\n\n\nSelf-study\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSelf-study: Deep Learning by Goodfellow et al.\n\n\n\n\n\n\n\nSelf-study\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSelf-study: McElreath - Statistical Rethinking\n\n\n\n\n\n\n\nSelf-study\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow bad are bananas\n\n\n\n\n\n\n\nSustainibility\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2023\n\n\n\n\n\n\n  \n\n\n\n\nManual colors in ggplot2\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 21, 2023\n\n\n\n\n\n\n  \n\n\n\n\nStrange behavior in ggplot2\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSIRS model dynamics\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSustainibility data\n\n\n\n\n\n\n\nSustainibility\n\n\n\n\n\n\n\n\n\n\n\nJan 10, 2023\n\n\n\n\n\n\n  \n\n\n\n\nContribution plots\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR interfaces to trial registries\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSimulate time to-event-data\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\nModel meadow\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2022\n\n\n\n\n\n\n  \n\n\n\n\nRetrieving and plotting COVID-19 data\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nLeo’s project\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSIR-like model with deSolve in deterministic and stochastic version\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAntipsychotics\n\n\n\n\n\n\n\nPsychiatry\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTimeline\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPsychopharmaca and time\n\n\n\n\n\n\n\nPsychiatry\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nChess ressources\n\n\n\n\n\n\n\nMishmash\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHumidity\n\n\n\n\n\n\n\nSustainibility\n\n\n\n\n\n\n\n\n\n\n\nSep 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSIR model\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2022\n\n\n\n\n\n\nNo matching items"
  }
]