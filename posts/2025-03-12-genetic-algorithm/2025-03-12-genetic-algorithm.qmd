---
title: Genetic algorithm
date: 2025-03-12
categories:
  - R
---
## Background
* We want to fit a model with parameters to data
* Given: Data points and a function to calculate a loss function for a given parameter set
* What we don't have: Derivative of the loss function with respect to the parameters -- hence, gradient-descent-type approaches don't work
* Genetic algorithms are a population-based heuristic method that can optimize even when the loss function is non-differentiable or discontinuous

## Example
### True model
* We assume a linear model with $n$ data points and $p$ predictors:
$$
y = X\beta + \epsilon, \epsilon_i \sim \mathcal{N}
$$
* The true value of $\beta$ is also sampled from a normal distribution

### Optimization algorithm
* We define the residual sum of squares (RSS) as our loss function
* We sample a "population" of $k$  $\beta$ vectors from a normal distribution and store them in a $p \times k$ matrix 
* We repeat $m$ times:
  * *Selection*: The top 50% of individuals (i.e., those with the lowest RSS) are selected and then duplicated to maintain the population size
  * *Mutation*: We nudge the elements of $P$ by element-wise multiplication with a matrix with identical dimension filled with values sampled from a log-normal distribution with *log_mean=0* and *log_sd=s*
    * all updates maintain the sign of the parameter (=limitation of the algorithm)

## Simulate data
```{r}
set.seed(1)
n <- 5              # number of data points
p <- 3              # number of predictors
beta <- rnorm(p)    
X <- matrix(ncol = p, data = rnorm(n * p))
y <- (X %*% beta)[, 1] + rnorm(n)
dat <- as.data.frame(X)
dat$y <- y
```

## Step by step
```{r}
# Create parameter population:
k <- 6
P <- matrix(nrow = p, data = rnorm(k * p))

# Predict for first population:
X %*% P[, 1]

# Predict for all populations:
X %*% P

# Residuals for all populations:
X %*% P - y

# Define function to calc RSS:
calc_RSS <- function(X, y, P) {
  apply((X %*% P - y)^2, 2, sum)
}

# Calc RSS:
(l <- calc_RSS(X, y, P))

# determine indices of better half of population:
(ii <- order(l)[1:(k / 2)])

# select better half and replicate:
(P <- P[, rep(ii, each = 2)])

# mutate population:
(P <- P * rlnorm(length(P), sdlog = 0.05))
```


## Whole game
### Put algorithm in a function
```{r}
#' Genetic Algorithm Optimizer
#'
#' Implements a genetic algorithm to optimize a parameter matrix for minimizing residual sum of squares (RSS).
#'
#' @param X A matrix of predictor variables.
#' @param y A numeric vector of response values.
#' @param k An integer specifying the number of parameter sets (population size).
#' @param m An integer specifying the number of iterations (default: 1000).
#' @param s A numeric value specifying the standard deviation of the log-normal mutation factor (default: 0.01).
#'
#' @return A list containing:
#' \describe{
#'   \item{P}{A matrix of optimized parameter sets.}
#'   \item{rss_history}{A numeric vector of minimum RSS values across iterations.}
#' }
#'
#' @details
#' This function applies a genetic algorithm to optimize parameters for a given dataset. It initializes a random population of parameters, evaluates the RSS, selects the best-performing half of the population, duplicates them, and applies mutations.
genetic_algorithm_optimizer <- function(X, y, k, 
                                        m = 1000, s = 0.01) {
  P <- matrix(nrow = p, data = rnorm(k * p)) 
  rss_history <- vector(length = m)
  for (i in 1:m) {
    l <- calc_RSS(X, y, P)
    rss_history[i] <- min(l)
    # Selection: Keep the top 50% with lowest RSS and duplicate them
    ii <- rep(order(l)[1:(k / 2)], 2)
    P <- P[, ii]
    # Mutation:
    P <- P * rlnorm(length(P), sdlog = s)
  }
  list(P = P, rss_history = rss_history)
}
```

### Run algorithm
* Some suggestions for playing around with tuning parameters:
  * Vary number of individuals $k$ from 2 to 1000 (only non-odd integers please)
  * Vary number of iterations $m$ from 10 to 1000 (look at plot of RSS to adjust in a smart fashion)
  * Vary standard deviation on log scale of mutation strength $s$ from 0.001 to 0.1
```{r}
sol <- genetic_algorithm_optimizer(X, y, 
                                   k = 10,
                                   m = 1000, 
                                   s = 0.01)
plot(sol$rss_history, 
     type = 'l', 
     log = 'y', 
     main = 'RSS of "best individual"', 
     ylab = 'RSS on log scale', 
     xlab = 'Iteration')

# Comparing the results of linear model solution and genetic algorithm
# when converged, all individuals should be very similar 
# --> showing only individual 1:
data.frame(analytic_solution = coef(lm(y ~ - 1 + ., data = dat)), 
           genetic_algorithm_solution = sol$P[, 1]) 
```

* Genetic algorithm seems to have difficulties "finding" solutions with the correct sign of the first element of $\beta$ for small populations -- larger populations (e. g. *k = 1000*) do the job quite reliably
