---
title: Leave-one-out cross-validation in R
date: 2023-08-08
categories:
  - R
  - 'Linear model vs neural nets'
---

## Strategy
* Fit "straight" models: linear model, neural net with 2 layers with 3 perceptrons each
* Perform "Leave-one-out-cross-validation"
* Calculate mean absolute prediction error of the different methods
  * In cross-validation case for the left-out data point

## Implementation
```{r}
#| label: init
#| message: false
set.seed(123)
library('tidyverse')
library('knitr')
library('neuralnet')
update_geom_defaults("line", list(linewidth = 1))
```

```{r}
leave_one_out <- Vectorize(
  function(i, type) {
    training <- cars[-i, ]
    test <- cars[i, ]
    if(type == 'lm') {
      fm <- lm(speed ~ dist, data = training)
      pp <- predict(fm, newdata = test)
    } else {
      fm <- neuralnet(speed ~ dist, 
                      linear.output = TRUE, data = training, 
                      hidden = c(3, 3), 
                      stepmax = 1e+7, threshold = .5)
      pp <- predict(fm, newdata = test)[, 1]
    }
    return(pp)
  }, vectorize.args = 'i')
```


```{r}
#| cache: true
fm_lm <- lm(speed ~ dist, data = cars)
fm_nn <- neuralnet(speed ~ dist, 
                   linear.output = TRUE, data = cars, 
                   hidden = c(3, 3), 
                   stepmax = 1e+7, threshold = .5)
data <- tibble(
  cars,
  p_lm_straight = predict(fm_lm),
  p_nn_straight= predict(fm_nn, newdata = cars)[, 1],
  p_lm_looc = leave_one_out(1:nrow(cars), type = 'lm'),
  p_nn_looc = leave_one_out(1:nrow(cars), type = 'nn')
)
```

```{r}
pred_df <- tibble(dist = 0:125)
pred_df <- pred_df |> 
  mutate(pred_lm = predict(fm_lm, pred_df),
         pred_nn = predict(fm_nn, pred_df)[, 1])

text_to_add <- tibble(dist = 80, value = 7, text = 'Original data as black dots')
pred_df |> pivot_longer(cols = -dist) |> 
  ggplot(aes(dist, value, colour = name)) + geom_line() +
  geom_point(mapping = aes(dist, speed), data = cars, inherit.aes = FALSE) +
  geom_label(aes(dist, value, label = text), data = text_to_add, inherit.aes = FALSE) +
  labs(colour = 'Predictions', y = 'speed', 
       title = 'Predictions for straight models for all integers from 0 to 125')
```



```{r}
data |> 
  select(-speed) |> 
  pivot_longer(cols = -dist) |> 
  ggplot(aes(dist, value, colour = name)) +
  geom_line(linewidth = 1) + 
  labs(y = 'speed', colour = '',
       title = 'Predictions for all dist values that are in the original data set as well') +
  geom_point(aes(dist, speed), cars, inherit.aes = FALSE) +
  geom_label(aes(dist, value, label = text), data = text_to_add, inherit.aes = FALSE)



data_long <- data |> 
  pivot_longer(cols = -c(dist, speed)) 

data_long |> 
  ggplot(aes(dist, speed - value)) + geom_point() + facet_wrap(vars(name)) +
  labs(y = 'Prediction error')

data_long |> 
  mutate(error = value - speed,
         squarred_error = error^2) |> 
  rename(Method = 'name') |> 
  group_by(Method) |> 
  summarize('Mean absolute error' = mean(abs(error)),
            'Mean squarred error' = mean(squarred_error)) |> 
  kable(align = 'c')
```

## Conclusion
* The neural net seems to overfit!